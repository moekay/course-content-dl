{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "colab": {
   "name": "em_synapses.ipynb",
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ComputerVision/em_synapses.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "renayVUI7b9x"
   },
   "source": [
    "# Knowledge Extraction from a Convolutional Neural Network\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Jan Funke\n",
    "\n",
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Train a convolutional neural network to classify images and a CycleGAN to translate between images of different types.\n",
    "\n",
    "This notebook contains everything to train a VGG network on labelled images and to train a CycleGAN to translate between images.\n",
    "\n",
    "We will use electron microscopy images of Drosophila synapses for this project. Those images can be classified according to the neurotransmitter type they release.\n"
   ],
   "id": "renayVUI7b9x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd7d427d"
   },
   "source": [
    "---\n",
    "# Project Ideas\n",
    "\n",
    "1. Improve the classifier. This code uses a VGG network for the classification. On the synapse dataset, we will get a validation accuracy of around 80%. Try to see if you can improve the classifier accuracy.\n",
    "    * (easy) Data augmentation: The training code for the classifier is quite simple in this example. Enlarge the amount of available training data by adding augmentations (transpose and mirror the images, add noise, change the intensity, etc.).\n",
    "    * (easy) Network architecture: The VGG network has a few parameters that one can tune. Try a few to see what difference it makes.\n",
    "    * (easy) Inspect the classifier predictions: Take random samples from the test dataset and classify them. Show the images together with their predicted and actual labels.\n",
    "    * (medium) Other networks:  Try different architectures (e.g., a ResNet) and see if the accuracy can be improved.\n",
    "    * (medium) Inspect errors made by the classifier. Which classes are most accurately predicted? Which classes are confused with each other?\n",
    "    \n",
    "    \n",
    "2. Explore the CycleGAN.\n",
    "    * (easy) The example code below shows how to translate between GABA and glutamate. Try different combinations, and also in the reverse direction. Can you start to see differences between some pairs of classes? Which are the ones where the differences are the most or the least obvious?\n",
    "    * (hard) Watching the CycleGAN train can be a bit boring. Find a way to show (periodically) the current image and its translation to see how the network is improving over time. Hint: The `cycle_gan` module has a `Visualizer`, which might be helpful.\n",
    "    \n",
    "\n",
    "3. Try on your own data!\n",
    "    * Have a look at how the synapse images are organized in `data/raw/synapses`. Copy the directory structure and use your own images. Depending on your data, you might have to adjust the image size (128x128 for the synapses) and number of channels in the VGG network and CycleGAN code.\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "This notebook was written by Jan Funke, using code from Nils Eckstein and a modified version of the [CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) implementation.\n"
   ],
   "id": "bd7d427d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5642d709"
   },
   "source": [
    "---\n",
    "# Section 1: Train an Image Classifier\n",
    "\n",
    "In this section, we will implement and train a VGG classifier to classify images of synapses into one of six classes, corresponding to the neurotransmitter type that is released at the synapse: GABA, acethylcholine, glutamate, octopamine, serotonin, and dopamine."
   ],
   "id": "5642d709"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c61a11c6"
   },
   "source": [
    "## Data Preparation"
   ],
   "id": "c61a11c6"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "821dc497"
   },
   "source": [
    "# Download the resources for this tutorial (one zip file)\n",
    "!wget 'https://www.dropbox.com/sh/ucpjfd3omjieu80/AAAvZynLtzvhyFx7_jwVhUK2a?dl=0&preview=data.zip' -O resources.zip"
   ],
   "id": "821dc497",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "b0be60d5"
   },
   "source": [
    "# Unzip the 'synapses' dataset and model checkpoints\n",
    "# (this will take a while)\n",
    "!unzip -o resources.zip data.zip\n",
    "!unzip -o resources.zip checkpoints.zip\n",
    "!unzip -o data.zip 'data/raw/synapses/*'\n",
    "!unzip -o checkpoints.zip 'checkpoints/synapses/*'\n",
    "# make sure the order of classes matches the pretrained model\n",
    "!mv data/raw/synapses/gaba data/raw/synapses/0_gaba      \n",
    "!mv data/raw/synapses/acetylcholine data/raw/synapses/1_acetylcholine\n",
    "!mv data/raw/synapses/glutamate data/raw/synapses/2_glutamate \n",
    "!mv data/raw/synapses/serotonin data/raw/synapses/3_serotonin \n",
    "!mv data/raw/synapses/octopamine data/raw/synapses/4_octopamine \n",
    "!mv data/raw/synapses/dopamine data/raw/synapses/5_dopamine "
   ],
   "id": "b0be60d5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b84ec7b"
   },
   "source": [
    "## Classifier Training"
   ],
   "id": "0b84ec7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a79ab567"
   },
   "source": [
    "### Create and Inspect Datasets\n",
    "\n",
    "First, we create a `torch` data loaders for training, validation, and testing. We will use weighted sampling to account for the class imbalance during training."
   ],
   "id": "a79ab567"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ae50b16a"
   },
   "source": [
    "from skimage.io import imread\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_image(filename):\n",
    "    \n",
    "    image = imread(filename)\n",
    "    \n",
    "    # images are grescale, we only need one of the RGB channels \n",
    "    image = image[:, :, 0]\n",
    "    \n",
    "    # img is uint8 in [0, 255], but we want float32 in [-1, 1]\n",
    "    image = image.astype(np.float32)/255.0\n",
    "    image = (image - 0.5)/0.5\n",
    "    \n",
    "    return image\n",
    "\n",
    "# create a dataset for all images of all classes\n",
    "full_dataset = ImageFolder(root='data/raw/synapses', loader=load_image)\n",
    "\n",
    "# randomly split the dataset into train, validation, and test\n",
    "num_images = len(full_dataset)\n",
    "# ~70% for training\n",
    "num_training = int(0.7 * num_images)\n",
    "# ~15% for validation\n",
    "num_validation = int(0.15 * num_images)\n",
    "# ~15% for testing\n",
    "num_test = num_images - (num_training + num_validation)\n",
    "# split the data randomly (but with a fixed random seed)\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [num_training, num_validation, num_test],\n",
    "    generator=torch.Generator().manual_seed(23061912))\n",
    "\n",
    "# compute class weights in training dataset for uniform sampling\n",
    "ys = np.array([y for _, y in train_dataset])\n",
    "counts = np.bincount(ys)\n",
    "label_weights = 1.0 / counts\n",
    "weights = label_weights[ys]\n",
    "\n",
    "print(\"Number of images per class:\")\n",
    "for c, n, w in zip(full_dataset.classes, counts, label_weights):\n",
    "    print(f\"\\t{c}:\\tn={n}\\tweight={w}\")\n",
    "\n",
    "# create a data loader with uniform sampling\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "# this data loader will serve 8 images in a \"mini-batch\" at a time\n",
    "dataloader = DataLoader(train_dataset, batch_size=8, drop_last=True, sampler=sampler)"
   ],
   "id": "ae50b16a",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9010bdc"
   },
   "source": [
    "The cell below visualizes a single, randomly chosen batch from the training data loader. Feel free to execute this cell multiple times to get a feeling for the dataset. See if you can tell the difference between synapses of different types!"
   ],
   "id": "e9010bdc"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3d8c6f3a"
   },
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_batch(x, y):\n",
    "    fig, axs = plt.subplots(1, x.shape[0], figsize=(14, 14), sharey=True)\n",
    "    for i in range(x.shape[0]):\n",
    "        axs[i].imshow(np.squeeze(x[i]), cmap='gray')\n",
    "        axs[i].set_title(train_dataset.dataset.classes[y[i].item()])\n",
    "    plt.show()\n",
    "\n",
    "# show a random batch from the data loader\n",
    "# (run this cell repeatedly to see different batches)\n",
    "for x, y in dataloader:\n",
    "    show_batch(x, y)\n",
    "    break"
   ],
   "id": "3d8c6f3a",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f882416f"
   },
   "source": [
    "### Create a Model, Loss, and Optimizer"
   ],
   "id": "f882416f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "54f177cc"
   },
   "source": [
    "class Vgg2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            fmaps=12,\n",
    "            downsample_factors=[(2, 2), (2, 2), (2, 2), (2, 2)],\n",
    "            output_classes=6):\n",
    "\n",
    "        super(Vgg2D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        current_fmaps = 1\n",
    "        current_size = tuple(input_size)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv2d(\n",
    "                    current_fmaps,\n",
    "                    fmaps,\n",
    "                    kernel_size=3,\n",
    "                    padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv2d(\n",
    "                    fmaps,\n",
    "                    fmaps,\n",
    "                    kernel_size=3,\n",
    "                    padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool2d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(\n",
    "                current_size[0] *\n",
    "                current_size[1] *\n",
    "                current_fmaps,\n",
    "                4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(\n",
    "                4096,\n",
    "                4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(\n",
    "                4096,\n",
    "                output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        shape = tuple(raw.shape)\n",
    "        raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ],
   "id": "54f177cc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5da43245"
   },
   "source": [
    "# get the size of our images\n",
    "for x, y in train_dataset:\n",
    "    input_size = x.shape\n",
    "    break\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg2D(input_size)\n",
    "\n",
    "# create a loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# create an optimzer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ],
   "id": "5da43245",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01688095"
   },
   "source": [
    "### Train the Model"
   ],
   "id": "01688095"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fa65090d"
   },
   "source": [
    "# use a GPU, if it is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ],
   "id": "fa65090d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecbab4f7"
   },
   "source": [
    "The next cell merely defines some convenience functions for training, validation, and testing:"
   ],
   "id": "ecbab4f7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "1a8c7fe9"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train():\n",
    "    '''Train the model for one epoch.'''\n",
    "\n",
    "    # set the model into train mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    for x, y in tqdm(dataloader, 'train'):\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += l\n",
    "        num_batches += 1\n",
    "\n",
    "    return epoch_loss/num_batches\n",
    "\n",
    "def evaluate(dataloader, name):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in tqdm(dataloader, name):\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        predictions = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        correct += int(torch.sum(predictions == y).cpu().detach().numpy())\n",
    "        total += len(y)\n",
    "    \n",
    "    accuracy = correct/total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def validate():\n",
    "    '''Evaluate prediction accuracy on the validation dataset.'''\n",
    "    \n",
    "    model.eval()\n",
    "    dataloader = DataLoader(validation_dataset, batch_size=32)\n",
    "   \n",
    "    return evaluate(dataloader, 'validate')\n",
    "\n",
    "def test():\n",
    "    '''Evaluate prediction accuracy on the test dataset.'''\n",
    "    \n",
    "    model.eval()\n",
    "    dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    return evaluate(dataloader, 'test')"
   ],
   "id": "1a8c7fe9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68bcfbbf"
   },
   "source": [
    "We are ready to train. After each epoch (roughly going through each training image once), we report the training loss and the validation accuracy."
   ],
   "id": "68bcfbbf"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "d0af7638"
   },
   "source": [
    "for epoch in range(100):\n",
    "    \n",
    "    epoch_loss = train()\n",
    "    print(f\"epoch {epoch}, training loss={epoch_loss}\")\n",
    "    \n",
    "    accuracy = validate()\n",
    "    print(f\"epoch {epoch}, validation accuracy={accuracy}\")"
   ],
   "id": "d0af7638",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45e31b87"
   },
   "source": [
    "Feel free to watch your model train... \n",
    "![model_train.jpeg](attachment:model_train.jpeg)\n",
    "\n",
    "Or interrupt the current cell and run the next one, which will load a checkpoint that we already prepared."
   ],
   "id": "45e31b87"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "53fb8dda"
   },
   "source": [
    "# SHORTCUT (OPTIONAL): load a pretrained model\n",
    "\n",
    "# change this to True and run this cell if you want a shortcut\n",
    "yes_I_want_the_pretrained_model = False\n",
    "\n",
    "if yes_I_want_the_pretrained_model:\n",
    "    checkpoint = torch.load('checkpoints/synapses/classifier/vgg_checkpoint', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ],
   "id": "53fb8dda",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4f6e3663"
   },
   "source": [
    "accuracy = test()\n",
    "print(f\"final test accuracy: {accuracy}\")"
   ],
   "id": "4f6e3663",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f43bba5"
   },
   "source": [
    "This concludes the first section. We now have a classifier that can discriminate between images of different types.\n",
    "\n",
    "If you used the images we provided, the classifier is not perfect (you should get an accuracy of around 80%), but pretty good considering that there are six different types of images. Furthermore, it is not so clear for humans how the classifier does it. Feel free to explore the data a bit more and see for yourself if you can tell the difference betwee, say, GABAergic and glutamatergic synapses.\n",
    "\n",
    "So this is an interesting situation: The VGG network knows something we don't quite know. In the next section, we will see how we can visualize the relevant differences between images of different types."
   ],
   "id": "3f43bba5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72b5240c"
   },
   "source": [
    "---\n",
    "# Section 2: Train a GAN to Translate Images\n",
    "\n",
    "We will train a so-called CycleGAN to translate images from one class to another."
   ],
   "id": "72b5240c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "41c9e63b"
   },
   "source": [
    "# get the CycleGAN code and dependencies\n",
    "!git clone https://github.com/funkey/neuromatch_xai\n",
    "!mv neuromatch_xai/cycle_gan .\n",
    "!pip install dominate"
   ],
   "id": "41c9e63b",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5da5c01"
   },
   "source": [
    "In this example, we will translate between GABAergic and glutamatergic synapses.\n",
    "\n",
    "First, we have to copy images of either type into a format that the CycleGAN library is happy with. Afterwards, we can start training on those images."
   ],
   "id": "e5da5c01"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "2b2519c4"
   },
   "source": [
    "import cycle_gan\n",
    "cycle_gan.prepare_dataset('data/raw/synapses/', ['0_gaba', '2_glutamate'])\n",
    "cycle_gan.train('data/raw/synapses/', '0_gaba', '2_glutamate', 128)"
   ],
   "id": "2b2519c4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d328904"
   },
   "source": [
    "...this time again.\n",
    "![model_train_2.jpeg](attachment:model_train_2.jpeg)\n",
    "\n",
    "Training the CycleGAN takes a lot longer than the VGG we trained above (on the synapse dataset, this will be around 7 days...).\n",
    "\n",
    "To continue, interrupt the kernel and continue with the next one, which will just use one of the pretrained CycleGAN models for the synapse dataset."
   ],
   "id": "0d328904"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "a182c3bc"
   },
   "source": [
    "# translate images from class A to B, and classify each with the VGG network trained above\n",
    "cycle_gan.test(\n",
    "    data_dir='data/raw/synapses/',\n",
    "    class_A='0_gaba',\n",
    "    class_B='2_glutamate',\n",
    "    img_size=128,\n",
    "    checkpoints_dir='checkpoints/synapses/cycle_gan/gaba_glutamate/',\n",
    "    vgg_checkpoint='checkpoints/synapses/classifier/vgg_checkpoint'\n",
    ")"
   ],
   "id": "a182c3bc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17fc1703"
   },
   "source": [
    "Read all translated images and sort them by how much the translation \"fools\" the VGG classifier trained above:"
   ],
   "id": "17fc1703"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2a582ba6"
   },
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "class_A_index = 0\n",
    "class_B_index = 2\n",
    "\n",
    "result_dir = 'data/raw/synapses/cycle_gan/0_gaba_2_glutamate/results/test_latest/images/'\n",
    "classification_results = []\n",
    "for f in glob.glob(result_dir + '/*.json'):\n",
    "    result = json.load(open(f))\n",
    "    result['basename'] = f.replace('_aux.json', '')\n",
    "    classification_results.append(result)\n",
    "classification_results.sort(\n",
    "    key=lambda c: c['aux_real'][class_A_index] * c['aux_fake'][class_B_index],\n",
    "    reverse=True)"
   ],
   "id": "2a582ba6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc0d486"
   },
   "source": [
    "Show the top real and fake images that make the classifier change its mind:"
   ],
   "id": "2cc0d486"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "1567b00e"
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.io import imread\n",
    "\n",
    "def show_pair(a, b, score_a, score_b, class_a, class_b):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 20), sharey=True)\n",
    "    axs[0].imshow(a, cmap='gray')\n",
    "    axs[0].set_title(f\"p({class_a}) = \" + str(score_a))\n",
    "    axs[1].imshow(b, cmap='gray')\n",
    "    axs[1].set_title(f\"p({class_b}) = \" + str(score_b))\n",
    "    plt.show()\n",
    "\n",
    "# show the top successful translations (according to our VGG classifier)\n",
    "for i in range(10):\n",
    "    basename = classification_results[i]['basename']\n",
    "    score_A = classification_results[i]['aux_real'][class_A_index]\n",
    "    score_B = classification_results[i]['aux_fake'][class_B_index]\n",
    "    real_A = imread(basename + '_real.png')\n",
    "    fake_B = imread(basename + '_fake.png')\n",
    "    show_pair(real_A, fake_B, score_A, score_B, 'gaba', 'glutamate')"
   ],
   "id": "1567b00e",
   "execution_count": null,
   "outputs": []
  }
 ]
}