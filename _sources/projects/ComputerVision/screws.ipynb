{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "screws.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ComputerVision/screws.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-H6ciSlwcqV"
   },
   "source": [
    "#Something Screwy - image recognition, detection, and classification of screws\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Joe Donovan (joe311@gmail.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoiOUyRnHjAD"
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWi_gNA-HjC2"
   },
   "source": [
    "Useful link: \n",
    "NMA daily guide to projects - https://deeplearning.neuromatch.io/projects/docs/project_guidance.html \n",
    "\n",
    "The overall goal of the project is to learn about object recognition, classification, and detection. \n",
    "You'll start with simple networks, and potentially work up to larger pretained models. Your loss function is to optimize learning, not model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUPGdzfnmR0l"
   },
   "source": [
    "##Structure \n",
    "###[Data loading & structure](#dataloading) and [helpers](#helpers)\n",
    "###[Data inspection](#datainspection)\n",
    "###E1. Object classification - [setup](#nutsetup) and [network](#nutwork)\n",
    "###E2. [Damaged screw detection](#damagedscrew)\n",
    "###E3. [Multi-class classification](#multiclass)\n",
    "###M1. [Object detection](#objectdetection)\n",
    "###M2. [Network performance/introspection](#inspection)\n",
    "###M3. [Oriented bounding boxes](#oriented)\n",
    "###M4. [Class clustering](#clustering)\n",
    "###H1. [Perspective and scale](#perspective)\n",
    "###H2. [Transfer Learning](#transfer)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6i-3mg3kqwmJ"
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import json \n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from scipy import ndimage\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setup matplotlib \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v_KWEVjLOtWT",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "#@title Setup matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams, gridspec \n",
    "from matplotlib import patches, transforms as plt_transforms\n",
    "\n",
    "rcParams['figure.figsize'] = [16, 6]\n",
    "rcParams['font.size'] =14\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNU6Sg7bfGmb"
   },
   "source": [
    "## Data loading <a name=\"dataloading\"></a>\n",
    "Let's start by downloading the data and taking a look at it  \n",
    "\n",
    "Properly understanding and exploring the structure of your data is a crucial step to any project "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lrwN5O8Vq7wm"
   },
   "source": [
    "#Download dataset, took around 4 minutes for me \n",
    "#https://www.mvtec.com/company/research/datasets/mvtec-screws\n",
    "\n",
    "if not os.path.isfile('mvtec_screws_v1.0.tar.gz'):\n",
    "  !wget ftp://guest:GU.205dldo@ftp.softronics.ch/mvtec_screws/v1.0/mvtec_screws_v1.0.tar.gz\n",
    "#unpack tar datafile\n",
    "datapath = 'screwdata'\n",
    "if not os.path.exists(datapath):\n",
    "  !mkdir screwdata\n",
    "  !tar -xzf mvtec_screws_v1.0.tar.gz -C screwdata"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2kjof2zmrFno"
   },
   "source": [
    "#Some json files and a folder full of images\n",
    "os.listdir(datapath)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_y24JvIGszbZ"
   },
   "source": [
    "#There's some details in the readme\n",
    "!cat screwdata/README_v1.0.txt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wR2fiYWPs6rh"
   },
   "source": [
    "#Load the json file with the annotation metadata\n",
    "with open(os.path.join(datapath, 'mvtec_screws.json')) as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "print(data.keys())\n",
    "print(data['images'][0])\n",
    "print(data['annotations'][0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B_DxfFRLz0hr"
   },
   "source": [
    "#Load the images, and make some helpful dict to map the data\n",
    "imgdir = os.path.join(datapath, 'images')\n",
    "\n",
    "#remap images to dict by id \n",
    "imgdict = {l['id']:l for l in data['images']}\n",
    "#read in all images, can take some time\n",
    "for i in imgdict.values():\n",
    "  i['image'] = io.imread(os.path.join(imgdir, i['file_name']))[:,:,:3] #drop alpha channel, if it's there\n",
    "\n",
    "#remap annotations to dict by image_id\n",
    "from collections import defaultdict\n",
    "annodict = defaultdict(list) \n",
    "for annotation in data['annotations']:\n",
    "  annodict[annotation['image_id']].append(annotation)\n",
    "\n",
    "#setup list of categories\n",
    "categories = data['categories']\n",
    "ncategories = len(categories)\n",
    "cat_ids = [i['id'] for i in categories]\n",
    "category_names = {7:'nut', 3:'wood screw', 2:'lag wood screw', 8:'bolt', 6:'black oxide screw', 5:'shiny screw', 4:'short wood screw', 1:'long lag screw', 9:'large nut', 11:'nut', 10:'nut', 12:'machine screw', 13:'short machine screw' }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4dgBcz0pMPM"
   },
   "source": [
    "##Helper functions <a name=\"helpers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XYtxSuAmgwkE"
   },
   "source": [
    "#helpful function for extracting rotated subimages etc\n",
    "def unpack_bbox(bbox):\n",
    "  #bbox as in the json/COCO data format (centerx, centery, width, height, theta is in radians)\n",
    "\n",
    "  rot_center = np.array((bbox[1], bbox[0])).T\n",
    "  width = bbox[3] \n",
    "  height = bbox[2]\n",
    "  theta = -bbox[4]+np.pi/2 #radians\n",
    "  return rot_center, width, height, theta\n",
    "\n",
    "def rotcorners_from_coords(rot_center, width, height, theta):\n",
    "  rotation = np.array(( (np.cos(theta), -np.sin(theta)),\n",
    "               (np.sin(theta),  np.cos(theta))))\n",
    "  \n",
    "  wvec = np.dot(rotation, (width/2, 0))\n",
    "  hvec = np.dot(rotation, (0, height/2))\n",
    "  corner_points = rot_center + [wvec+hvec, wvec-hvec, -wvec+hvec, -wvec-hvec]\n",
    "  return corner_points\n",
    "\n",
    "def rotbbox_from_coords(rot_center, width, height, theta):\n",
    "  corner_points = rotcorners_from_coords(rot_center, width, height, theta)\n",
    "  rot_bbox = np.array((corner_points.min(0), corner_points.max(0))).astype(np.int)\n",
    "  #constrain inside image \n",
    "  rot_bbox[rot_bbox < 0] = 0\n",
    "\n",
    "  return rot_bbox\n",
    "\n",
    "def extract_subimg_bbox(im, bbox):\n",
    "  return extract_subimg(im, *unpack_bbox(bbox))\n",
    "\n",
    "def extract_subimg(im, rot_center, width, height, theta): \n",
    "  rot_bbox = rotbbox_from_coords(rot_center, width, height, theta)\n",
    "\n",
    "  subimg = im[rot_bbox[0,1]:rot_bbox[1,1],rot_bbox[0,0]:rot_bbox[1,0]]\n",
    "  rotated_im = ndimage.rotate(subimg, np.degrees(theta)+180)\n",
    "  newcenter = (np.array(rotated_im.shape)/2).astype(np.int)\n",
    "  rotated_im = rotated_im[int(newcenter[0]-height/2):int(newcenter[0]+height/2), int(newcenter[1]-width/2):int(newcenter[1]+width/2), :3]  #drop alpha channel, if it's there\n",
    "\n",
    "  return rotated_im"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5726L-lphT1"
   },
   "source": [
    "##Let's check out some data <a name=\"datainspection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BTZLYuF301rq"
   },
   "source": [
    "#Let's look at one image and it's associated annotations\n",
    "imageid = 100\n",
    "im = imgdict[imageid]['image']\n",
    "gs = gridspec.GridSpec(1, 1+len(annodict[imageid]), width_ratios=[1,]+[.1]*len(annodict[imageid]), wspace=.05)\n",
    "plt.figure()\n",
    "ax = plt.subplot(gs[0])\n",
    "plt.imshow(im)\n",
    "cmap_normal = plt.Normalize(0, ncategories)\n",
    "\n",
    "for i, annotation in enumerate(annodict[imageid]):\n",
    "  bbox = annotation['bbox']\n",
    "  \n",
    "  # plt.scatter(*rot_center)\n",
    "  # plt.scatter(*corner_points.T, c='r')\n",
    "  \n",
    "  ax = plt.subplot(gs[0])\n",
    "  color = plt.cm.jet(cmap_normal(annotation['category_id']))\n",
    "  rect = patches.Rectangle((bbox[1]-bbox[3]/2 , bbox[0]-bbox[2]/2), bbox[3], bbox[2], linewidth=1, edgecolor=color, facecolor='none')\n",
    "  t = plt_transforms.Affine2D().rotate_around(bbox[1], bbox[0], -bbox[4]+np.pi/2)\n",
    "  rect.set_transform(t + plt.gca().transData)\n",
    "  ax.add_patch(rect)\n",
    "\n",
    "  plt.subplot(gs[i+1])\n",
    "  rotated_im = extract_subimg_bbox(im, bbox)\n",
    "  plt.imshow(rotated_im)  \n",
    "  plt.axis('off')\n",
    "  plt.title(annotation['category_id'])\n",
    "  \n",
    "plt.colorbar(ticks=range(ncategories), label='category')\n",
    "plt.clim(-0.5, ncategories-.5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B4DQwy8VN10f"
   },
   "source": [
    "#create a dict mapping category id to all subimages, can take some time to run\n",
    "cat_imgdict = defaultdict(list) \n",
    "for img_id, image in imgdict.items(): \n",
    "  for annotation in annodict[img_id]:\n",
    "    bbox = annotation['bbox']\n",
    "    subimg = extract_subimg_bbox(image['image'], bbox)\n",
    "    cat_imgdict[annotation['category_id']].append(subimg.copy())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WSLsAg1cgCuN"
   },
   "source": [
    "#How many images are in each category?\n",
    "for k, v  in cat_imgdict.items():\n",
    "  print(f\"Category ID {k} has {len(v)} items\") #f-strings are neat - see https://realpython.com/python-f-strings/ "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eLr_uSeooax3"
   },
   "source": [
    "#Plot some examples from each category \n",
    "for catid, examples in cat_imgdict.items():\n",
    "  num_examples = 5\n",
    "  gs = gridspec.GridSpec(1, num_examples)\n",
    "  plt.figure()\n",
    "  for i, example in enumerate(examples[:num_examples]):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(example)\n",
    "    plt.suptitle(f\"{category_names[catid]} {catid}\")\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCJm137Qs7qp"
   },
   "source": [
    "# E1. Object classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6NDvqVtqDPx"
   },
   "source": [
    "## Setting up for our first challenge: <a name=\"nutsetup\"></a>\n",
    "The challenge of detecting hetergogenously sized objects scattered throughout an image can be challenging, so let's start with something simpler - detecting whether a fixed sized image contains a nut or is blank."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TaGtO0TmniAi"
   },
   "source": [
    "#Start with fixed sized patches that either have a screw or not\n",
    "use_categories = [7, 10]\n",
    "#for screw patches use categories that have smaller bounding boxes\n",
    "patch_size = np.array((128, 128))\n",
    "num_patches_per_category = 500\n",
    "\n",
    "nut_patches = []\n",
    "for img_id, image in imgdict.items(): \n",
    "  for annotation in annodict[img_id]:\n",
    "    if annotation['category_id'] in use_categories:\n",
    "      bbox = annotation['bbox']\n",
    "      rot_center, width, height, theta = unpack_bbox(bbox)\n",
    "      subimg = extract_subimg(image['image'], rot_center, patch_size[0], patch_size[1], 0)\n",
    "      if all(subimg.shape[:2] == patch_size): \n",
    "        nut_patches.append(subimg)\n",
    "      # plt.figure()\n",
    "      # plt.imshow(subimg)\n",
    "\n",
    "  if len(nut_patches) >= num_patches_per_category:\n",
    "    break\n",
    "\n",
    "#Select random blank patches \n",
    "blank_patches = []\n",
    "for i in range(len(nut_patches)):\n",
    "  while True: #until a suitable random patch is found\n",
    "    #choose random image \n",
    "    imgid, imgobj = random.choice(list(imgdict.items()))\n",
    "    im = imgobj['image']\n",
    "    #choose random place at least half a patch size from edges \n",
    "    rand_center = np.random.randint((patch_size//2), np.array(im.shape)[:2] - patch_size//2)\n",
    "    corners = rotcorners_from_coords(rand_center, patch_size[0], patch_size[1], 0)\n",
    "    #check if the random patch intersects with any labeled objects\n",
    "    if not any([Polygon(corners).intersects(Polygon(rotcorners_from_coords(*unpack_bbox(annotation['bbox'])))) for annotation in annodict[imgid]]):\n",
    "      rand_patch = im[rand_center[0]-patch_size[0]//2:rand_center[0]+patch_size[0]//2, rand_center[1]-patch_size[1]//2:rand_center[1]+patch_size[1]//2]\n",
    "      blank_patches.append(rand_patch)\n",
    "      break \n",
    "\n",
    "  # plt.figure()\n",
    "  # plt.imshow(rand_patch)\n",
    "#TODO seems like rarely the patches aren't fully blank - are some labels missing??\n",
    "\n",
    "#could also use some images from cifar etc. \n",
    "num_examples = 10\n",
    "plt.figure()\n",
    "gs = gridspec.GridSpec(2, num_examples, wspace=.05)\n",
    "for i in range(num_examples):\n",
    "  plt.subplot(gs[0, i])\n",
    "  plt.imshow(nut_patches[i])\n",
    "  plt.subplot(gs[1, i])\n",
    "  plt.imshow(blank_patches[i])\n",
    "\n",
    "\n",
    "patch_labels = [1,]*len(nut_patches) + [0,]*len(blank_patches)  #1 if nut\n",
    "all_patches = nut_patches + blank_patches #list concat\n",
    "\n",
    "#randomly shuffle\n",
    "shuffle_idx = np.random.choice(len(patch_labels), len(patch_labels), replace=False)\n",
    "patch_labels = [patch_labels[i] for i in shuffle_idx]\n",
    "all_patches = [all_patches[i] for i in shuffle_idx]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ROO2rdc7ZoDA"
   },
   "source": [
    "#Check shapes are correct\n",
    "# assert all([p.shape == (128,128,3) for p in all_patches])\n",
    "[i for i,p in enumerate(all_patches) if p.shape != (128,128,3)]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwsQacMyp80Y"
   },
   "source": [
    "##Preparing our first network  <a name=\"nutwork\"></a>\n",
    "Before immediately jumping into coding a network, first think about what the structure of the network should look like. \n",
    "Hint - it's often helpful to start thinking about the shape/dimensionality of the inputs and outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import/setup torch\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VyUz8FD52cLs",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "#@title Import/setup torch\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "#check GPU etc. \n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"GPU is not enabled in this notebook. \\n\"\n",
    "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook. \\n\"\n",
    "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "  return device\n",
    "DEVICE = set_device()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BoQjyTBHhLeO"
   },
   "source": [
    "#Preprocess data \n",
    "preprocess = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_frac = .2\n",
    "train_number = int(len(all_patches)*train_frac)\n",
    "# test_nuumber = all_patches.len()-train_number\n",
    "train_patches, train_labels = all_patches[:train_number], patch_labels[:train_number]\n",
    "test_patches, test_labels = all_patches[train_number:], patch_labels[train_number:]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(preprocess(all_patches[0]).permute(1,2, 0)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aA_DCjdw2cOJ"
   },
   "source": [
    "class SimpleScrewNet(nn.Module):  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    LeakyReLU = nn.LeakyReLU()\n",
    "    MaxPool2d = nn.MaxPool2d(2, stride=2)\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=7, stride=2),\n",
    "        LeakyReLU,\n",
    "        MaxPool2d, \n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=5),\n",
    "        # nn.Conv2d(32, 32, kernel_size=5),\n",
    "        LeakyReLU,\n",
    "        MaxPool2d, \n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=5),\n",
    "        LeakyReLU,\n",
    "        MaxPool2d, \n",
    "        \n",
    "        nn.Flatten(1),\n",
    "\n",
    "        nn.Linear(1024, 64),\n",
    "        # nn.Dropout(),\n",
    "        nn.Linear(64, 2),\n",
    "\n",
    "        # nn.Conv2d(3, 6, 5),\n",
    "        # nn.MaxPool2d(2, 2),\n",
    "        # nn.Conv2d(6, 16, 5),\n",
    "        # nn.Linear(16 * 5 * 5, 120),\n",
    "        # nn.Linear(120, 84),\n",
    "        # nn.Linear(84, 2),\n",
    "\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Simply pass the data through the layers\n",
    "    return self.layers(x)  \n",
    "\n",
    "snet = SimpleScrewNet().to(DEVICE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wUOndLKRgSH"
   },
   "source": [
    "#Inspect model structure and layer sizes\n",
    "summary(snet, input_size=(3, 128, 128))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NZXeGRklniCh"
   },
   "source": [
    "#Loss \n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(snet.parameters(),lr=0.000001,momentum=0.3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XBqtNd7yWfTb"
   },
   "source": [
    "#Train \n",
    "num_epoch = 5 \n",
    "train_losses= [] #loss per epoch\n",
    "test_losses= [] #loss per epoch\n",
    "test_corrects = [] #% correct per epoch\n",
    "\n",
    "test_correct = []\n",
    "with torch.no_grad():\n",
    "  for img,lbl in zip(test_patches, test_labels):\n",
    "    img=torch.from_numpy(img).float().permute(2,1,0).unsqueeze(0).cuda()\n",
    "    lbl=torch.torch.as_tensor(lbl).unsqueeze(0).cuda()\n",
    "    predict=snet(img)\n",
    "    test_correct.append((predict.argmax() == lbl).item())\n",
    "test_correct = np.array(test_correct)\n",
    "print(f'Before starting: {test_correct.mean()*100:.2f}% of test images correct')\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "  train_loss=0.0\n",
    "  test_loss=0.0\n",
    "  test_correct = []\n",
    "\n",
    "  snet.train()\n",
    "  # for img,lbl in train_ds_loader:\n",
    "  for img,lbl in zip(train_patches, train_labels):\n",
    "    img=torch.from_numpy(img).float().permute(2,1,0).unsqueeze(0).cuda()\n",
    "    lbl=torch.torch.as_tensor(lbl).unsqueeze(0).cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # print(img.shape)\n",
    "    predict=snet(img)\n",
    "    loss=loss_fn(predict,lbl)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss+=loss.item()*img.size(0)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for img,lbl in zip(test_patches, test_labels):\n",
    "      img=torch.from_numpy(img).float().permute(2,1,0).unsqueeze(0).cuda()\n",
    "      lbl=torch.torch.as_tensor(lbl).unsqueeze(0).cuda()\n",
    "      predict=snet(img)\n",
    "      loss=loss_fn(predict,lbl)\n",
    "      test_loss+=loss.item()*img.size(0)\n",
    "      test_correct.append((predict.argmax() == lbl).item())\n",
    "\n",
    "  test_correct = np.array(test_correct).mean()\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "  test_corrects.append(test_correct)\n",
    "  print('Epoch:{} Train Loss:{:.3f} Test Losss:{:.3f} Percent correct: {:.2f}%'.format(epoch,train_loss,test_loss, test_correct*100)) \n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e0O_QuS9ASFH"
   },
   "source": [
    "#calculate percentage correct\n",
    "correct = []\n",
    "with torch.no_grad():\n",
    "    for img,lbl in zip(test_patches, test_labels):\n",
    "      img=torch.from_numpy(img).float().permute(2,1,0).unsqueeze(0).cuda()\n",
    "      lbl=torch.torch.as_tensor(lbl).unsqueeze(0).cuda()\n",
    "      predict=snet(img)\n",
    "      correct += [(predict.argmax() == lbl).item()]\n",
    "correct = np.array(correct)\n",
    "print(f'{correct.mean()}% of test images correct')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PfOt10IAGT_i"
   },
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yzxeqgjtbKB"
   },
   "source": [
    "##E2. Damaged screw detection<a name=\"damagedscrew\"></a>\n",
    "\n",
    "There's a dataset with anamolous screws on Kaggle - https://www.kaggle.com/ruruamour/screw-dataset \n",
    "\n",
    "Download and inspect the dataset, then setup a network for classification of damaged or normal screw"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kWlv3pLh-K-A"
   },
   "source": [
    "#Here's a code snippet for downloading from kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "\n",
    "#create/download api token from Kaggle -> upper right side -> account -> API -> create API token, and copy it here\n",
    "api_token = {\"username\":\"username here\",\"key\":\"enter api key here\"}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle datasets download -d ruruamour/screw-dataset "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awCBRVBgtgBn"
   },
   "source": [
    "##E3. Multi-class classification <a name=\"multiclass\"></a>\n",
    "So far we've been doing object classification on single classes from the dataset. \n",
    "Change your network to use the 13 classes from the original dataset to classify images of fixed sized into the 13 classes. \n",
    "Note that not all classes are the same sized, so you'll have to use larger image patches and likely change the configuration of your network some. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZRd1hotHAM8K"
   },
   "source": [
    "#Reminder, you can use extract_subimg(im, rot_center, width, height, theta) to extract image patches"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taNbowSlrk9B"
   },
   "source": [
    "##M1. Object detection<a name=\"objectdetection\"></a>\n",
    "\n",
    "Object classification of fixed sized images with a single item is nice, but for many real world tasks detection of multiple objects throughout an image is crucial. Now we will try to create a network for object detection. \n",
    "\n",
    "Here's a useful intro to some of the different types of object classification tasks: \n",
    "https://machinelearningmastery.com/object-recognition-with-deep-learning/\n",
    "\n",
    "First start by thinking about how the network could capture the location of multiple objects - a single classifer layer at the end of the network won't be enough. The [YOLO paper](https://arxiv.org/abs/1506.02640) might be a helpful read as well as [this algorithm comparison](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e). \n",
    "Try to implement your own network (keep in mind for a practice training time you won't be able to use as deep of a network as many of the papers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrPRhUt7wJ_B"
   },
   "source": [
    "##M2. Network performance/introspection <a name=\"inspection\"></a>\n",
    "An important skill for deep learning, as well as any data or programming task is to know how to inspect and debug the performance of your system. \n",
    "Check out what your intermediate layers are actually learning - does this give you any hints to improve your performance? \n",
    "The [W1D2 tutorial](https://deeplearning.neuromatch.io/tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html) might also be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFlm643UrpUK"
   },
   "source": [
    "##M3. Oriented bounding boxes<a name=\"oriented\"></a>\n",
    "The standard yolo just draws bounding boxes, and doesn't handle rotated objects elegantly\n",
    "There's several works that extend [yolo with oriented boxes](https://github.com/feifeiwei/OBB-YOLOv3) or have other network structures that can produce oriented bounding boxes([see here](https://github.com/yijingru/BBAVectors-Oriented-Object-Detection))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgnAR8r5wgVL"
   },
   "source": [
    "##M4. Class clustering <a name=\"clustering\"></a>\n",
    "We've been using provided labels to define our object classes (a form of supervised learning). For many datasets you won't labels or they will be incomplete. \n",
    "\n",
    "Try unsupervised clustering to segment the data into groups. \n",
    "Either classical approaches which [sklearn](https://scikit-learn.org/stable/unsupervised_learning.html) will be very helpful for, or using deep learning approaches ([example 1](https://towardsdatascience.com/image-clustering-implementation-with-pytorch-587af1d14123), [example 2](https://github.com/facebookresearch/deepcluster)).\n",
    "\n",
    "How do the unsupervised clusters compare with the provided labels? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0vj1TCGwgXo"
   },
   "source": [
    "##H1. Perspective and scale<a name=\"perspective\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zw5rJi2uw-gT"
   },
   "source": [
    "##H2. Transfer Learning <a name=\"transfer\"></a>\n",
    "There's many models for object detection/segmentation, for instance: \n",
    "[yolo3 minimal](https://github.com/eriklindernoren/PyTorch-YOLOv3), [yolov5](https://github.com/ultralytics/yolov5), [detetectron2](https://github.com/facebookresearch/detectron2), [Scaled-YOLOv4](https://models.roboflow.com/object-detection/scaled-yolov4)\n",
    "\n",
    "I'd recommend reading the original [YOLO paper](https://arxiv.org/abs/1506.02640) and then starting with [yolo3 minimal](https://github.com/eriklindernoren/PyTorch-YOLOv3) (less performance, but more readable code than the more complicated frameworks)\n",
    "\n",
    "Starting from one of these pretrained networks train it on your screw dataset. How does it's performance compare to your simpler network's performance? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ-pVjVJr8Wu"
   },
   "source": [
    "##Other links: \n",
    "Library of open 3D cad models of bolts etc. \n",
    "\n",
    "https://www.bolts-library.org/en/index.html"
   ]
  }
 ]
}