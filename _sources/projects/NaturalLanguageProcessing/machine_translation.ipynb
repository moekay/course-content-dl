{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "machine_translation.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/NaturalLanguageProcessing/machine_translation.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrjfOU4ieORd"
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Juan Manuel Rodriguez, Salomey Osei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIxkN7e4uO1X"
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCZZzuHYuQjQ"
   },
   "source": [
    "---\n",
    "# Objective\n",
    "\n",
    "The main goal of this project is to train a sequence to sequence NN that transtlate a language into another language, e.g. french to english. This notebook is based on this [Pytorch tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), but change several thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6pQqIlXyy0x"
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R6dc1cXyxK45"
   },
   "source": [
    "# Imports\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eMgWHCkGuOH0"
   },
   "source": [
    "# Download the data\n",
    "import requests, zipfile\n",
    "\n",
    "zip_file_url = 'https://download.pytorch.org/tutorial/data.zip'\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tvCQ8k1RvmRU"
   },
   "source": [
    "N = 10  # print the 10 first lines\n",
    "with open('data/eng-fra.txt') as f:\n",
    "  for i in range(N):\n",
    "    line = next(f).strip()\n",
    "    print(line)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_g71k5Jfkrz"
   },
   "source": [
    "---\n",
    "# Representing the data\n",
    "\n",
    "We create a language representation defining indixes for each word. In addition to the words, our languages have three special tokens:\n",
    "\n",
    "* SOS: Start Of Sentence\n",
    "* EOS: End Of Sentence\n",
    "* PAD: Padding token used to fill inputs vectors where there are no other words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hMkDdjXEYlzb"
   },
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.word2index = {}\n",
    "    self.word2count = {}\n",
    "    self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "    self.n_words = 3  # Count SOS and EOS and PAD\n",
    "\n",
    "  def addSentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.addWord(word)\n",
    "\n",
    "  def addWord(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word2count[word] += 1\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "  return ''.join(\n",
    "      c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn'\n",
    "  )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "  s = unicodeToAscii(s.lower().strip())\n",
    "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "  return s\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "  print(\"Reading lines...\")\n",
    "\n",
    "  # Read the file and split into lines\n",
    "  lines = io.open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "      read().strip().split('\\n')\n",
    "\n",
    "  # Split every line into pairs and normalize\n",
    "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "  # Reverse pairs, make Lang instances\n",
    "  if reverse:\n",
    "      pairs = [list(reversed(p)) for p in pairs]\n",
    "      input_lang = Lang(lang2)\n",
    "      output_lang = Lang(lang1)\n",
    "  else:\n",
    "      input_lang = Lang(lang1)\n",
    "      output_lang = Lang(lang2)\n",
    "\n",
    "  return input_lang, output_lang, pairs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i47NTavyfmvV"
   },
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "  return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "      len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "      p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "  return [pair for pair in pairs if filterPair(pair)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FZrCKd0gfo-i"
   },
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "  print(\"Read %s sentence pairs\" % len(pairs))\n",
    "  pairs = filterPairs(pairs)\n",
    "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "  print(\"Counting words...\")\n",
    "  for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "  print(\"Counted words:\")\n",
    "  print(input_lang.name, input_lang.n_words)\n",
    "  print(output_lang.name, output_lang.n_words)\n",
    "  return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuzulQZEfuDY"
   },
   "source": [
    "## Language word distributions\n",
    "\n",
    "We can check which is the word distribution in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BeyIxcc6gcUY"
   },
   "source": [
    "def plot_lang(lang, top_k=100):\n",
    "  words = list(lang.word2count.keys())\n",
    "  words.sort(key=lambda w: lang.word2count[w], reverse=True)\n",
    "  print(words[:top_k])\n",
    "  count_occurences = sum(lang.word2count.values())\n",
    "\n",
    "  accumulated = 0\n",
    "  counter = 0\n",
    "\n",
    "  while accumulated < count_occurences * 0.8:\n",
    "    accumulated += lang.word2count[words[counter]]\n",
    "    counter += 1\n",
    "\n",
    "  print(f\"The {counter * 100 / len(words)}% most common words \"\n",
    "        f\"account for the {accumulated * 100 / count_occurences}% of the occurrences\")\n",
    "  plt.bar(range(100), [lang.word2count[w] for w in words[:top_k]])\n",
    "  plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1sPwWCs0hf3w"
   },
   "source": [
    "plot_lang(input_lang)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CPCMszrOicLn"
   },
   "source": [
    "plot_lang(output_lang)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69W-IrrCint7"
   },
   "source": [
    "## The RNN\n",
    "\n",
    "Our goal is to create a network that takes an input sentence in one language and then provides the translation of the sentence in an output language.  Our network will use an RNN which will consist of an encoder, and a decoder. The encoder will first transform our input sentence into a vector, and pass this condensed vector into the decoder which will then translate the text in our given language. The process of this is further explained in the diagram below:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/static/seq2seq.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "**Note:** Please note that this same approach can be used for next sentence prediction task"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PlfhVKoWf-Fg"
   },
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    embedded = self.embedding(input)#.view(1, 1, -1)\n",
    "    output = embedded\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self, batch_size):\n",
    "    return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UDC-FMywgOZv"
   },
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    output = self.embedding(input)\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    output = self.softmax(self.out(output))\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfZisthYlKWr"
   },
   "source": [
    "---\n",
    "# Representing the text\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z9A9tfanhsJt"
   },
   "source": [
    "def to_train(input_lang, output_lang, pairs, max_len=MAX_LENGTH+2):\n",
    "  x_input = []\n",
    "  x_output = []\n",
    "  target = []\n",
    "  for i, o in pairs:\n",
    "    s_i = [2] * max_len + [0] + [input_lang.word2index[w] for w in i.split(\" \")] + [1] \n",
    "    s_o = [0] + [output_lang.word2index[w] for w in o.split(\" \")] + [1] + [2] * max_len\n",
    "    s_to = s_o[1:] + [2]\n",
    "    x_input.append(s_i[-max_len:])\n",
    "    x_output.append(s_o[:max_len])\n",
    "    target.append(s_to[:max_len])\n",
    "  return x_input, x_output, target"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MfyjPPFEjkYZ"
   },
   "source": [
    "x_input, x_partial, y = to_train(input_lang, output_lang, pairs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IUDhs4ljlUGy"
   },
   "source": [
    "print('Representation of an input sentece:')\n",
    "print(x_input[0])\n",
    "print(' '.join([input_lang.index2word[w] for w in x_input[0]]))\n",
    "print('\\nRepresentation of an partial sentece:')\n",
    "print(x_partial[0])\n",
    "print(' '.join([output_lang.index2word[w] for w in x_partial[0]]))\n",
    "print('\\nRepresentation of an target sentece:')\n",
    "print(y[0])\n",
    "print(' '.join([output_lang.index2word[w] for w in y[0]]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR4GHXfRmKOT"
   },
   "source": [
    "We represent the input sentence using left padding because the GRU network process the sentence left to rignt, and we want that the output is as close to our sentence as possible. In contrast, we use right padding to the partial translation sentence because we want that our context is process inmediatly by our decoder. Finally, our target is our partial translation left-shifted.\n",
    "\n",
    "## Training\n",
    "Using this representation, we can train our model. Notice that our feed the full sentences as partial translations instanted of feeding partial sentences. This speed-ups our training, as next words in the sentence do not affects the output of the network and the gradients up to that point. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "APcsuSycgfop"
   },
   "source": [
    "def predict(encoder, decoder, input, output):\n",
    "  _, hidden = encoder(input, encoder.initHidden(input.shape[0]))\n",
    "  out, _ = decoder(output, hidden)\n",
    "  return out\n",
    "\n",
    "def train(encoder, decoder, loss, input, output, target, learning_rate=0.001, epochs=10, batch_size=100):\n",
    "\n",
    "  plot_losses = []\n",
    "  plot_full_losses = []\n",
    "\n",
    "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    c_input, c_output, c_target = shuffle(input, output, target)\n",
    "    c_input = torch.tensor(c_input, dtype=torch.long, device=device)\n",
    "    c_output = torch.tensor(c_output, dtype=torch.long, device=device)\n",
    "    c_target = torch.tensor(c_target, dtype=torch.long, device=device)\n",
    "    acc_loss = 0\n",
    "    for i in range(0, c_target.shape[0], batch_size):\n",
    "      c_batch_size = c_target[i:i+batch_size, ...].shape[0]\n",
    "      encoder_optimizer.zero_grad()\n",
    "      decoder_optimizer.zero_grad()\n",
    "\n",
    "      out = predict(encoder, decoder, c_input[i:i+batch_size, ...], c_output[i:i+batch_size, ...])\n",
    "      #Reshapes the output and target to use the expected loss format. \n",
    "      # N x Classes for the output\n",
    "      # N for the targets\n",
    "      # Where N is the batch size\n",
    "      out = out.reshape(c_batch_size * c_input.shape[1], -1)\n",
    "      r_target = c_target[i:i+batch_size, ...].reshape(c_batch_size * c_input.shape[1])\n",
    "\n",
    "      c_loss = loss(out, r_target)\n",
    "      # Mask the errors for padding as they are not usefull!\n",
    "      valid = torch.where(r_target == 2, 0, 1)\n",
    "      c_loss = c_loss * valid\n",
    "      c_loss = torch.sum(c_loss) #/ torch.sum(valid)\n",
    "\n",
    "      c_loss.backward()\n",
    "\n",
    "      encoder_optimizer.step()\n",
    "      decoder_optimizer.step()\n",
    "      plot_full_losses.append(c_loss.detach().numpy())\n",
    "      acc_loss += c_loss.detach().numpy()\n",
    "    plot_losses.append(acc_loss /math.ceil(c_target.shape[0] / batch_size))\n",
    "  return plot_losses, plot_full_losses"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DNIiOhdfwKRC"
   },
   "source": [
    "hidden_size = 300\n",
    "num_epochs = 10  # Change this to 50 (original value!)\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words)\n",
    "epoch_error, batch_error = train(encoder, decoder,\n",
    "                                 nn.NLLLoss(reduction='none'),\n",
    "                                 x_input, x_partial, y,\n",
    "                                 epochs=num_epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "osLoVaf30RdC"
   },
   "source": [
    "#print(epoch_error)\n",
    "#print(batch_error)\n",
    "\n",
    "plt.plot(batch_error)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_error)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tK6nXTwqTJr"
   },
   "source": [
    "---\n",
    "# Prediction and generation\n",
    "\n",
    "In the following cells, we can can see how our Seq2Seq model produces a prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Oh9JbyF9cd6"
   },
   "source": [
    "p = predict(encoder, decoder, torch.tensor([x_input[40]],\n",
    "                                           dtype=torch.long,\n",
    "                                           device=device),\n",
    "            torch.tensor([x_partial[40]], dtype=torch.long, device=device))\n",
    "\n",
    "p = p.detach().numpy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Geqg4Ly93NE"
   },
   "source": [
    "print(np.argmax(p, axis=-1))\n",
    "print(x_partial[40])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yxi0OitrEG2"
   },
   "source": [
    "---\n",
    "# Generating a translation\n",
    "\n",
    "The generation is a very simple iterative process:\n",
    "\n",
    "1. Initialize the partiar translation using only the start of sentence token '\n",
    "SOS' (its id, which is 0).\n",
    "1. Repeat:\n",
    "    1. Predict the probability distribution for the next token given the partial translation.\n",
    "    1. Pick the most probable token. (other option is to sample the distribution).\n",
    "    1. Add that token to the translation.\n",
    "    1. If the token is EOF, break the loop.\n",
    "1. Return the partial translation, which is now a full translation.\n",
    "\n",
    "If we want to generate several candidates, we can use other generation algorithm. [Beam Search](https://www.youtube.com/watch?v=RLWuzLLSIgw) is a great option for this."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pk6uUzGY9FZz"
   },
   "source": [
    "def gen_translation(encoder, decoder, text, input_lang, output_lang,\n",
    "                    max_len=MAX_LENGTH+2):\n",
    "\n",
    "  text =  [2] * max_len + [0] + [input_lang.word2index[w] for w in text.split(\" \")] + [1]\n",
    "  text = torch.tensor([text[-max_len:]], dtype=torch.long, device=device)\n",
    "  out = [0] + [2] * max_len\n",
    "  out = [out[:max_len]]\n",
    "  for i in range(1, max_len):\n",
    "    pt_out =torch.tensor(out, dtype=torch.long, device=device)\n",
    "    p = predict(encoder, decoder, text, pt_out).detach().numpy()\n",
    "    out[0][i] = np.argmax(p, axis=-1)[0, i-1]\n",
    "    if np.argmax(p, axis=-1)[0, i-1] == 1:\n",
    "      break\n",
    "\n",
    "  return ' '.join([output_lang.index2word[idx] for idx in out[0]])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LLlak9FU1k6Z"
   },
   "source": [
    "gen_translation(encoder, decoder, pairs[40][0], input_lang, output_lang)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fCDh2CU43eAN"
   },
   "source": [
    "for i in range(40):\n",
    "  print('> {}'.format(pairs[i][0]))\n",
    "  print('= {}'.format(pairs[i][1]))\n",
    "  print('< {}'.format(gen_translation(encoder, decoder,\n",
    "                                      pairs[i][0],\n",
    "                                      input_lang,\n",
    "                                      output_lang)))\n",
    "  print('*' * 40)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3m2Qiq9odKAD"
   },
   "source": [
    "for i in range(40):\n",
    "  print('> {}'.format(pairs[-i][0]))\n",
    "  print('= {}'.format(pairs[-i][1]))\n",
    "  print('< {}'.format(gen_translation(encoder, decoder,\n",
    "                                      pairs[-i][0],\n",
    "                                      input_lang,\n",
    "                                      output_lang)))\n",
    "  print('*' * 40)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1IzAEWktgrQ"
   },
   "source": [
    "---\n",
    "# To dos\n",
    "\n",
    "1. We use the full dataset to train/test. This is not a great idea, you should split the dataset into training/test.\n",
    "2. We did some empirical evaluation looking at the translated senteces. Other evaluation can be done using metrics like [BLUE](https://www.nltk.org/api/nltk.translate.html?highlight=bleu_score#module-nltk.translate.bleu_score) score.\n",
    "3. We try it with languages that are writting in a left-rigth as input and output. What happens if the languages are not written in this way? [Datasets](https://www.manythings.org/anki/) [Even more](https://tatoeba.org/en/downloads) \n",
    "4. It would be possible to do machine translation using other NN architectures, such as attention based model. \n",
    "5. We are not handling proper nouns, and that could be a problem.\n",
    "6. This can be applied to next sentence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEysFWQo3ECG"
   },
   "source": [
    "---\n",
    "# Further reading\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "* [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)"
   ]
  }
 ]
}