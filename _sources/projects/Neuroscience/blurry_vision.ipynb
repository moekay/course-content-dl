{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/Neuroscience/blurry_vision.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Vision with Lost Glasses: Modelling how the brain deals with noisy input\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Rishika Mohanta, Furkan Özçelik, Salomey Osei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Objective\n",
    "\n",
    "Imagine you lost your spectacle and the world around you is completely blurred out. As you stumble around, you see a small animal walk towards you. Can you figure out what it is? Probably yes right?\n",
    "\n",
    "In this situation, or in foggy/night-time conditions, visual input is of poor quality; images are blurred and have low contrast and yet our brains manage to recognize it. Is it possible to model the process? Does previous experience help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install torch_intermediate_layer_getter --quiet\n",
    "!pip install Pillow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from shutil import copyfile, rmtree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "# For getting output of intermediate layers in Pytorch\n",
    "from torch_intermediate_layer_getter import IntermediateLayerGetter as LayerGetter\n",
    "# For interactive visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helpers functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helpers functions\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**The Asirra dataset**\n",
    "\n",
    "The CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof) challenge is the motivation behind the creation of this dataset.\n",
    "\n",
    "\n",
    "Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately.\n",
    "\n",
    "*Reference: Dataset can be found here* (https://www.kaggle.com/c/dogs-vs-cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download Cleaned Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Download Cleaned Data\n",
    "\n",
    "# @markdown This particular dataset is a cleaned and processed version of Kaggle's Dogs vs. Cats Competition. The Data is organised as three folders `dataset`, `dataset_blur_2`, `dataset_blur_5`.\n",
    "# @markdown Each folder has a `train` and `test` subfolder and each subfolder has a `cat` and `dog` folder which contain the images.\n",
    "\n",
    "\n",
    "# @markdown `dataset` contains the clear images\n",
    "\n",
    "# @markdown `dataset_blur_2` contains the images with a Gaussian Blur (radius = 2)\n",
    "\n",
    "# @markdown `dataset_blur_5` contains the images with a Gaussian Blur (radius = 5)\n",
    "\n",
    "import os, zipfile, requests\n",
    "\n",
    "filenames = [\"catvdog_clear.zip\",\n",
    "             \"catvdog_blur_2.zip\",\n",
    "             \"catvdog_blur_5.zip\"]\n",
    "\n",
    "urls = [\"https://osf.io/hj2gd/download\",\n",
    "        \"https://osf.io/xp6qd/download\",\n",
    "        \"https://osf.io/wj43a/download\"]\n",
    "for fname, url in zip(filenames, urls):\n",
    "  if not os.path.isfile(fname):\n",
    "    try:\n",
    "      r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "      else:\n",
    "        with open(fname, \"wb\") as fid:\n",
    "          fid.write(r.content)\n",
    "\n",
    "for fname in filenames:\n",
    "  zip_ref = zipfile.ZipFile(fname, 'r')\n",
    "  zip_ref.extractall()\n",
    "  zip_ref.close()\n",
    "  os.remove(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Define preprocessing pipeline and dataloaders\n",
    "Here we only load the dataset with clear images and gaussian blur with radius 5. You can explore the other by loading the appropriate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define Preprocessing Filters\n",
    "preprocessing = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Resize((256, 256)),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                         (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Create Clean Training Dataset using ImageFolder\n",
    "clear_train_data = torchvision.datasets.ImageFolder(\n",
    "    root=\"dataset/train\",\n",
    "    transform=preprocessing\n",
    ")\n",
    "\n",
    "# Create Clean Test Dataset using ImageFolder\n",
    "clear_test_data = torchvision.datasets.ImageFolder(\n",
    "    root=\"dataset/test\",\n",
    "    transform=preprocessing\n",
    ")\n",
    "\n",
    "# Create Noisy Training Dataset using ImageFolder\n",
    "noisy_train_data = torchvision.datasets.ImageFolder(\n",
    "    root=\"dataset_blur_5/train\",\n",
    "    transform=preprocessing\n",
    ")\n",
    "\n",
    "# Create Noisy Test Dataset using ImageFolder\n",
    "noisy_test_data = torchvision.datasets.ImageFolder(\n",
    "    root=\"dataset_blur_5/test\",\n",
    "    transform=preprocessing\n",
    ")\n",
    "\n",
    "# function to apply a training-validation set split on a dataset\n",
    "def validation_split(train_data, val_ratio = 0.2):\n",
    "  train_indices, val_indices, _, _ = train_test_split(range(len(train_data)),\n",
    "                                                      train_data.targets,\n",
    "                                                      stratify=train_data.targets,\n",
    "                                                      test_size=val_ratio)\n",
    "  train_split = torch.utils.data.Subset(train_data, train_indices)\n",
    "  val_split = torch.utils.data.Subset(train_data, val_indices)\n",
    "  return train_split,val_split\n",
    "\n",
    "# Define Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# Define Dataloaders for Training, Validation and Test sets\n",
    "clear_train_split,clear_val_split = validation_split(clear_train_data)\n",
    "clear_train_batches = torch.utils.data.DataLoader(clear_train_split,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True)\n",
    "clear_val_batches = torch.utils.data.DataLoader(clear_val_split,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "clear_test_batches = torch.utils.data.DataLoader(clear_test_data,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "noisy_train_split,noisy_val_split = validation_split(noisy_train_data)\n",
    "noisy_train_batches = torch.utils.data.DataLoader(noisy_train_split,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True)\n",
    "noisy_val_batches = torch.utils.data.DataLoader(noisy_val_split,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "noisy_test_batches = torch.utils.data.DataLoader(noisy_test_data,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Get an example of a clear and noisy versions of cat and dog image\n",
    "clear_cat_image = clear_train_data[5][0].unsqueeze(0)\n",
    "clear_dog_image = clear_train_data[19997][0].unsqueeze(0)\n",
    "noisy_cat_image = noisy_train_data[5][0].unsqueeze(0)\n",
    "noisy_dog_image = noisy_train_data[19997][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Plot examples of Noisy and Noise-free Images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "ax=fig.add_subplot(141)\n",
    "ax.imshow(clear_cat_image.squeeze(0).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.title('Cat (Noise-free)')\n",
    "ax=fig.add_subplot(142)\n",
    "ax.imshow(clear_dog_image.squeeze(0).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.title('Dog (Noise-free)')\n",
    "ax=fig.add_subplot(143)\n",
    "ax.imshow(noisy_cat_image.squeeze(0).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.title('Cat (Noisy)')\n",
    "ax=fig.add_subplot(144)\n",
    "ax.imshow(noisy_dog_image.squeeze(0).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.title('Dog (Noisy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Deep Learning Models of the Ventral Visual Stream\n",
    "\n",
    "AlexNet and other Deep Convolutional Neural networks are considered to be a good model of the ventral visual stream for visual categorization. Studies have tried to compare different layers of the Deep Neural Networks with the activity in different brain regions. See Below.\n",
    "\n",
    "<img src=\"https://www.biorxiv.org/content/biorxiv/early/2020/01/02/407007/F1.large.jpg\" width=\"500\" />\n",
    "\n",
    "Figure from [Schrimpf et al. 2020](https://www.biorxiv.org/content/10.1101/407007v2.full)\n",
    "\n",
    "Lets try to model the visual categorisation task using AlexNet as model of the ventral visual stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Create Test Neural Network (AlexNet, 2012 with Batch Normalization and a downscaling factor)\n",
    "\n",
    "We add Batch Normalization to make training faster and add a scaling factor to test the architechture in a relatively smaller and computationally feasible model.\n",
    "The parameter `downscale` scales down the number of channels in the network by the factor given by the value. For computational feasibility on Google Colab we use an Alexnet downscaled by a factor of 2.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*bD_DMBtKwveuzIkQTwjKQQ.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define AlexNet with different modules representing different brain areas\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes=1000, downscale=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      num_classes: int\n",
    "      downscale: int\n",
    "    \"\"\"\n",
    "    super(AlexNet, self).__init__()\n",
    "    self.retina = nn.Sequential(\n",
    "        nn.Conv2d(3, 64//downscale, kernel_size=11, stride=4, padding=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(64//downscale),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    )\n",
    "    self.lgn = nn.Sequential(\n",
    "        nn.Conv2d(64//downscale, 192//downscale, kernel_size=5, padding=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(192//downscale),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    )\n",
    "    self.v1 = nn.Sequential(\n",
    "        nn.Conv2d(192//downscale, 384//downscale, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(384//downscale),\n",
    "    )\n",
    "    self.v2 = nn.Sequential(\n",
    "        nn.Conv2d(384//downscale, 256//downscale, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(256//downscale),\n",
    "    )\n",
    "    self.v4 = nn.Sequential(\n",
    "        nn.Conv2d(256//downscale, 256//downscale, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(256//downscale),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    )\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "    self.it = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(256//downscale * 6 * 6, 4096//downscale),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096//downscale, 4096//downscale),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    self.classifier = nn.Linear(4096//downscale, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: torch.Tensor\n",
    "    Returns:\n",
    "      x: torch.Tensor\n",
    "    \"\"\"\n",
    "    x = self.retina(x)\n",
    "    x = self.lgn(x)\n",
    "    x = self.v1(x)\n",
    "    x = self.v2(x)\n",
    "    x = self.v4(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.it(x)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Visualize the Neural Network using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Create an neural network object and visualise it with an example image\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "writer = SummaryWriter('runs/AlexNet')\n",
    "writer.add_graph(net, clear_dog_image)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# define function for running some epochs of training\n",
    "def train(num_epochs, train_batch, val_batch,\n",
    "          training_losses=None, validation_losses=None, device='cpu'): #Training\n",
    "\n",
    "  net.train()\n",
    "  if training_losses is None:\n",
    "    training_losses = []\n",
    "  if validation_losses is None:\n",
    "    validation_losses = []\n",
    "  for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch_idx, (data, target) in enumerate(train_batch):\n",
    "      data = data.to(device).float()\n",
    "      target = target.to(device).long()\n",
    "      # reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "      # forward pass + backward pass + optimize\n",
    "      prediction = net(data)\n",
    "      loss = criterion(prediction, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      training_losses += [loss.item()]\n",
    "    for batch_idx, (data, target) in enumerate(val_batch):\n",
    "      data = data.to(device).float()\n",
    "      target = target.to(device).long()\n",
    "      # forward pass only\n",
    "      prediction = net(data)\n",
    "      loss = criterion(prediction, target)\n",
    "      validation_losses += [loss.item()]\n",
    "  return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# define function to calculate current accuracy with a given dataloader\n",
    "def accuracy(dataloader, device='cpu'): #Get the accuracies\n",
    "  net.eval()\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for data, target in tqdm.tqdm(dataloader):\n",
    "    data = data.to(device).float()\n",
    "    target = target.to(device).long()\n",
    "\n",
    "    prediction = net(data)\n",
    "    _, predicted = torch.max(prediction, 1)\n",
    "    count += target.size(0)\n",
    "    correct += (predicted == target).sum().item()\n",
    "\n",
    "  acc = 100 * correct / count\n",
    "  return count, acc\n",
    "\n",
    "\n",
    "# define function to evaluate and print training and test accuracy\n",
    "def evaluate(net, device='cpu', title=\"\"):\n",
    "  net.eval()\n",
    "  train_count, train_acc = accuracy(clear_train_batches, device=device)\n",
    "  test_count, test_acc = accuracy(clear_test_batches, device=device)\n",
    "  print(f'Accuracy on the {train_count} clear training samples {title}: {train_acc:0.2f}')\n",
    "  print(f'Accuracy on the {test_count} clear testing samples {title}: {test_acc:0.2f}')\n",
    "  train_count, train_acc = accuracy(noisy_train_batches, device=device)\n",
    "  test_count, test_acc = accuracy(noisy_test_batches, device=device)\n",
    "  print(f'Accuracy on the {train_count} blurry training samples {title}: {train_acc:0.2f}')\n",
    "  print(f'Accuracy on the {test_count} blurry testing samples {title}: {test_acc:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Hypothesis:\n",
    "$H_0$: The visual system model with experience of seeing images without blur will have a much better performance on the blurry images out of the box compared to the naive learner.\n",
    "\n",
    "$H_1$: The visual system model with experience of seeing images without blur will have a better performance on the blurry images after training compared to the naive learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Naive Learner\n",
    "\n",
    "A naive learner model looks at only the blurry images and tries to learn the difference between a cat and a dog \n",
    "\n",
    "*Warning: Training takes approximately 2-3 minutes per epoch so choose the number of epochs with that in mind. Here the we run only one epoch as an example, try running 5-10 epochs for good performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define network, loss function and optimizer\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "net.to(DEVICE)\n",
    "\n",
    "# Evaluate before training\n",
    "evaluate(net, device=DEVICE, title=\"before training\")\n",
    "\n",
    "# Define number of epochs\n",
    "num_training_epochs = 1\n",
    "\n",
    "# Save network weights\n",
    "torch.save(net.state_dict(), \"naive_before_training\")\n",
    "\n",
    "# Training loop\n",
    "naive_training_losses, naive_validation_losses = train(num_training_epochs,\n",
    "                                                       noisy_train_batches,\n",
    "                                                       noisy_val_batches,\n",
    "                                                       device=DEVICE)\n",
    "\n",
    "# Save network weights\n",
    "torch.save(net.state_dict(), \"naive_after_training\")\n",
    "\n",
    "# Evaluate after training\n",
    "evaluate(net, device=DEVICE, title=\"after training\")\n",
    "\n",
    "# Plot Loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, num_epochs + 1),\n",
    "         [np.mean(x) for x in np.array_split(naive_training_losses,\n",
    "                                             num_training_epochs)],\n",
    "         \"o-\", label=\"Training Loss\")\n",
    "plt.plot(np.arange(1, num_epochs + 1),\n",
    "         [np.mean(x) for x in np.array_split(naive_validation_losses,\n",
    "                                             num_training_epochs)],\n",
    "         \"o-\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Expert Experienced Learner\n",
    "\n",
    "A expert learner model first learns to look at less noisy images of cats and dogs and then try to distinguish the blurry images. It then learns on blurry images and tries to understand the difference between a cat and a dog \n",
    "\n",
    "*Warning: Pretraining takes approximately 5-6 minutes and Training takes 2-3 minutes per epoch so choose the number of epochs with that in mind.Here the we run only one epoch as an example, try running 5-10 epochs for both pretraining and training for good performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define network, loss function and optimizer\n",
    "net = AlexNet(num_classes=2,downscale=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "net.to(DEVICE)\n",
    "\n",
    "# Evaluate before pretraining\n",
    "evaluate(net, device=DEVICE, title=\"before training\")\n",
    "\n",
    "# Define number of epochs\n",
    "num_pretraining_epochs = 1\n",
    "num_training_epochs = 1\n",
    "num_epochs = num_pretraining_epochs + num_training_epochs\n",
    "\n",
    "# Save network weights\n",
    "torch.save(net.state_dict(), \"expert_before_training\")\n",
    "\n",
    "# Pretraining loop\n",
    "training_losses, validation_losses = train(num_pretraining_epochs,\n",
    "                                           clear_train_batches,\n",
    "                                           clear_val_batches,\n",
    "                                           device=DEVICE)\n",
    "\n",
    "# Evaluate after pretraining\n",
    "evaluate(net, device=DEVICE, title=\"after pretraining\")\n",
    "\n",
    "# Save network weights\n",
    "torch.save(net.state_dict(), \"expert_after_pretraining\")\n",
    "\n",
    "# Training loop\n",
    "experienced_training_losses, experienced_validation_losses = train(num_training_epochs,\n",
    "                                                                   noisy_train_batches,\n",
    "                                                                   noisy_val_batches,\n",
    "                                                                   training_losses=training_losses,\n",
    "                                                                   validation_losses=validation_losses,\n",
    "                                                                   device=DEVICE)\n",
    "\n",
    "# Save network weights\n",
    "torch.save(net.state_dict(), \"expert_after_training\")\n",
    "\n",
    "# Evaluate after training\n",
    "evaluate(net, device=DEVICE, title=\"after training\")\n",
    "\n",
    "# Plot Loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, num_epochs + 1),\n",
    "         [np.mean(x) for x in np.array_split(experienced_training_losses,\n",
    "                                             num_epochs)],\n",
    "         \"o-\", label=\"Training Loss\")\n",
    "plt.plot(np.arange(1, num_epochs + 1),\n",
    "         [np.mean(x) for x in np.array_split(experienced_validation_losses,\n",
    "                                             num_epochs)],\n",
    "         \"o-\", label=\"Validation Loss\")\n",
    "plt.axvline(num_pretraining_epochs, linestyle='dashed', color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Wow thats interesting! The model of the visual system has a hard time to distinguish between blurry cats and dogs even after having seen images of cats and dog before! It takes further training on the blurry images to be able to learn the difference well. Even then, it is only a little better than the naive model. Would it be same for a human being?\n",
    "\n",
    "Now, if you want to understand thats happening during training and pretraining, is there a way to visualise the effect of on the network itself? Yes!! You can look at either how: \n",
    "\n",
    "1.   Convolution kernels, OR\n",
    "2.   Average output of Intermediate layers\n",
    "\n",
    "have changed over pretraining and training to try and understand how the networks learns to look for more specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Looking at Convolution Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper Function to plot the 16 filters of the first layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper Function to plot the 16 filters of the first layer\n",
    "def plot_filter(net, title=\"\"):\n",
    "  layer = 0\n",
    "  with torch.no_grad():\n",
    "    params = list(net.parameters())\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(4, 4))\n",
    "    filters = []\n",
    "    for filter_index in range(min(16, params[layer].shape[0])):\n",
    "      row_index = filter_index // 4\n",
    "      col_index = filter_index % 4\n",
    "\n",
    "      filter = params[layer][filter_index, :, :, :]\n",
    "      filter_image = filter.permute((1,2,0))\n",
    "      scale = np.abs(filter_image).max()\n",
    "      scaled_image = filter_image / (2 * scale) + 0.5\n",
    "      filters.append(scaled_image)\n",
    "      axs[row_index, col_index].imshow(scaled_image)\n",
    "      axs[row_index, col_index].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('naive_before_training'))\n",
    "plot_filter(net, \"Before Training (Naive Model)\")\n",
    "\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('naive_after_training'))\n",
    "plot_filter(net, \"After Training (Naive Model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Expert Experienced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('expert_before_training'))\n",
    "plot_filter(net, \"Before Training (Expert Model)\")\n",
    "\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('expert_after_pretraining'))\n",
    "plot_filter(net, \"After Pretraining (Expert Model)\")\n",
    "\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('expert_after_training'))\n",
    "plot_filter(net, \"After Training (Expert Model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Looks like with the small amount of training, the filters are not very meaningful. Also, there seem to be only very small differences are visible at the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Looking at Intermediate Layer Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# choose intermidiate layers from which to get the output\n",
    "return_layers = {\n",
    "    # \"<name of layer in the AlexNet Class>\" : \"<key for the layer output in the returned dictionary>\"\n",
    "      'v1': 'v1',\n",
    "      'v2': 'v2',\n",
    "      'v4': 'v4',\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper functions to get and plot intermediate layer output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper functions to get and plot intermediate layer output\n",
    "\n",
    "def plot_intermediate_layers(image, net, return_layers=return_layers):\n",
    "  # Get output for the image from the intermediate layers\n",
    "  intermediate_output = LayerGetter(net, return_layers=return_layers)(image)\n",
    "  fig = plt.figure(figsize=(12, 3))\n",
    "  ax=fig.add_subplot(141)\n",
    "  ax.imshow(image.squeeze(0).permute(1, 2, 0))\n",
    "  plt.axis('off')\n",
    "  plt.title('Original Image')\n",
    "  ax=fig.add_subplot(142)\n",
    "  ax.imshow(intermediate_output[0]['v1'].detach().cpu().squeeze(0).mean(axis=0))\n",
    "  plt.axis('off')\n",
    "  plt.title('V1 layer (Average)')\n",
    "  ax=fig.add_subplot(143)\n",
    "  ax.imshow(intermediate_output[0]['v2'].detach().cpu().squeeze(0).mean(axis=0))\n",
    "  plt.axis('off')\n",
    "  plt.title('V2 layer (Average)')\n",
    "  ax=fig.add_subplot(144)\n",
    "  ax.imshow(intermediate_output[0]['v4'].detach().cpu().squeeze(0).mean(axis=0))\n",
    "  plt.axis('off')\n",
    "  plt.title('V4 layer (Average)')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(\"Naive Model\\n=============\")\n",
    "print(\"Before Training\")\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('naive_before_training'))\n",
    "plot_intermediate_layers(noisy_dog_image, net)\n",
    "plot_intermediate_layers(noisy_cat_image, net)\n",
    "\n",
    "print(\"After Training\")\n",
    "net = AlexNet(num_classes=2, downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('naive_after_training'))\n",
    "plot_intermediate_layers(noisy_dog_image, net)\n",
    "plot_intermediate_layers(noisy_cat_image, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Over training, the naive model seems to have learnt more complex features as apparent from the intermediate layer outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Expert Experienced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(\"Expert Model\\n=============\")\n",
    "print(\"Before Training\")\n",
    "net = AlexNet(num_classes=2,downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('expert_before_training'))\n",
    "plot_intermediate_layers(noisy_dog_image,net)\n",
    "plot_intermediate_layers(noisy_cat_image,net)\n",
    "\n",
    "print(\"After Pretraining\")\n",
    "net = AlexNet(num_classes=2,downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('expert_after_pretraining'))\n",
    "plot_intermediate_layers(noisy_dog_image,net)\n",
    "plot_intermediate_layers(noisy_cat_image,net)\n",
    "\n",
    "print(\"After Training\")\n",
    "net = AlexNet(num_classes=2,downscale=2)\n",
    "# Load and assign previous state\n",
    "net.load_state_dict(torch.load('expert_after_training'))\n",
    "plot_intermediate_layers(noisy_dog_image,net)\n",
    "plot_intermediate_layers(noisy_cat_image,net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Clearly, the network output is only somewhat similar between pretraining and training. The differences seem to suggest that the process of learning over blurred images did change the features the network focused which probably contibuted to improved response to blurry images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown If you wish to generate cat and dog images with other degrees of blur or different type of noise\n",
    "# @markdown refer to this cell which contains the code for dataset cleaning and filtering the original dataset.\n",
    "# @markdown Feel free to uncomment and change the code to generate other variations of the dataset.\n",
    "\n",
    "# @markdown *Note: The dataset generation process can take a while to run.*\n",
    "\n",
    "\n",
    "# # Download the Data\n",
    "# if \"cats-and-dogs.zip\" not in os.listdir():\n",
    "#   !wget --no-check-certificate \\\n",
    "#     \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
    "#     -O \"cats-and-dogs.zip\"\n",
    "\n",
    "# local_zip = 'cats-and-dogs.zip'\n",
    "# zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
    "# zip_ref.extractall()\n",
    "# zip_ref.close()\n",
    "\n",
    "# cat_folder = \"PetImages/Cat/\"\n",
    "# dog_folder = \"PetImages/Dog/\"\n",
    "\n",
    "# def check_file(file):\n",
    "#   try:\n",
    "#     im = Image.open(file).convert('RGB')\n",
    "#     #im = im.filter(ImageFilter.BLUR)\n",
    "#     return True\n",
    "#   except:\n",
    "#     return False\n",
    "\n",
    "# cat_files = list(filter(check_file, [cat_folder+i for i in os.listdir(cat_folder)]))\n",
    "# dog_files = list(filter(check_file, [dog_folder+i for i in os.listdir(dog_folder)]))\n",
    "# np.random.shuffle(cat_files)\n",
    "# np.random.shuffle(dog_files)\n",
    "\n",
    "# # Train test split\n",
    "# test_ratio = 0.1\n",
    "# # assert len(cat_files) == len(dog_files)\n",
    "# N = min(len(cat_files),len(dog_files))\n",
    "\n",
    "# training_length = int((1-test_ratio)*N)\n",
    "# train_indices = np.arange(training_length)\n",
    "# test_indices = np.arange(training_length,N)\n",
    "\n",
    "# # Create data directory\n",
    "# try:\n",
    "#   os.mkdir(\"dataset\")\n",
    "#   os.mkdir(\"dataset/train/\")\n",
    "#   os.mkdir(\"dataset/test/\")\n",
    "#   os.mkdir(\"dataset/train/cat/\")\n",
    "#   os.mkdir(\"dataset/train/dog/\")\n",
    "#   os.mkdir(\"dataset/test/cat/\")\n",
    "#   os.mkdir(\"dataset/test/dog/\")\n",
    "#   os.mkdir(\"dataset_blur_5\")\n",
    "#   os.mkdir(\"dataset_blur_5/train/\")\n",
    "#   os.mkdir(\"dataset_blur_5/test/\")\n",
    "#   os.mkdir(\"dataset_blur_5/train/cat/\")\n",
    "#   os.mkdir(\"dataset_blur_5/train/dog/\")\n",
    "#   os.mkdir(\"dataset_blur_5/test/cat/\")\n",
    "#   os.mkdir(\"dataset_blur_5/test/dog/\")\n",
    "#   os.mkdir(\"dataset_blur_2\")\n",
    "#   os.mkdir(\"dataset_blur_2/train/\")\n",
    "#   os.mkdir(\"dataset_blur_2/test/\")\n",
    "#   os.mkdir(\"dataset_blur_2/train/cat/\")\n",
    "#   os.mkdir(\"dataset_blur_2/train/dog/\")\n",
    "#   os.mkdir(\"dataset_blur_2/test/cat/\")\n",
    "#   os.mkdir(\"dataset_blur_2/test/dog/\")\n",
    "# except:\n",
    "#   pass\n",
    "\n",
    "# for i in tqdm.tqdm(range(training_length)):\n",
    "#   target = f\"dataset/train/cat/{i+1}.jpg\"\n",
    "#   copyfile(cat_files[i],target)\n",
    "#   target = f\"dataset/train/dog/{i+1}.jpg\"\n",
    "#   copyfile(dog_files[i],target)\n",
    "#   target = f\"dataset_blur_2/train/cat/{i+1}.jpg\"\n",
    "#   im = Image.open(cat_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
    "#   im.save(target)\n",
    "#   target = f\"dataset_blur_2/train/dog/{i+1}.jpg\"\n",
    "#   im = Image.open(dog_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
    "#   im.save(target)\n",
    "#   target = f\"dataset_blur_5/train/cat/{i+1}.jpg\"\n",
    "#   im = Image.open(cat_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
    "#   im.save(target)\n",
    "#   target = f\"dataset_blur_5/train/dog/{i+1}.jpg\"\n",
    "#   im = Image.open(dog_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
    "#   im.save(target)\n",
    "\n",
    "# for i in tqdm.tqdm(range(training_length,N)):\n",
    "#   target = f\"dataset/test/cat/{int(i-training_length+1)}.jpg\"\n",
    "#   copyfile(cat_files[i],target)\n",
    "#   target = f\"dataset/test/dog/{int(i-training_length+1)}.jpg\"\n",
    "#   copyfile(dog_files[i],target)\n",
    "#   target = f\"dataset_blur_2/test/cat/{int(i-training_length+1)}.jpg\"\n",
    "#   im = Image.open(cat_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
    "#   im.save(target)\n",
    "#   target = f\"dataset_blur_2/test/dog/{int(i-training_length+1)}.jpg\"\n",
    "#   im = Image.open(dog_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
    "#   im.save(target)\n",
    "#   target = f\"dataset_blur_5/test/cat/{int(i-training_length+1)}.jpg\"\n",
    "#   im = Image.open(cat_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
    "#   im.save(target)\n",
    "#   target = f\"dataset_blur_5/test/dog/{int(i-training_length+1)}.jpg\"\n",
    "#   im = Image.open(dog_files[i]).convert('RGB')\n",
    "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
    "#   im.save(target)\n",
    "\n",
    "# rmtree(\"PetImages\")\n",
    "\n",
    "# def zipdir(path, ziph):\n",
    "#     # ziph is zipfile handle\n",
    "#     for root, dirs, files in os.walk(path):\n",
    "#         for file in files:\n",
    "#             ziph.write(os.path.join(root, file),\n",
    "#                        os.path.relpath(os.path.join(root, file),\n",
    "#                                        os.path.join(path, '..')))\n",
    "\n",
    "# zipf = zipfile.ZipFile('catvdog_clear.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "# zipdir('dataset/', zipf)\n",
    "# zipf.close()\n",
    "# zipf = zipfile.ZipFile('catvdog_blur_2.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "# zipdir('dataset_blur_2/', zipf)\n",
    "# zipf.close()\n",
    "# zipf = zipfile.ZipFile('catvdog_blur_5.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "# zipdir('dataset_blur_5/', zipf)\n",
    "# zipf.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "blurry_vision",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}