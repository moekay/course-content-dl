{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/Neuroscience/pose_estimation.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Animal Pose Estimation\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Kristin Branson\n",
    "\n",
    "__Produtction editors:__ Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Objectives\n",
    "\n",
    "Train a deep network that can predict the locations of parts of animals. This Colab Notebook has all the code necessary to train a UNet network to predict the positions of 17 parts on a fruit fly. \n",
    "\n",
    "**Project ideas:**\n",
    "\n",
    "1. (easy-medium) Improve the pose estimator. Some possible ideas to explore:\n",
    "  * (easy) Data augmentation: It is common to train a network to be robust to certain kinds of perturbations by adding random perturbations to the training data. Try modifying the COCODataset to perform data augmentation. The mmpose toolkit has some data transforms relevant for pose tracking here\n",
    "https://github.com/open-mmlab/mmpose/blob/b0acfc423da672e61db75e00df9da106b6ead574/mmpose/datasets/pipelines/top_down_transform.py\n",
    "  * (medium) Network architecture: There are tons of networks people have designed for pose estimation. The mmpose toolbox has many networks implemented: \n",
    "https://github.com/open-mmlab/mmpose. Can you improve the accuracy with more exotic networks than the UNet? To do this, you should define a new network class, similar to the definition of [UNet](https://colab.research.google.com/drive/1SLgOHcgo1bfMDx5wlnLqm05AZe6aVG0l?authuser=1#scrollTo=Yf4vdxN7v9Rz&line=5&uniqifier=1). If you need a different loss function, you will also need to change the criterion used for training. \n",
    "  * (easy to medium) Optimization algorithm: Feed-forward convolutional networks have been engineered (e.g. by adding batch normalization layers) to be pretty robust to the exact choice of gradient descent algorithm, but there is still room for improvement in this code. \n",
    "  * Other ideas? Look at the errors the network is making -- how might we improve the tracker? \n",
    "  * (medium) Our training data set was relatively large -- 4216 examples. Can we get away with less examples than this? Can we change our algorithm to work better with smaller data sets? One idea to look into is pre-training the network on a different data set. \n",
    "  * Note: The data provided consists of both a training and a test set. It is important to not overfit to the test set, and only use it for a final evaluation. This code splits the training set into a training and a validation data set. Use this split data for testing out different algorithms. Then, after you finish developing your algorithm you can evaluate it on the test data. \n",
    "2. (easy) Train a pose estimator for a different data set. \n",
    "  * This Notebook has code for training a fly part tracker. More animal pose data sets can be found here: https://mmpose.readthedocs.io/en/latest/tasks/2d_animal_keypoint.html\n",
    "  * You can label your own dataset using animal tracking software like \n",
    "  DeepLabCut http://www.mackenziemathislab.org/deeplabcut\n",
    "  or APT http://kristinbranson.github.io/APT/\n",
    "  * To use a different data set, you might need to make a new Dataset class similar to our COCODataset class. \n",
    "3. (medium) Explore how well the network generalizes to data collected in other labs. Can you train a pose estimator that works on lots of different types of data? \n",
    "3. (easy) Explore using tensorboard with this network. Tensorboard lets you monitor and visualize training, and is an important tool as you develop and debug algorithms. A tutorial on using Tensorboard is here\n",
    "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
    "A Colab Notebook using tensorboard is here:\n",
    "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb\n",
    "4. (hard) Explore how the network is making its decisions using explainable AI techniques. \n",
    "\n",
    "Acknowledgments:\n",
    "This Notebook was developed by Kristin Branson. It borrows from:\n",
    "* APT https://github.com/kristinbranson/APT\n",
    "* Milesi Alexandre's UNet implementation https://github.com/milesial/Pytorch-UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install opencv-python --quiet\n",
    "!pip install google.colab --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper function\n",
    "def PlotLabelAndPrediction(batch, hm_pred, idx=None, title_string=''):\n",
    "  \"\"\"\n",
    "  PlotLabelAndPrediction(batch,pred,idx=None):\n",
    "  Plot the input, labels, and predictions for a batch.\n",
    "  \"\"\"\n",
    "  isbatch = isinstance(batch['id'], torch.Tensor)\n",
    "\n",
    "  if idx is None and isbatch:\n",
    "    idx = range(len(batch['id']))\n",
    "  if isbatch:\n",
    "    n = len(idx)\n",
    "  else:\n",
    "    n = 1\n",
    "    idx = [None,]\n",
    "  locs_pred = heatmap2landmarks(hm_pred.cpu().numpy())\n",
    "  for i in range(n):\n",
    "\n",
    "    plt.subplot(n, 4, 4*i + 1)\n",
    "    im = COCODataset.get_image(batch, idx[i])\n",
    "    plt.imshow(im,cmap='gray')\n",
    "    locs = COCODataset.get_landmarks(batch, idx[i])\n",
    "    for k in range(train_dataset.nlandmarks):\n",
    "      plt.plot(locs[k, 0], locs[k, 1],\n",
    "               marker='.', color=colors[k],\n",
    "               markerfacecolor=colors[k])\n",
    "    if isbatch:\n",
    "      batchid = batch['id'][i]\n",
    "    else:\n",
    "      batchid = batch['id']\n",
    "    plt.title(f\"{title_string}{batchid}\")\n",
    "\n",
    "    plt.subplot(n, 4, 4*i + 2)\n",
    "    plt.imshow(im,cmap='gray')\n",
    "    locs = COCODataset.get_landmarks(batch, idx[i])\n",
    "    if isbatch:\n",
    "      locs_pred_curr = locs_pred[i, ...]\n",
    "    else:\n",
    "      locs_pred_curr = locs_pred\n",
    "    for k in range(train_dataset.nlandmarks):\n",
    "      plt.plot(locs_pred_curr[k, 0], locs_pred_curr[k, 1],\n",
    "               marker='.', color=colors[k],\n",
    "               markerfacecolor=colors[k])\n",
    "    if i == 0: plt.title('pred')\n",
    "\n",
    "    plt.subplot(n, 4, 4*i + 3)\n",
    "    hmim = COCODataset.get_heatmap_image(batch, idx[i])\n",
    "    plt.imshow(hmim)\n",
    "    if i == 0: plt.title('label')\n",
    "\n",
    "    plt.subplot(n, 4, 4*i + 4)\n",
    "    if isbatch:\n",
    "      predcurr = hm_pred[idx[i], ...]\n",
    "    else:\n",
    "      predcurr = hm_pred\n",
    "    plt.imshow(heatmap2image(predcurr.cpu().numpy(), colors=colors))\n",
    "    if i == 0: plt.title('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\ntorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Download the data\n",
    "import os, requests, tarfile\n",
    "\n",
    "fname = 'fly_bubble_20201204.tar.gz'\n",
    "url = 'https://osf.io/q7vhy/download'\n",
    "datadir = 'view0'\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as ftar:\n",
    "    ftar.write(r.content)\n",
    "  print('Fly pose data have been downloaded.')\n",
    "\n",
    "# Untar fly pose data\n",
    "if not os.path.exists(datadir):\n",
    "  with tarfile.open(fname, 'r') as f:\n",
    "    f.extractall('.')  # specify which folder to extract to\n",
    "    # remove tar file\n",
    "    os.remove(fname)\n",
    "    print('Fly pose data have been unzipped.')\n",
    "else:\n",
    "  print('Fly pose data already unzipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Mount your gDrive\n",
    "\n",
    "Get the pose data set. To do this, you need to make a shortcut to a shared Google Drive directory in your Google Drive. \n",
    "\n",
    "1.   Go to the shared Google Drive: https://drive.google.com/drive/folders/1a06ZAmQXvUqZZQGI9XWWjABl4vOF8v6Z?usp=sharing\n",
    "2.   Select \"Add shortcut to Drive\" and select \"My Drive\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/static/Screenshot_AddShortcutToPoseData.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "gDrive = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " set the `gDrive=True` and run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown set the `gDrive=True` and run the cell.\n",
    "from google.colab import drive\n",
    "\n",
    "if gDrive:\n",
    "  print('The first time you run this, it will ask you to verify that Google Colab can access your Google Drive.')\n",
    "  print('Follow the instructions -- go to the linked website, and copy-paste the provided code.')\n",
    "  drive.flush_and_unmount()\n",
    "  drive.mount('/content/drive', force_remount=True)\n",
    "  assert os.path.exists('/content/drive/My Drive'), 'Google drive not mounted'\n",
    "\n",
    "  # Unzip fly pose data\n",
    "  datadir = 'view0'\n",
    "  if not os.path.exists(datadir):\n",
    "    assert os.path.exists('/content/drive/My Drive/fly_bubble_pose/fly_bubble_20201204.tar.gz'), 'Fly pose data zip file not found'\n",
    "    !tar -xvzf '/content/drive/My Drive/fly_bubble_pose/fly_bubble_20201204.tar.gz' > /dev/null\n",
    "    assert os.path.exists(datadir), 'view0 not created after unzipping data'\n",
    "  else:\n",
    "    print('Fly pose data already unzipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Make sure all the data exists\n",
    "traindir = os.path.join(datadir, 'train')\n",
    "trainannfile = os.path.join(datadir, 'train_annotations.json')\n",
    "testdir = os.path.join(datadir, 'test')\n",
    "testannfile = os.path.join(datadir, 'test_annotations.json')\n",
    "assert os.path.exists(traindir) and os.path.exists(testdir) and os.path.exists(trainannfile) and os.path.exists(testannfile), 'Could not find all necessary data after unzipping'\n",
    "print('Found all the fly pose data')\n",
    "\n",
    "# Read annotation information\n",
    "with open(trainannfile) as f:\n",
    "  trainann = json.load(f)\n",
    "f.close()\n",
    "ntrainims = len(trainann['images'])\n",
    "# Make sure we have all the images\n",
    "t = glob(os.path.join(traindir,'*.png'))\n",
    "print(f\"N. train images = {ntrainims}, number of images unzipped = {len(t)}\")\n",
    "assert ntrainims == len(t), 'number of annotations and number of images do not match'\n",
    "\n",
    "# get some features of the data set\n",
    "i = 0\n",
    "filestr = trainann['images'][0]['file_name']\n",
    "imfile = os.path.join(traindir,filestr)\n",
    "im = cv2.imread(imfile, cv2.IMREAD_UNCHANGED)\n",
    "imsize = im.shape\n",
    "if len(imsize) == 2:\n",
    "  imsize += (1, )\n",
    "print(f\"input image size: {imsize}\")\n",
    "\n",
    "landmark_names = ['head_fc', 'head_bl', 'head_br', 'thorax_fr', 'thorax_fl',\n",
    "                  'thorax_bc', 'abdomen', 'leg_ml_in', 'leg_ml_c',' leg_mr_in',\n",
    "                  'leg_mr_c', 'leg_fl_tip', 'leg_ml_tip', 'leg_bl_tip',\n",
    "                  'leg_br_tip','leg_mr_tip','leg_fr_tip']\n",
    "nlandmarks = trainann['annotations'][0]['num_keypoints']\n",
    "assert nlandmarks == len(landmark_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Visulaize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Show some example images\n",
    "nimsshow = 3  # number of images to plot\n",
    "imsshow = np.random.choice(ntrainims,nimsshow)\n",
    "fig = plt.figure(figsize=(4*nimsshow, 4), dpi=100)  # make the figure bigger\n",
    "for i in range(nimsshow):\n",
    "  filestr = trainann['images'][imsshow[i]]['file_name']\n",
    "  imfile = os.path.join(traindir, filestr)\n",
    "  im = cv2.imread(imfile, cv2.IMREAD_UNCHANGED)\n",
    "  plt.subplot(1,nimsshow,i+1)\n",
    "  plt.imshow(im,cmap='gray')\n",
    "  x = trainann['annotations'][imsshow[i]]['keypoints'][::3]\n",
    "  y = trainann['annotations'][imsshow[i]]['keypoints'][1::3]\n",
    "  plt.scatter(x, y, marker='.', c=np.arange(nlandmarks), cmap='jet')\n",
    "  plt.title(filestr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# define a dataset class to load the data\n",
    "\n",
    "def heatmap2image(hm, cmap='jet', colors=None):\n",
    "  \"\"\"\n",
    "  heatmap2image(hm,cmap='jet',colors=None)\n",
    "  Creates and returns an image visualization from landmark heatmaps. Each\n",
    "  landmark is colored according to the input cmap/colors.\n",
    "  Inputs:\n",
    "    hm: nlandmarks x height x width ndarray, dtype=float in the range 0 to 1.\n",
    "    hm[p,i,j] is a score indicating how likely it is that the pth landmark\n",
    "    is at pixel location (i,j).\n",
    "    cmap: string.\n",
    "    Name of colormap for defining colors of landmark points. Used only if colors\n",
    "    is None.\n",
    "    Default: 'jet'\n",
    "    colors: list of length nlandmarks.\n",
    "    colors[p] is an ndarray of size (4,) indicating the color to use for the\n",
    "    pth landmark. colors is the output of matplotlib's colormap functions.\n",
    "    Default: None\n",
    "  Output:\n",
    "    im: height x width x 3 ndarray\n",
    "    Image representation of the input heatmap landmarks.\n",
    "  \"\"\"\n",
    "  hm = np.maximum(0., np.minimum(1. ,hm))\n",
    "  im = np.zeros((hm.shape[1], hm.shape[2], 3))\n",
    "  if colors is None:\n",
    "    if isinstance(cmap, str):\n",
    "      cmap = matplotlib.cm.get_cmap(cmap)\n",
    "    colornorm = matplotlib.colors.Normalize(vmin=0, vmax=hm.shape[0])\n",
    "    colors = cmap(colornorm(np.arange(hm.shape[0])))\n",
    "  for i in range(hm.shape[0]):\n",
    "    color = colors[i]\n",
    "    for c in range(3):\n",
    "      im[..., c] = im[..., c] + (color[c] * .7 + .3) * hm[i, ...]\n",
    "  im = np.minimum(1.,im)\n",
    "\n",
    "  return im\n",
    "\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "  \"\"\"\n",
    "  COCODataset\n",
    "  Torch Dataset based on the COCO keypoint file format.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, annfile, datadir=None, label_sigma=3.,\n",
    "               transform=None, landmarks=None):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    This must be defined in every Torch Dataset and can take any inputs you\n",
    "    want it to.\n",
    "    Inputs:\n",
    "      annfile: string\n",
    "      Path to json file containing annotations.\n",
    "      datadir: string\n",
    "      Path to directory containing images. If None, images are assumed to be in\n",
    "      the working directory.\n",
    "      Default: None\n",
    "      label_sigma: scalar float\n",
    "      Standard deviation in pixels of Gaussian to be used to make the landmark\n",
    "      heatmap.\n",
    "      Default: 3.\n",
    "      transform: None\n",
    "      Not used currently\n",
    "      landmarks: ndarray (or list, something used for indexing into ndarray)\n",
    "      Indices of landmarks available to use in this dataset. Reducing the\n",
    "      landmarks used can make training faster and require less memory, and is\n",
    "      useful for testing code. If None, all landmarks are used.\n",
    "      Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    # read in the annotations from the json file\n",
    "    with open(annfile) as f:\n",
    "      self.ann = json.load(f)\n",
    "    # where the images are\n",
    "    self.datadir = datadir\n",
    "\n",
    "    # landmarks to use\n",
    "    self.nlandmarks_all = self.ann['annotations'][0]['num_keypoints']\n",
    "    if landmarks is None:\n",
    "      self.nlandmarks = self.nlandmarks_all\n",
    "    else:\n",
    "      self.nlandmarks = len(landmarks)\n",
    "    self.landmarks = landmarks\n",
    "\n",
    "    # for data augmentation/rescaling\n",
    "    self.transform = transform\n",
    "\n",
    "    # output will be heatmap images, one per landmark, with Gaussian values\n",
    "    # around the landmark location -- precompute some stuff for that\n",
    "    self.label_filter = None\n",
    "    self.label_filter_r = 1\n",
    "    self.label_filter_d = 3\n",
    "    self.label_sigma = label_sigma\n",
    "    self.init_label_filter()\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    Overloaded len function.\n",
    "    This must be defined in every Torch Dataset and must take only self\n",
    "    as input.\n",
    "    Returns the number of examples in the dataset.\n",
    "    \"\"\"\n",
    "    return len(self.ann['images'])\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    \"\"\"\n",
    "    Overloaded getitem function.\n",
    "    This must be defined in every Torch Dataset and must take only self\n",
    "    and item as input. It returns example number item.\n",
    "    item: scalar integer.\n",
    "    The output example is a dict with the following fields:\n",
    "    image: torch float32 tensor of size ncolors x height x width\n",
    "    landmarks: nlandmarks x 2 float ndarray\n",
    "    heatmaps: torch float32 tensor of size nlandmarks x height x width\n",
    "    id: scalar integer, contains item\n",
    "    \"\"\"\n",
    "\n",
    "    # read in the image for training example item\n",
    "    # and convert to a torch tensor\n",
    "    filename = self.ann['images'][item]['file_name']\n",
    "    if self.datadir is not None:\n",
    "      filename = os.path.join(self.datadir, filename)\n",
    "    assert os.path.exists(filename)\n",
    "    im = torch.from_numpy(cv2.imread(filename, cv2.IMREAD_UNCHANGED))\n",
    "\n",
    "    # convert to float32 in the range 0. to 1.\n",
    "    if im.dtype == float:\n",
    "      pass\n",
    "    elif im.dtype == torch.uint8:\n",
    "      im = im.float() / 255.\n",
    "    elif im.dtype == torch.uint16:\n",
    "      im = im.float() / 65535.\n",
    "    else:\n",
    "      print('Cannot handle im type '+str(im.dtype))\n",
    "      raise TypeError\n",
    "\n",
    "    imsz = im.shape\n",
    "    # convert to a tensor of size ncolors x h x w\n",
    "    if im.dim() == 3:\n",
    "      im = torch.transpose(im, [2, 0, 1])  # now 3 x h x w\n",
    "    else:\n",
    "      im = torch.unsqueeze(im, 0)  # now 1 x h x w\n",
    "\n",
    "    # landmark locations\n",
    "    locs = np.reshape(self.ann['annotations'][item]['keypoints'],\n",
    "                      [self.nlandmarks_all, 3])\n",
    "    locs = locs[:, :2]\n",
    "    if self.landmarks is not None:\n",
    "      locs = locs[self.landmarks, :]\n",
    "\n",
    "    # create heatmap target prediction\n",
    "    heatmaps = self.make_heatmap_target(locs, imsz)\n",
    "\n",
    "    # return a dict with the following fields:\n",
    "    # image: torch float32 tensor of size ncolors x height x width\n",
    "    # landmarks: nlandmarks x 2 float ndarray\n",
    "    # heatmaps: torch float32 tensor of size nlandmarks x height x width\n",
    "    # id: scalar integer, contains item\n",
    "    features = {'image':im,\n",
    "                'landmarks':locs.astype(np.float32),\n",
    "                'heatmaps':heatmaps,\n",
    "                'id':item}\n",
    "\n",
    "    return features\n",
    "\n",
    "  def init_label_filter(self):\n",
    "    \"\"\"\n",
    "    init_label_filter(self)\n",
    "    Helper function\n",
    "    Create a Gaussian filter for the heatmap target output\n",
    "    \"\"\"\n",
    "    # radius of the filter\n",
    "    self.label_filter_r = max(int(round(3 * self.label_sigma)), 1)\n",
    "    # diameter of the filter\n",
    "    self.label_filter_d = 2 * self.label_filter_r + 1\n",
    "\n",
    "    # allocate\n",
    "    self.label_filter = np.zeros([self.label_filter_d, self.label_filter_d])\n",
    "    # set the middle pixel to 1.\n",
    "    self.label_filter[self.label_filter_r, self.label_filter_r] = 1.\n",
    "    # blur with a Gaussian\n",
    "    self.label_filter = cv2.GaussianBlur(self.label_filter,\n",
    "                                         (self.label_filter_d,\n",
    "                                          self.label_filter_d),\n",
    "                                         self.label_sigma)\n",
    "    # normalize\n",
    "    self.label_filter = self.label_filter / np.max(self.label_filter)\n",
    "    # convert to torch tensor\n",
    "    self.label_filter = torch.from_numpy(self.label_filter)\n",
    "\n",
    "  def make_heatmap_target(self, locs, imsz):\n",
    "    \"\"\"\n",
    "    make_heatmap_target(self,locs,imsz):\n",
    "    Helper function\n",
    "    Creates the heatmap tensor of size imsz corresponding to landmark locations locs\n",
    "    Inputs:\n",
    "      locs: nlandmarks x 2 ndarray\n",
    "      Locations of landmarks\n",
    "      imsz: image shape\n",
    "    Returns:\n",
    "      target: torch tensor of size nlandmarks x imsz[0] x imsz[1]\n",
    "      Heatmaps corresponding to locs\n",
    "    \"\"\"\n",
    "    # allocate the tensor\n",
    "    target = torch.zeros((locs.shape[0], imsz[0], imsz[1]), dtype=torch.float32)\n",
    "    # loop through landmarks\n",
    "    for i in range(locs.shape[0]):\n",
    "      # location of this landmark to the nearest pixel\n",
    "      x = int(np.round(locs[i, 0])) # losing sub-pixel accuracy\n",
    "      y = int(np.round(locs[i, 1]))\n",
    "      # edges of the Gaussian filter to place, minding border of image\n",
    "      x0 = np.maximum(0, x - self.label_filter_r)\n",
    "      x1 = np.minimum(imsz[1] - 1, x + self.label_filter_r)\n",
    "      y0 = np.maximum(0, y - self.label_filter_r)\n",
    "      y1 = np.minimum(imsz[0] - 1, y + self.label_filter_r)\n",
    "      # crop filter if it goes outside of the image\n",
    "      fil_x0 = self.label_filter_r - (x - x0)\n",
    "      fil_x1 = self.label_filter_d - (self.label_filter_r - (x1 - x))\n",
    "      fil_y0 = self.label_filter_r - (y - y0)\n",
    "      fil_y1 = self.label_filter_d - (self.label_filter_r - (y1 - y))\n",
    "      # copy the filter to the relevant part of the heatmap image\n",
    "      target[i, y0:y1 + 1, x0:x1 + 1] = self.label_filter[fil_y0:fil_y1 + 1,\n",
    "                                                          fil_x0:fil_x1 + 1]\n",
    "\n",
    "    return target\n",
    "\n",
    "  @staticmethod\n",
    "  def get_image(d, i=None):\n",
    "    \"\"\"\n",
    "    static function, used for visualization\n",
    "    COCODataset.get_image(d,i=None)\n",
    "    Returns an image usable with plt.imshow()\n",
    "    Inputs:\n",
    "      d: if i is None, item from a COCODataset.\n",
    "      if i is a scalar, batch of examples from a COCO Dataset returned\n",
    "      by a DataLoader.\n",
    "      i: Index of example into the batch d, or None if d is a single example\n",
    "    Returns the ith image from the patch as an ndarray plottable with\n",
    "    plt.imshow()\n",
    "    \"\"\"\n",
    "    if i is None:\n",
    "      im = np.squeeze(np.transpose(d['image'].numpy(), (1, 2, 0)), axis=2)\n",
    "    else:\n",
    "      im = np.squeeze(np.transpose(d['image'][i,...].numpy(), (1, 2, 0)), axis=2)\n",
    "    return im\n",
    "\n",
    "  @staticmethod\n",
    "  def get_landmarks(d, i=None):\n",
    "    \"\"\"\n",
    "    static helper function\n",
    "    COCODataset.get_landmarks(d,i=None)\n",
    "    Returns a nlandmarks x 2 ndarray indicating landmark locations.\n",
    "    Inputs:\n",
    "      d: if i is None, item from a COCODataset.\n",
    "      if i is a scalar, batch of examples from a COCO Dataset returned\n",
    "      by a DataLoader.\n",
    "      i: Index of example into the batch d, or None if d is a single example\n",
    "    \"\"\"\n",
    "    if i is None:\n",
    "      locs = d['landmarks']\n",
    "    else:\n",
    "      locs = d['landmarks'][i]\n",
    "    return locs\n",
    "\n",
    "  @staticmethod\n",
    "  def get_heatmap_image(d, i, cmap='jet', colors=None):\n",
    "    \"\"\"\n",
    "    static function, used for visualization\n",
    "    COCODataset.get_heatmap_image(d,i=None)\n",
    "    Returns an image visualization of heatmaps usable with plt.imshow()\n",
    "    Inputs:\n",
    "      d: if i is None, item from a COCODataset.\n",
    "      if i is a scalar, batch of examples from a COCO Dataset returned\n",
    "      by a DataLoader.\n",
    "      i: Index of example into the batch d, or None if d is a single example\n",
    "      Returns the ith heatmap from the patch as an ndarray plottable with\n",
    "      plt.imshow()\n",
    "      cmap: string.\n",
    "      Name of colormap for defining colors of landmark points. Used only if colors\n",
    "      is None.\n",
    "      Default: 'jet'\n",
    "      colors: list of length nlandmarks.\n",
    "      colors[p] is an ndarray of size (4,) indicating the color to use for the\n",
    "      pth landmark. colors is the output of matplotlib's colormap functions.\n",
    "      Default: None\n",
    "    Output:\n",
    "      im: height x width x 3 ndarray\n",
    "      Image representation of the input heatmap landmarks.\n",
    "    \"\"\"\n",
    "    if i is None:\n",
    "      hm = d['heatmaps']\n",
    "    else:\n",
    "      hm = d['heatmaps'][i, ...]\n",
    "    hm = hm.numpy()\n",
    "    im = heatmap2image(hm, cmap=cmap, colors=colors)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# instantiate train data loader\n",
    "\n",
    "# only use a subset of the landmarks\n",
    "#landmarks = np.where(list(map(lambda x: x in ['head_fc','leg_fl_tip','leg_fr_tip'],landmark_names)))[0]\n",
    "# use all the landmarks\n",
    "landmarks = None\n",
    "\n",
    "train_dataset = COCODataset(trainannfile, datadir=traindir, landmarks=landmarks)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=2,\n",
    "                                               shuffle=True)\n",
    "\n",
    "# plot example images using the dataloader\n",
    "fig = plt.figure(figsize=(nimsshow * 4, 8), dpi=100)\n",
    "\n",
    "# choose some colors for each landmark\n",
    "cmap = matplotlib.cm.get_cmap('jet')\n",
    "colornorm = matplotlib.colors.Normalize(vmin=0, vmax=train_dataset.nlandmarks)\n",
    "colors = cmap(colornorm(np.arange(train_dataset.nlandmarks)))\n",
    "\n",
    "count = 0\n",
    "for i_batch, sample_batch in enumerate(train_dataloader):\n",
    "  for j in range(len(sample_batch['id'])):\n",
    "    plt.subplot(2, nimsshow, count + 1)\n",
    "    # use our helper functions for getting and formatting data from the batch\n",
    "    im = COCODataset.get_image(sample_batch, j)\n",
    "    locs = COCODataset.get_landmarks(sample_batch, j)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    for k in range(train_dataset.nlandmarks):\n",
    "      plt.plot(locs[k, 0], locs[k, 1], marker='.', color=colors[k],\n",
    "               markerfacecolor=colors[k])\n",
    "    plt.title('%d'%sample_batch['id'][j])\n",
    "    hmim = COCODataset.get_heatmap_image(sample_batch, j, colors=colors)\n",
    "    plt.subplot(2, nimsshow, count + 1 + nimsshow)\n",
    "    plt.imshow(hmim)\n",
    "    count += 1\n",
    "    if count >= nimsshow:\n",
    "      break\n",
    "  if count >= nimsshow:\n",
    "    break\n",
    "\n",
    "# Show the structure of a batch\n",
    "print(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define network structure - UNet\n",
    "# Copy-paste & modify from https://github.com/milesial/Pytorch-UNet\n",
    "\n",
    "# The UNet is defined modularly.\n",
    "# It is a series of downsampling layers defined by the module Down\n",
    "# followed by upsampling layers defined by the module Up. The output is\n",
    "# a convolutional layer with an output channel for each landmark, defined by\n",
    "# the module OutConv.\n",
    "# Each down and up layer is actually two convolutional layers with\n",
    "# a ReLU nonlinearity and batch normalization, defined by the module\n",
    "# DoubleConv.\n",
    "# The Down module consists of a 2x2 max pool layer followed by the DoubleConv\n",
    "# module.\n",
    "# The Up module consists of an upsampling, either defined via bilinear\n",
    "# interpolation (bilinear=True), or a learned convolutional transpose, followed\n",
    "# by a DoubleConv module.\n",
    "# The Output layer is a single 2-D convolutional layer with no nonlinearity.\n",
    "# The nonlinearity is incorporated into the network loss function.\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "  \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "  def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "    super().__init__()\n",
    "    if not mid_channels:\n",
    "        mid_channels = out_channels\n",
    "    self.double_conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(mid_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "  \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "    self.maxpool_conv = nn.Sequential(\n",
    "        nn.MaxPool2d(2),\n",
    "        DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "  \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "  def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "    super().__init__()\n",
    "\n",
    "    # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "    if bilinear:\n",
    "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "      self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "    else:\n",
    "      self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "      self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "  def forward(self, x1, x2):\n",
    "    x1 = self.up(x1)\n",
    "    # input is CHW\n",
    "    diffY = x2.size()[2] - x1.size()[2]\n",
    "    diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                    diffY // 2, diffY - diffY // 2])\n",
    "    # if you have padding issues, see\n",
    "    # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "    # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "    x = torch.cat([x2, x1], dim=1)\n",
    "    return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super(OutConv, self).__init__()\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.conv(x)\n",
    "\n",
    "# copy-pasted and modified from unet_model.py\n",
    "\n",
    "class UNet(nn.Module):\n",
    "  def __init__(self, n_channels, n_landmarks, bilinear=True):\n",
    "    super(UNet, self).__init__()\n",
    "    self.n_channels = n_channels\n",
    "    self.n_landmarks = n_landmarks\n",
    "    self.bilinear = bilinear\n",
    "    self.nchannels_inc = 8\n",
    "\n",
    "    # define the layers\n",
    "\n",
    "    # number of channels in the first layer\n",
    "    nchannels_inc = self.nchannels_inc\n",
    "    # increase the number of channels by a factor of 2 each layer\n",
    "    nchannels_down1 = nchannels_inc*2\n",
    "    nchannels_down2 = nchannels_down1*2\n",
    "    nchannels_down3 = nchannels_down2*2\n",
    "    # decrease the number of channels by a factor of 2 each layer\n",
    "    nchannels_up1 = nchannels_down3//2\n",
    "    nchannels_up2 = nchannels_up1//2\n",
    "    nchannels_up3 = nchannels_up2//2\n",
    "\n",
    "    if bilinear:\n",
    "      factor = 2\n",
    "    else:\n",
    "      factor = 1\n",
    "\n",
    "    self.layer_inc = DoubleConv(n_channels, nchannels_inc)\n",
    "\n",
    "    self.layer_down1 = Down(nchannels_inc, nchannels_down1)\n",
    "    self.layer_down2 = Down(nchannels_down1, nchannels_down2)\n",
    "    self.layer_down3 = Down(nchannels_down2, nchannels_down3//factor)\n",
    "\n",
    "    self.layer_up1 = Up(nchannels_down3, nchannels_up1//factor, bilinear)\n",
    "    self.layer_up2 = Up(nchannels_up1, nchannels_up2//factor, bilinear)\n",
    "    self.layer_up3 = Up(nchannels_up2, nchannels_up3//factor, bilinear)\n",
    "\n",
    "    self.layer_outc = OutConv(nchannels_up3//factor, self.n_landmarks)\n",
    "\n",
    "  def forward(self, x, verbose=False):\n",
    "    x1 = self.layer_inc(x)\n",
    "    if verbose: print(f'inc: shape = {x1.shape}')\n",
    "    x2 = self.layer_down1(x1)\n",
    "    if verbose:print(f'inc: shape = {x2.shape}')\n",
    "    x3 = self.layer_down2(x2)\n",
    "    if verbose: print(f'inc: shape = {x3.shape}')\n",
    "    x4 = self.layer_down3(x3)\n",
    "    if verbose: print(f'inc: shape = {x4.shape}')\n",
    "    x = self.layer_up1(x4, x3)\n",
    "    if verbose: print(f'inc: shape = {x.shape}')\n",
    "    x = self.layer_up2(x, x2)\n",
    "    if verbose: print(f'inc: shape = {x.shape}')\n",
    "    x = self.layer_up3(x, x1)\n",
    "    if verbose: print(f'inc: shape = {x.shape}')\n",
    "    logits = self.layer_outc(x)\n",
    "    if verbose: print(f'outc: shape = {logits.shape}')\n",
    "\n",
    "    return logits\n",
    "\n",
    "  def output(self, x, verbose=False):\n",
    "    return torch.sigmoid(self.forward(x, verbose=verbose))\n",
    "\n",
    "  def __str__(self):\n",
    "    s = ''\n",
    "    s += 'inc: '+str(self.layer_inc)+'\\n'\n",
    "    s += 'down1: '+str(self.layer_down1)+'\\n'\n",
    "    s += 'down2: '+str(self.layer_down2)+'\\n'\n",
    "    s += 'down3: '+str(self.layer_down3)+'\\n'\n",
    "    s += 'up1: '+str(self.layer_up1)+'\\n'\n",
    "    s += 'up2: '+str(self.layer_up2)+'\\n'\n",
    "    s += 'up3: '+str(self.layer_up3)+'\\n'\n",
    "    s += 'outc: '+str(self.layer_outc)+'\\n'\n",
    "    return s\n",
    "\n",
    "  def __repr__(self):\n",
    "    return str(self)\n",
    "\n",
    "\n",
    "def heatmap2landmarks(hms):\n",
    "  idx = np.argmax(hms.reshape(hms.shape[:-2] + (hms.shape[-2]*hms.shape[-1], )),\n",
    "                  axis=-1)\n",
    "  locs = np.zeros(hms.shape[:-2] + (2, ))\n",
    "  locs[...,1],locs[...,0] = np.unravel_index(idx,hms.shape[-2:])\n",
    "  return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Insantiate the network\n",
    "net = UNet(n_channels=imsize[-1], n_landmarks=train_dataset.nlandmarks)\n",
    "net.to(device=device) # have to be careful about what is done on the CPU vs GPU\n",
    "\n",
    "# try the network out before training\n",
    "batch = next(iter(train_dataloader))\n",
    "with torch.no_grad():\n",
    "  hms0 = net.output(batch['image'].to(device=device), verbose=True)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4*len(batch['id'])), dpi= 100)\n",
    "PlotLabelAndPrediction(batch, hms0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# load a network if one is already saved and you want to restart training\n",
    "# savefile = '/content/drive/My Drive/PoseEstimationNets/UNet20210510T140305/Final_epoch4.pth'\n",
    "savefile = None\n",
    "loadepoch = 0\n",
    "# savefile = None\n",
    "if savefile is not None:\n",
    "  net.load_state_dict(\n",
    "      torch.load(savefile, map_location=device)\n",
    "      )\n",
    "  m = re.search('[^\\d](?P<epoch>\\d+)\\.pth$', savefile)\n",
    "  if m is None:\n",
    "    print('Could not parse epoch from file name')\n",
    "  else:\n",
    "    loadepoch = int(m['epoch'])\n",
    "    print(f\"Parsed epoch from loaded net file name: {loadepoch}\")\n",
    "  net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "# following https://github.com/milesial/Pytorch-UNet/blob/master/train.py\n",
    "\n",
    "# parameters related to training the network\n",
    "batchsize = 2 # number of images per batch -- amount of required memory\n",
    "              # for training will increase linearly in batchsize\n",
    "nepochs = 3   # number of times to cycle through all the data during training\n",
    "learning_rate = 0.001 # initial learning rate\n",
    "weight_decay = 1e-8 # how learning rate decays over time\n",
    "momentum = 0.9 # how much to use previous gradient direction\n",
    "nepochs_per_save = 1 # how often to save the network\n",
    "val_frac = 0.1 # what fraction of data to use for validation\n",
    "\n",
    "# where to save the network\n",
    "# make sure to clean these out every now and then, as you will run out of space\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime('%Y%m%dT%H%M%S')\n",
    "# If you use your gDrive do not forget to set `gDrive` to `True`\n",
    "if gDrive:\n",
    "  savedir = '/content/drive/My Drive/PoseEstimationNets'\n",
    "else:\n",
    "  savedir = '/content/PoseEstimationNets'\n",
    "\n",
    "# if the folder does not exist, create it.\n",
    "if not os.path.exists(savedir):\n",
    "  os.mkdir(savedir)\n",
    "\n",
    "checkpointdir = os.path.join(savedir, 'UNet' + timestamp)\n",
    "os.mkdir(checkpointdir)\n",
    "\n",
    "# split into train and validation datasets\n",
    "n_val = int(len(train_dataset) * val_frac)\n",
    "n_train = len(train_dataset) - n_val\n",
    "train, val = torch.utils.data.random_split(train_dataset, [n_train, n_val])\n",
    "train_dataloader = torch.utils.data.DataLoader(train,\n",
    "                                               batch_size=batchsize,\n",
    "                                               shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val,\n",
    "                                             batch_size=batchsize,\n",
    "                                             shuffle=False)\n",
    "\n",
    "# gradient descent flavor\n",
    "optimizer = optim.RMSprop(net.parameters(),\n",
    "                          lr=learning_rate,\n",
    "                          weight_decay=weight_decay,\n",
    "                          momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "\n",
    "# Following https://github.com/milesial/Pytorch-UNet\n",
    "# Use binary cross entropy loss combined with sigmoid output activation function.\n",
    "# We combine here for numerical improvements\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# store loss per epoch\n",
    "epoch_losses = np.zeros(nepochs)\n",
    "epoch_losses[:] = np.nan\n",
    "\n",
    "# when we last saved the network\n",
    "saveepoch = None\n",
    "\n",
    "# how many gradient descent updates we have made\n",
    "iters = loadepoch*len(train_dataloader)\n",
    "\n",
    "# loop through entire training data set nepochs times\n",
    "for epoch in range(loadepoch, nepochs):\n",
    "  net.train() # put in train mode (affects batchnorm)\n",
    "  epoch_loss = 0\n",
    "  with tqdm(total=ntrainims,\n",
    "            desc=f\"Epoch {epoch + 1}/{nepochs}\",\n",
    "            unit='img') as pbar:\n",
    "\n",
    "    # loop through each batch in the training data\n",
    "    for batch in train_dataloader:\n",
    "      # compute the loss\n",
    "      imgs = batch['image']\n",
    "      imgs = imgs.to(device=device, dtype=torch.float32) # transfer to GPU\n",
    "      hm_labels = batch['heatmaps']\n",
    "      hm_labels = hm_labels.to(device=device, dtype=torch.float32) # transfer to GPU\n",
    "      hm_preds = net(imgs) # evaluate network on batch\n",
    "      loss = criterion(hm_preds,hm_labels) # compute loss\n",
    "      epoch_loss += loss.item()\n",
    "      pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "      # gradient descent\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "      optimizer.step()\n",
    "      iters += 1\n",
    "\n",
    "      pbar.update(imgs.shape[0])\n",
    "  print(f\"loss (epoch) = {epoch_loss}\")\n",
    "  epoch_losses[epoch] = epoch_loss\n",
    "\n",
    "  # save checkpoint networks every now and then\n",
    "  if epoch % nepochs_per_save == 0:\n",
    "    print(f\"Saving network state at epoch {epoch + 1}\")\n",
    "    # only keep around the last two epochs for space purposes\n",
    "    if saveepoch is not None:\n",
    "      savefile0 = os.path.join(checkpointdir,\n",
    "                               f\"CP_latest_epoch{saveepoch+1}.pth\")\n",
    "      savefile1 = os.path.join(checkpointdir,\n",
    "                               f\"CP_prev_epoch{saveepoch+1}.pth\")\n",
    "      if os.path.exists(savefile0):\n",
    "        try:\n",
    "          os.rename(savefile0,savefile1)\n",
    "        except:\n",
    "          print(f\"Failed to rename checkpoint file {savefile0} to {savefile1}\")\n",
    "    saveepoch = epoch\n",
    "    savefile = os.path.join(checkpointdir,f\"CP_latest_epoch{saveepoch + 1}.pth\")\n",
    "    torch.save(net.state_dict(),\n",
    "               os.path.join(checkpointdir, f\"CP_latest_epoch{epoch + 1}.pth\"))\n",
    "\n",
    "torch.save(net.state_dict(),\n",
    "           os.path.join(checkpointdir, f\"Final_epoch{epoch + 1}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# try the trained network out on training and val images\n",
    "net.eval()\n",
    "batch = next(iter(train_dataloader))\n",
    "with torch.no_grad():\n",
    "  train_hms1 = torch.sigmoid(net(batch['image'].to(device)))\n",
    "\n",
    "fig=plt.figure(figsize=(12, 4*train_hms1.shape[0]), dpi= 100)\n",
    "PlotLabelAndPrediction(batch,train_hms1,title_string='Train ')\n",
    "\n",
    "batch = next(iter(val_dataloader))\n",
    "with torch.no_grad():\n",
    "  val_hms1 = net.output(batch['image'].to(device))\n",
    "fig = plt.figure(figsize=(12, 4 * val_hms1.shape[0]), dpi=100)\n",
    "PlotLabelAndPrediction(batch, val_hms1, title_string='Val ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Evaluate the training and validation error\n",
    "\n",
    "def eval_net(net, loader):\n",
    "  net.eval()\n",
    "  n_val = len(loader) * loader.batch_size\n",
    "  errs = None\n",
    "  count = 0\n",
    "\n",
    "  for batch in loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "      hm_preds = torch.sigmoid(net(batch['image'].to(device))).cpu().numpy()\n",
    "\n",
    "    idx = np.argmax(hm_preds.reshape((hm_preds.shape[0],\n",
    "                                      hm_preds.shape[1],\n",
    "                                      hm_preds.shape[2] * hm_preds.shape[3])),\n",
    "                    axis=2)\n",
    "    loc_preds = np.zeros((hm_preds.shape[0], hm_preds.shape[1], 2))\n",
    "    loc_preds[:, :, 1], loc_preds[:, :, 0] = np.unravel_index(idx,\n",
    "                                                              hm_preds.shape[2:])\n",
    "\n",
    "    loc_labels = batch['landmarks'].numpy()\n",
    "    l2err = np.sqrt(np.sum((loc_preds - loc_labels)**2., axis=2))\n",
    "    idscurr = batch['id'].numpy()\n",
    "\n",
    "    if errs is None:\n",
    "      errs = np.zeros((n_val, l2err.shape[1]))\n",
    "      errs[:] = np.nan\n",
    "      ids = np.zeros(n_val, dtype=int)\n",
    "\n",
    "    errs[count:(count + l2err.shape[0]), :] = l2err\n",
    "    ids[count:(count + l2err.shape[0])] = idscurr\n",
    "    count += l2err.shape[0]\n",
    "\n",
    "  errs = errs[:count, :]\n",
    "  ids = ids[:count]\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  return errs, ids\n",
    "\n",
    "\n",
    "l2err_per_landmark_val, val_ids = eval_net(net, val_dataloader)\n",
    "l2err_per_landmark_train, train_ids = eval_net(net, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot the error distribution\n",
    "nbins = 25\n",
    "bin_edges = np.linspace(0, np.percentile(l2err_per_landmark_val, 99.),\n",
    "                        nbins + 1)\n",
    "bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2.\n",
    "bin_edges[-1] = np.inf\n",
    "frac_val = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
    "frac_train = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
    "for i in range(l2err_per_landmark_val.shape[1]):\n",
    "  frac_val[:, i], _ = np.histogram(l2err_per_landmark_val[:, i],\n",
    "                                   bin_edges, density=True)\n",
    "  frac_train[:, i], _ = np.histogram(l2err_per_landmark_train[:, i],\n",
    "                                     bin_edges, density=True)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4 * train_dataset.nlandmarks), dpi=100)\n",
    "for i in range(train_dataset.nlandmarks):\n",
    "  if landmarks is None:\n",
    "    landmark_name = landmark_names[i]\n",
    "  else:\n",
    "    landmark_name = landmark_names[landmarks[i]]\n",
    "  plt.subplot(train_dataset.nlandmarks, 1, i + 1)\n",
    "  hval = plt.plot(bin_centers,\n",
    "                  frac_val[:, i], '.-',\n",
    "                  label='Val', color=colors[i, :])\n",
    "  plt.plot(bin_centers, frac_train[:, i], ':',\n",
    "           label='Train', color=colors[i, :])\n",
    "  plt.legend()\n",
    "  plt.title(f\"{landmark_name} error (px)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot examples with big errors\n",
    "idx = np.argsort(-np.sum(l2err_per_landmark_val, axis=1))\n",
    "\n",
    "for i in range(5):\n",
    "  d = train_dataset[val_ids[idx[i]]]\n",
    "  img = d['image'].unsqueeze(0)\n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "    pred = net.output(img.to(device))\n",
    "\n",
    "  fig=plt.figure(figsize=(12, 4), dpi=100)\n",
    "  with np.printoptions(precision=2):\n",
    "    errstr = str(l2err_per_landmark_val[idx[i]])\n",
    "  PlotLabelAndPrediction(d,pred[0, ...])  #,title_string='Err = %s '%errstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Visulaization of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Visualize the first layer of convolutional features\n",
    "with torch.no_grad():\n",
    "  w = net.layer_inc.double_conv[0].weight.cpu().numpy()\n",
    "nr = int(np.ceil(np.sqrt(w.shape[0])))\n",
    "nc = int(np.ceil(w.shape[0] / nr))\n",
    "fig, ax = plt.subplots(nr, nc)\n",
    "for i in range(w.shape[0]):\n",
    "  r, c = np.unravel_index(i, (nr, nc))\n",
    "  fil = np.transpose(w[i, :, :, :], [1, 2, 0])\n",
    "  if fil.shape[-1] == 1:\n",
    "    fil = fil[:, :, 0]\n",
    "  ax[r][c].imshow(fil)\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Final evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# final evaluation on the test set. for proper evaluation, and to avoid overfitting\n",
    "# to the test set, we want to change parameters based on the validation set, and\n",
    "# only at the very end evaluate on the test set\n",
    "\n",
    "with open(testannfile) as f:\n",
    "  testann = json.load(f)\n",
    "f.close()\n",
    "ntestims = len(testann['images'])\n",
    "# Make sure we have all the images\n",
    "t = glob(os.path.join(testdir, '*.png'))\n",
    "print(f\"N. test images = {ntestims}, number of images unzipped = {len(t)}\")\n",
    "assert ntestims==len(t), 'number of annotations and number of images do not match'\n",
    "\n",
    "test_dataset = COCODataset(testannfile, datadir=testdir, landmarks=landmarks)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=2,\n",
    "                                              shuffle=True)\n",
    "\n",
    "l2err_per_landmark_test, test_ids = eval_net(net, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot the error distribution\n",
    "nbins = 25\n",
    "bin_edges = np.linspace(0, np.percentile(l2err_per_landmark_val, 99.),\n",
    "                        nbins + 1)\n",
    "bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2.\n",
    "bin_edges[-1] = np.inf\n",
    "frac_val = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
    "frac_train = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
    "frac_test = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
    "for i in range(l2err_per_landmark_val.shape[1]):\n",
    "  frac_val[:, i], _ = np.histogram(l2err_per_landmark_val[:, i],\n",
    "                                   bin_edges, density=True)\n",
    "  frac_train[:, i], _ = np.histogram(l2err_per_landmark_train[:, i],\n",
    "                                     bin_edges, density=True)\n",
    "  frac_test[:, i], _ = np.histogram(l2err_per_landmark_test[:, i],\n",
    "                                    bin_edges, density=True)\n",
    "\n",
    "fig=plt.figure(figsize=(8, 4 * train_dataset.nlandmarks), dpi=100)\n",
    "for i in range(train_dataset.nlandmarks):\n",
    "  if landmarks is None:\n",
    "    landmark_name = landmark_names[i]\n",
    "  else:\n",
    "    landmark_name = landmark_names[landmarks[i]]\n",
    "  plt.subplot(train_dataset.nlandmarks, 1, i + 1)\n",
    "  plt.plot(bin_centers, frac_test[:, i], '.-',\n",
    "           label='Test', color=colors[i, :])\n",
    "  plt.plot(bin_centers, frac_val[:, i], '--',\n",
    "           label='Val', color=colors[i, :])\n",
    "  plt.plot(bin_centers, frac_train[:, i], ':',\n",
    "           label='Train', color=colors[i, :])\n",
    "  plt.legend()\n",
    "  plt.title(f\"{landmark_name} error (px)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot examples with big errors\n",
    "idx = np.argsort(-np.sum(l2err_per_landmark_test, axis=1))\n",
    "\n",
    "for i in range(5):\n",
    "  d = test_dataset[test_ids[idx[i]]]\n",
    "  img = d['image'].unsqueeze(0)\n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "    pred = net.output(img.to(device))\n",
    "\n",
    "  fig=plt.figure(figsize=(12, 4), dpi=100)\n",
    "  with np.printoptions(precision=2):\n",
    "    errstr = str(l2err_per_landmark_test[idx[i]])\n",
    "  PlotLabelAndPrediction(d, pred[0, ...])  #,title_string='Err = %s '%errstr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pose_estimation",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}