{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Modeling sequencies and encoding text\n",
    "**Week 2, Day 3: Modern RNNs**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans\n",
    "\n",
    "__Content reviewers:__ Lily Cheng, Melvin Selim Atay, Ezekiel Williams\n",
    "\n",
    "__Content editors:__ Nina Kudryashova, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Roberto Guidotti, Spiros Chavlis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "----\n",
    "# Tutorial objectives\n",
    "\n",
    "Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tutorial slides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These are the slides for the videos in this tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in this tutorial\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/n263c/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "from IPython.display import clear_output\n",
    "!pip install torchtext==0.4.0 --quiet\n",
    "!pip install --upgrade gensim --quiet\n",
    "!pip install unidecode --quiet\n",
    "!pip install hmmlearn --quiet\n",
    "!pip install fasttext --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install python-Levenshtein --quiet\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "import fasttext\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Figure Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Load Dataset from `nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title  Load Dataset from `nltk`\n",
    "# no critical warnings, so we supress it\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "  #Tokenize the sentence\n",
    "  #from nltk.tokenize library use word_tokenize\n",
    "  token = word_tokenize(sentences)\n",
    "\n",
    "  return token\n",
    "\n",
    "\n",
    "def plot_train_val(x, train, val, train_label, val_label, title, y_label,\n",
    "                   color):\n",
    "  plt.plot(x, train, label=train_label, color=color)\n",
    "  plt.plot(x, val, label=val_label, color=color, linestyle='--')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel(y_label)\n",
    "  plt.title(title)\n",
    "\n",
    "\n",
    "def load_dataset(emb_vectors, sentence_length=50, seed=522):\n",
    "  TEXT = data.Field(sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    include_lengths=True,\n",
    "                    batch_first=True,\n",
    "                    fix_length=sentence_length)\n",
    "  LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "  train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "  TEXT.build_vocab(train_data, vectors=emb_vectors)\n",
    "  LABEL.build_vocab(train_data)\n",
    "\n",
    "  train_data, valid_data = train_data.split(split_ratio=0.7,\n",
    "                                            random_state=random.seed(seed))\n",
    "  train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data,\n",
    "                                                                  valid_data,\n",
    "                                                                  test_data),\n",
    "                                                                  batch_size=32,\n",
    "                                                                  sort_key=lambda x: len(x.text),\n",
    "                                                                  repeat=False,\n",
    "                                                                  shuffle=True)\n",
    "  vocab_size = len(TEXT.vocab)\n",
    "\n",
    "  print(f'Data are loaded. sentence length: {sentence_length} '\n",
    "        f'seed: {seed}')\n",
    "\n",
    "  return TEXT, vocab_size, train_iter, valid_iter, test_iter\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "  URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "  session = requests.Session()\n",
    "\n",
    "  response = session.get(URL, params={ 'id': id }, stream=True)\n",
    "  token = get_confirm_token(response)\n",
    "\n",
    "  if token:\n",
    "    params = { 'id': id, 'confirm': token }\n",
    "    response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "  save_response_content(response, destination)\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "  for key, value in response.cookies.items():\n",
    "    if key.startswith('download_warning'):\n",
    "      return value\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "  CHUNK_SIZE = 32768\n",
    "\n",
    "  with open(destination, \"wb\") as f:\n",
    "    for chunk in response.iter_content(CHUNK_SIZE):\n",
    "      if chunk: # filter out keep-alive new chunks\n",
    "        f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set random seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Executing `set_seed(seed=seed)` you are setting the seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set device (GPU or CPU). Execute `set_device()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "DEVICE = set_device()\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Sequences, Markov Chains & HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 1: Sequences & Markov Processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Sequences & Markov Processes\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1jg411774B\", width=730, height=410, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"ApkE7UFaJAQ\", width=730, height=410, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "In this notebook we will be exploring the world of sequences - thinking of what kind of data can be thought of as sequences, and how these sequences can be represented as Markov Chains and Hidden Markov Models. These ideas and methods were an important part of natural language processing and language modelling, and serve as a useful way to ground ourselves before we dive into neural network methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Why is this relevant? How are these sequences related to modern recurrent neural networks?\n",
    "\n",
    "Like we mentioned before, the notion of modelling sequences of data - in this particular case, **language**, is an ideal place to start. RNNs themselves were constructed keeping in mind sequences, and the ability to temporally model sequences is what inspired RNNs (and the family of LSTM, GRUs - we will see this in the next notebook).\n",
    "\n",
    "Markov models and hidden markov models serve as an introduction to these concepts because they were some of the earliest ways to think about sequences. They do not capture a lot of the complexity that RNNs excel at, but are an useful way of thinking of sequences, probabilities, and how we can use these concepts to perform  tasks such as text generation, or classification - tasks that RNNs excel at today. \n",
    "\n",
    "Think of this section as an introduction to thinking with sequences and text data, and as a historical introduction to the world of modelling sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1: What data are sequences?\n",
    "\n",
    "Native Sequences:\n",
    "\n",
    "- Temporally occurring events (e.g., history, stock prices)\n",
    "- Temporally processed events (e.g., communication)\n",
    "- Topologically connected components (e.g., polymers, peptides)\n",
    "\n",
    "Synthetic Sequences: \n",
    "\n",
    "- Anything processed as a sequence (e.g., scanned pixels in an image)\n",
    "\n",
    "Sequences can be represented as a Markov Process - since this notion of sequential data is intrinsically linked to RNNs, it is a good place for us to start, and natural language (text!) will be our sequence of choice. \n",
    "\n",
    "We will be using the Brown corpus which comes loaded with NLTK, and using the entire corpus - this requires a lot of RAM for some of the methods, so we recommend using a smaller subset of categories if you do not have enough RAM.\n",
    "\n",
    "We will be using some of the code from this [tutorial](https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html) and this [Jupyter notebook](https://github.com/StrikingLoo/ASOIAF-Markov/blob/master/ASOIAF.ipynb)\n",
    "\n",
    "The first few cells of code all involve set-up; some of this code will be hidden because they are not necessary to understand the ideas of markov models, but the way data is setup can be vital to the way the model performs (something in common with neural network models!).\n",
    "\n",
    "Let us start with loading our corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "category = ['editorial', 'fiction', 'government', 'news', 'religion']\n",
    "sentences = brown.sents(categories=category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have our sentences, let us look at some statistics to get an idea of what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "lengths = [len(sentence) for sentence in sentences]\n",
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Find the 80-th percentile: the minimal length of such a sentence, which is longer than at least 80% of sentences in the *Brown corpus*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "lengths.quantile(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This gives us an idea of what our dataset looks like, along with some average lengths. This kind of quick data exploration can be very useful - we know how long different sequences are, and how we might want to collect these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Since we will be modelling words as sequences in sentences, let us first collect all the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "corpus_words = []\n",
    "for sentence in sentences:\n",
    "  for word in sentence:\n",
    "    if \"''\" not in word and \"``\" not in word:\n",
    "      corpus_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Corpus length: {len(corpus_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "corpus_words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We'll now get distinct (unique) words and create a matrix to represent all these words. This is necessary because we will be using this matrix to look at the probability of the words in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating Matrices and Distinct Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Creating Matrices and Distinct Words\n",
    "distinct_words = list(set(corpus_words))\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
    "distinct_words_count = len(list(set(corpus_words)))\n",
    "next_word_matrix = np.zeros([distinct_words_count, distinct_words_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(\"Number of distinct words: \" + str(distinct_words_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the following lines of code we are populating the matrix that tracks the next word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Populating Matric that tracks next word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Populating Matric that tracks next word\n",
    "for i, word in enumerate(corpus_words[:-1]):\n",
    "  first_word_idx = word_idx_dict[word]\n",
    "  next_word_idx = word_idx_dict[corpus_words[i+1]]\n",
    "  next_word_matrix[first_word_idx][next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we have the information ready to construct a markov chain. The next word matrix is crucial in this, as it allows us to go from one word in the sequence to the next. We will soon see how this is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2: What is a Markov Chain or Model?\n",
    "\n",
    "A Markov Chain (or Model) is a:\n",
    "- stochastic model describing a sequence of possible events\n",
    "- the probability of each event depends only on the state attained in the previous event.\n",
    "- a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC) [vs. a continuous-time process or CTMC].\n",
    "- The classic formal language model is a Markov Model\n",
    "\n",
    "*Helpful explanations from [eric mjl's tutorial](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions)*!\n",
    "\n",
    "\n",
    "\n",
    "The simplest Markov models assume that we have a _system_ that contains a finite set of states,\n",
    "and that the _system_ transitions between these states with some probability at each time step $t$,\n",
    "thus generating a sequence of states over time.\n",
    "Let's call these states $S$, where\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\{s_1, s_2, ..., s_n\\}\n",
    "\\end{equation}\n",
    "\n",
    "To keep things simple, let's start with three states:\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\{s_1, s_2, s_3\\}\n",
    "\\end{equation}\n",
    "\n",
    "A Markov model generates a sequence of states, with one possible realization being:\n",
    "\n",
    "\\begin{equation}\n",
    "\\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\}\n",
    "\\end{equation}\n",
    "\n",
    "And generically, we represent it as a sequence of states $x_t, x_{t+1}... x_{t+n}$. (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/cell_chain.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Modelling transitions between states\n",
    "\n",
    "To know how a system transitions between states, we now need a **transition matrix**.\n",
    "\n",
    "The transition matrix describes the probability of transitioning from one state to another (The probability of staying in the same state is semantically equivalent to transitioning to the same state).\n",
    "\n",
    "By convention, transition matrix rows correspond to the state at time $t$,\n",
    "while columns correspond to state at time $t+1$.\n",
    "Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated.\n",
    "\n",
    "Let's call the transition matrix $P_{transition}$:\n",
    "\n",
    "\\begin{equation}\n",
    "P_{transition} = \n",
    "  \\begin{pmatrix}\n",
    "  p_{11} & p_{12} & p_{13} \\\\\n",
    "  p_{21} & p_{22} & p_{23} \\\\\n",
    "  p_{31} & p_{32} & p_{33} \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Using the transition matrix, we can express different behaviors of the system. For example:\n",
    "1. by assigning larger probability mass to the diagonals, we can express that the system likes to stay in the current state;\n",
    "2. by assigning larger probability mass to the off-diagonal, we can express that the system likes to transition out of its current state.\n",
    "\n",
    "In our case, this matrix is created by measuring how often one word appeared after another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function for most likely word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function for most likely word\n",
    "def most_likely_word_after(word):\n",
    "  # we check for the word most likely to occur using the matrix\n",
    "  most_likely = next_word_matrix[word_idx_dict[word]].argmax()\n",
    "  return distinct_words[most_likely]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Using our most likely word function, we can begin to create chains of words and create sequences. In the code below we create a naive chain that simply choses the most likely word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function for building Naive Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function for building Naive Chain\n",
    "def naive_chain(word, length=15):\n",
    "  current_word = word\n",
    "  sentence = word\n",
    "  # we now build a naive chain by picking up the most likely word\n",
    "  for _ in range(length):\n",
    "    sentence += ' '\n",
    "    next_word = most_likely_word_after(current_word)\n",
    "    sentence += next_word\n",
    "    current_word = next_word\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us now use this naive chain to see what comes up, using some simple words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(naive_chain('the'))\n",
    "print(naive_chain('I'))\n",
    "print(naive_chain('What'))\n",
    "print(naive_chain('park'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We notice that after the word `the`, `United States` comes up each time. All the other sequencies starting from other words also end up at `the` quite often. Since we use a *deterministic* markov chain model, its next state only depends on the previous one. Therefore, once the sequence comes to `the`, it inevitably continues the sequence with the `United States`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now be a little more sophisticated, and return words in a sequence using a *weighted choice*, which randomly selects the next word from a set of words with some probability (weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function for weighted choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function for weighted choice\n",
    "def weighted_choice(objects, weights):\n",
    "  \"\"\"\n",
    "  Returns randomly an element from the sequence of 'objects',\n",
    "      the likelihood of the objects is weighted according\n",
    "      to the sequence of 'weights', i.e. percentages.\n",
    "  \"\"\"\n",
    "\n",
    "  weights = np.array(weights, dtype=np.float64)\n",
    "  sum_of_weights = weights.sum()\n",
    "  # standardization:\n",
    "  np.multiply(weights, 1 / sum_of_weights)\n",
    "  weights = weights.cumsum()\n",
    "  x = random.random()\n",
    "  for i in range(len(weights)):\n",
    "    if x < weights[i]:\n",
    "      return objects[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function for sampling next word with weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function for sampling next word with weights\n",
    "def sample_next_word_after(word, alpha=0):\n",
    "  next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha\n",
    "  likelihoods = next_word_vector/next_word_vector.sum()\n",
    "  return weighted_choice(distinct_words, likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sample_next_word_after('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sample_next_word_after('The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "There! We don't see the same word twice, because of the added randomisation (i.e., stochasticity). Our algorithm calculates how likely it is to find a certain word after a given word (`The` in this case) in the corpus, and then generates 1 sample of the next word with a matching probability. \n",
    "\n",
    "In this example, we generated only one next word. Now, using this function, we'll build a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function for a stochastic chain using weighted choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function for a stochastic chain using weighted choice\n",
    "def stochastic_chain(word, length=15):\n",
    "  current_word = word\n",
    "  sentence = word\n",
    "\n",
    "  for _ in range(length):\n",
    "    sentence += ' '\n",
    "    next_word = sample_next_word_after(current_word)\n",
    "    sentence += next_word\n",
    "    current_word = next_word\n",
    "\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "stochastic_chain('Hospital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Neat - we can create stochastic chains for a single word. For a more effective language model, we would want to model sets of words - in the following cells, we create sets of words to predict a chain after a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def sequences_matrices(k=3):\n",
    "  # @title Code to build sets of words for more realistic sequences\n",
    "  sets_of_k_words = [' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k])]\n",
    "  sets_count = len(list(set(sets_of_k_words)))\n",
    "  next_after_k_words_matrix = dok_matrix((sets_count, len(distinct_words)))\n",
    "  distinct_sets_of_k_words = list(set(sets_of_k_words))\n",
    "  k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
    "  distinct_k_words_count = len(list(set(sets_of_k_words)))\n",
    "  for i, word in tqdm(enumerate(sets_of_k_words[:-k])):\n",
    "    word_sequence_idx = k_words_idx_dict[word]\n",
    "    next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "    next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1\n",
    "  return k_words_idx_dict,distinct_sets_of_k_words,next_after_k_words_matrix\n",
    "\n",
    "k_words_idx_dict, distinct_sets_of_k_words, next_after_k_words_matrix = sequences_matrices(k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's have a look at what that bit of code did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "distinct_sets_of_k_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Great! Now we are going to create a transition matrix for the sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Code to populate matrix of sets of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Code to populate matrix of sets of words\n",
    "for i, word in tqdm(enumerate(distinct_sets_of_k_words[:-k])):\n",
    "  word_sequence_idx = k_words_idx_dict[word]\n",
    "  next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "  next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We now have what we need to build a stochastic chain over a `K` set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function for stochastic Chain for sets of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function for stochastic Chain for sets of words\n",
    "def stochastic_chain_sequence(words, chain_length=15, k=2):\n",
    "  current_words = words.split(' ')\n",
    "  if len(current_words) != k:\n",
    "    raise ValueError(f'wrong number of words, expected {k}')\n",
    "  sentence = words\n",
    "\n",
    "  # pre-calculate seq embedding + transition matrix for a given k\n",
    "  matrices = sequences_matrices(k=k)\n",
    "\n",
    "  for _ in range(chain_length):\n",
    "    sentence += ' '\n",
    "    next_word = sample_next_word_after_sequence(matrices,' '.join(current_words))\n",
    "    sentence += next_word\n",
    "    current_words = current_words[1:]+[next_word]\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function to sample next word after sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function to sample next word after sequence\n",
    "def sample_next_word_after_sequence(matrices, word_sequence, alpha=0):\n",
    "  # unpack a tuple of matrices\n",
    "  k_words_idx_dict,distinct_sets_of_k_words, next_after_k_words_matrix = matrices\n",
    "\n",
    "  next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
    "  likelihoods = next_word_vector/next_word_vector.sum()\n",
    "  return weighted_choice(distinct_words, likelihoods.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "stochastic_chain_sequence('Judges under the', chain_length=3, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Great! This sentence was created using two of the techniques we recently saw - creating sets of words, and using a weighted average stochastic chain. Both of these methods contributed in making it a more meaningful sequence of words. Some of these notions are also captured by Recurrent Neural Networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 1.2: How does changing parameters the sentences generated?\n",
    "\n",
    "Try and use a set of words but using a naive chain, and try a stochastic chain with a low value of k (i.e., 2), and a higher value (i.e., 5). How do these different configurations change the quality of the sequences produced? Below you have sample code to try these out.\n",
    "\n",
    "```python\n",
    "stochastic_chain_sequence(..., chain_length=..., k=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You should be able to use these matrices and the previous functions to be able to create the necessary configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.3: What is a Hidden Markov Model?\n",
    "\n",
    "A 1960s advance (by Leonard Baum and colleagues): Hidden Markov Models are:\n",
    "- a Markov model in which the system modeled is assumed to be a Markov process/chain with unobservable (\"hidden\") states. \n",
    "- HMM assumes there is another surrogate process whose behavior \"depends\" on the state--you learn about the state by observing the surrogate process. \n",
    "- HMMs have successfully been applied in fields where the goal is to recover a data sequence not immediately observable (but other data that depend on the sequence are).\n",
    "- The first dominant application: Speech and text processing (1970s)\n",
    "\n",
    "In this sub-section we will use the python library [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/tutorial.html#training-hmm-parameters-and-inferring-the-hidden-states), which is part of the *scikit-learn* ecosystem. [nlg-with-hmmlearn](https://github.com/mfilej/nlg-with-hmmlearn) offers useful code snippets to adapt ```hmmlearn``` for text data. Because we are using a package that offers many out of the box implementations for HMMs, we don't have to worry about the states, transition matrices, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "sentences = brown.sents(categories=category)\n",
    "words = [word.lower() for sentence in sentences for word in sentence]\n",
    "lengths = [len(sentence) for sentence in sentences]\n",
    "alphabet = set(words)\n",
    "\n",
    "# Encode words\n",
    "le = LabelEncoder()\n",
    "_ = le.fit(list(alphabet))\n",
    "\n",
    "# Find word freqeuncies\n",
    "seq = le.transform(words)\n",
    "features = np.fromiter(seq, np.int64)\n",
    "features = np.atleast_2d(features).T\n",
    "fd = FreqDist(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have our data setup, we can create our model. We use a multinomial HMM with 8 states, and can either do a random initialisation or use word frequences. We recommend trying both options!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function to create default Multinomial HMM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function to create default Multinomial HMM model\n",
    "def get_model(num_states):\n",
    "  print(\"Initial parameter estimation using built-in method\")\n",
    "  model = hmm.MultinomialHMM(n_components=num_states, init_params='ste')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function to create default Multinomial HMM model information of relative frequencies of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function to create default Multinomial HMM model information of relative frequencies of words\n",
    "def frequencies(num_states):\n",
    "  print(\"Initial parameter estimation using relative frequencies\")\n",
    "\n",
    "  frequencies = np.fromiter((fd.freq(i) for i in range(len(alphabet))),\n",
    "                            dtype=np.float64)\n",
    "  emission_prob = np.stack([frequencies]*num_states)\n",
    "\n",
    "  model = hmm.MultinomialHMM(n_components=num_states, init_params='st')\n",
    "  model.emissionprob_ = emission_prob\n",
    "  return model\n",
    "\n",
    "\n",
    "print(frequencies(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**:\n",
    "\n",
    "The following lines of code are commented out because they take a long time (~17 mins for default Brown corpus categories). \n",
    "\n",
    "If you do not have that time, you can download the default model to try to generate text. You have to uncomment the appropriate lines.\n",
    "\n",
    "**Note:** Either you may want to uncomment Line 11 or Line 14, not both, as the output variable `model` will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Fitting a default multinomial HMM. This is lengthy (~17 mins)\n",
    "def run_model(features, length, num_states):\n",
    "  model = get_model(num_states)\n",
    "  model = model.fit(features, lengths)\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "num_states = 8\n",
    "## Uncomment, if you have time!\n",
    "# model = run_model(features, lengths, num_states)\n",
    "\n",
    "## Another way to get a model is to use default frequencies when initialising the model\n",
    "# model = frequencies(num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Alternatively, you could use a saved model. Here is a [link](https://drive.google.com/file/d/1IymcmcO48V6q3x-6dhf7-OU5NByo5W2F/view?usp=sharing) to the default model, which you can download and then upload into Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute this cell to download the saved model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Execute this cell to download the saved model.\n",
    "## code to load the saved model\n",
    "url = \"https://osf.io/5k6cs/download\"\n",
    "urllib.request.urlretrieve(url, 'model_w2d3_t1.pkl')\n",
    "with open(\"model_w2d3_t1.pkl\", \"rb\") as file:\n",
    "  model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function to generate words given a hmm model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Function to generate words given a hmm model\n",
    "def generate_text(model, num_lines = 5, random_len=15):\n",
    "  for _i in range(num_lines):\n",
    "    set_seed(_i)\n",
    "    symbols, _states = model.sample(random_len)\n",
    "\n",
    "    output = le.inverse_transform(np.squeeze(symbols))\n",
    "    for word in output:\n",
    "      print(word, end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "generate_text(model, num_lines=2, random_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We see that a hidden markov model also does well in generating text. We encourage you to try out different initialisations and hyperparameters to see how the model does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exercise 1.3: Transition probabilities \n",
    "\n",
    "\n",
    "We have seen how we can use sequences of text to form probability chains, as well as how we can use out of the box models to generate text. In this exercise, you will be using your own data to generate sequences using ```hmmlearn``` or any other implementation of a markov model. Explore the transition probabilities in your corpus and generate sentences. For example, one such exploration can be - how does using a model with the word frequencies incorporated in compare to using a default model?\n",
    "\n",
    "Perform any one such comparison or exploration, and generate 3 sentences or 50 words using your model. You should be able to use all the existing functions defined for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# load your own dataset and create a model using the frequencies based HMM model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Useful links for Markov Models and HMM:\n",
    "\n",
    "Here are some useful links if you wish to explore this topic further.\n",
    "\n",
    "- [Markov Chain Text](https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6)\n",
    "\n",
    "- [Python QuantEcon: Finite Markov Chains with Finance](https://python.quantecon.org/finite_markov.html)\n",
    "\n",
    "- [Markov Models from the ground up, with python](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/)\n",
    "\n",
    "- [GenTex](https://github.com/nareshkumar66675/GenTex)\n",
    "\n",
    "- [HMM learn](https://hmmlearn.readthedocs.io/en/latest/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 2: Textual Dimension Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Textual Dimension Reduction\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1oM4y1P7Mn\", width=730, height=410, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"kweySXAZ1os\", width=730, height=410, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "Words or subword units such as morphemes are the basic units that we use to express meaning  in language. The technique of mapping words to vectors of real numbers is known as word embedding. \n",
    "\n",
    "Word2vec is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm--as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let’s break this down:\n",
    "\n",
    "... \"use the kitchen knife to chop the vegetables\"…\n",
    "\n",
    "**C1   C2   C3   T   C4   C5   C6   C7**\n",
    "\n",
    "Here, the target word is knife, and the context words are the ones in its immediate (6-word) window. \n",
    "The first word2vec method we’ll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another. \n",
    "\n",
    "This method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away. \n",
    "\n",
    "Another word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Creating Word Embeddings\n",
    "\n",
    "We will create embeddings for a subset of categories in [Brown corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html).  In order to achieve this task we will use [gensim](https://radimrehurek.com/gensim/) library to create word2vec embeddings. Gensim’s word2vec expects a sequence of sentences as its input. Each sentence is a list of words.\n",
    "Calling `Word2Vec(sentences, iter=1)` will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. \n",
    "`Word2vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:\n",
    "\n",
    "`model = Word2Vec(sentences, min_count=10)  # default value is 5`\n",
    "\n",
    "\n",
    "A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n",
    "\n",
    "Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:\n",
    "\n",
    "`model = Word2Vec(sentences, size=200)  # default value is 100`\n",
    "\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n",
    "\n",
    "The last of the major parameters (full list [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:\n",
    "\n",
    "`model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "category = ['editorial', 'fiction', 'government', 'mystery', 'news', 'religion',\n",
    "            'reviews', 'romance', 'science_fiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def create_word2vec_model(category='news', size=50, sg=1, min_count=5):\n",
    "  try:\n",
    "    sentences = brown.sents(categories=category)\n",
    "    model = Word2Vec(sentences, vector_size=size, sg=sg, min_count=min_count)\n",
    "\n",
    "  except (AttributeError, TypeError):\n",
    "      raise AssertionError('Input variable \"category\" should be a string or list,'\n",
    "      '\"size\", \"sg\", \"min_count\" should be integers')\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_dictionary(model):\n",
    "  words = list(model.wv.key_to_index)\n",
    "  return words\n",
    "\n",
    "def get_embedding(word, model):\n",
    "  if word in model.wv.key_to_index:\n",
    "    return model.wv[word]\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "all_categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "w2vmodel = create_word2vec_model(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(model_dictionary(w2vmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(get_embedding('weather', w2vmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Visualizing Word Embedding\n",
    "\n",
    "We can now obtain the word embeddings for any word in the dictionary using word2vec. Let's visualize these embeddings to get an inuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use `tSNE` (t-distributed stochastic neighbor embedding), a statistical method for dimensionality deduction that allow us to visualize high-dimensional data in a 2D or 3D space. Here, we will use `tSNE` from [`scikit-learn`] module(https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (if you are not familiar with this method, think about `PCA`) to project our high dimensional embeddings in the 2D space.\n",
    "\n",
    "\n",
    "For each word in `keys`, we pick the top 10 similar words (using cosine similarity) and plot them.  \n",
    "\n",
    " What should be the arrangement of similar words?\n",
    " What should be arrangement of the key clusters with respect to each other?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "keys = ['voters', 'magic', 'love', 'God', 'evidence', 'administration', 'governments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def get_cluster_embeddings(keys):\n",
    "  embedding_clusters = []\n",
    "  word_clusters = []\n",
    "\n",
    "  # find closest words and add them to cluster\n",
    "  for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    if not word in w2vmodel.wv.key_to_index:\n",
    "      print('The word ', word, 'is not in the dictionary')\n",
    "      continue\n",
    "\n",
    "    for similar_word, _ in w2vmodel.wv.most_similar(word, topn=10):\n",
    "      words.append(similar_word)\n",
    "      embeddings.append(w2vmodel.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "  # get embeddings for the words in clusers\n",
    "  embedding_clusters = np.array(embedding_clusters)\n",
    "  n, m, k = embedding_clusters.shape\n",
    "  tsne_model_en_2d = TSNE(perplexity=10, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "  embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "  return embeddings_en_2d, word_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, embedding_clusters,\n",
    "                            word_clusters, a, filename=None):\n",
    "  plt.figure(figsize=(16, 9))\n",
    "  colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "  for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "    x = embeddings[:, 0]\n",
    "    y = embeddings[:, 1]\n",
    "    plt.scatter(x, y, color=color, alpha=a, label=label)\n",
    "    for i, word in enumerate(words):\n",
    "      plt.annotate(word,\n",
    "                   alpha=0.5,\n",
    "                   xy=(x[i], y[i]),\n",
    "                   xytext=(5, 2),\n",
    "                   textcoords='offset points',\n",
    "                   ha='right',\n",
    "                   va='bottom',\n",
    "                   size=10)\n",
    "  plt.legend(loc=\"lower left\")\n",
    "  plt.title(title)\n",
    "  plt.grid(True)\n",
    "  if filename:\n",
    "    plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "embeddings_en_2d, word_clusters = get_cluster_embeddings(keys)\n",
    "tsne_plot_similar_words('Similar words from Brown Corpus', keys, embeddings_en_2d, word_clusters, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Exploring meaning with word embeddings\n",
    "\n",
    "While word2vec was the method that started it all, research has since boomed, and we now have more sophisticated ways to represent words. One such method is FastText, developed at Facebook AI research, which breaks words into sub-words: such a technique also allows us to create embedding representations for unseen words. In this section, we will explore how semantics and meaning are captured using embedidngs, after downloading a pre-trained FastText model. Downloading pre-trained models is a way for us to plug in word embeddings and explore them without training them ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download FastText English Embeddings of dimension 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Download FastText English Embeddings of dimension 100\n",
    "\n",
    "fname = 'cc.en.100.bin.zip'\n",
    "download_str = \"Downloading\"\n",
    "if os.path.exists(fname):\n",
    "  download_str = \"Redownloading\"\n",
    "  !rm -rf $fname\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zipurl = 'https://osf.io/w9sr7/download'\n",
    "print(f\"{download_str} and unzipping the file... Please wait.\")\n",
    "with urlopen(zipurl) as zipresp:\n",
    "  with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "    zfile.extractall('.')\n",
    "print(\"Download completed!\")\n",
    "\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load 100 dimension FastText Vectors using FastText library\n",
    "ft_en_vectors = fasttext.load_model('cc.en.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Length of the embedding is: {len(ft_en_vectors.get_word_vector('king'))}\")\n",
    "print(f\"Embedding for the word King is: {ft_en_vectors.get_word_vector('king')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Cosine similarity is used for similarities between words. Similarity is a scalar between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now find the 10 most similar words to \"King\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ft_en_vectors.get_nearest_neighbors(\"king\", 10)  # Most similar by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Video 3: Semantic Measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Semantic Measurements\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV15w411R7SW\", width=730, height=410, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"Y45KIAOw4OY\", width=730, height=410, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "More on similarity between words. Let's check how similar different pairs of word are. Feel free to play around.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def getSimilarity(word1, word2):\n",
    "  v1 = ft_en_vectors.get_word_vector(word1)\n",
    "  v2 = ft_en_vectors.get_word_vector(word2)\n",
    "  return cosine_similarity(v1, v2)\n",
    "\n",
    "print(\"Similarity between the words King and Queen: \", getSimilarity(\"king\", \"queen\"))\n",
    "print(\"Similarity between the words King and Knight: \", getSimilarity(\"king\", \"knight\"))\n",
    "print(\"Similarity between the words King and Rock: \", getSimilarity(\"king\", \"rock\"))\n",
    "print(\"Similarity between the words King and Twenty: \", getSimilarity(\"king\", \"twenty\"))\n",
    "\n",
    "## Try the same for two more pairs\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Homonym Words$^\\dagger$\n",
    "\n",
    "Find the similarity for homonym words with their different meanings. The first one has been implemented for you.\n",
    "\n",
    "\n",
    "$^\\dagger$: Two or more words having the same spelling or pronunciation but different meanings and origins are called *homonyms*. E.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#######################     Words with multiple meanings     ##########################\n",
    "print(\"Similarity between the words Cricket and Insect: \", getSimilarity(\"cricket\", \"insect\"))\n",
    "print(\"Similarity between the words Cricket and Sport: \", getSimilarity(\"cricket\", \"sport\"))\n",
    "\n",
    "## Try the same for two more pairs\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Word Analogies\n",
    "\n",
    "Embeddings can be used to find word analogies.\n",
    "Let's try it:\n",
    "1.   Man : Woman  ::  King : _____\n",
    "2.  Germany: Berlin :: France : ______\n",
    "3.  Leaf : Tree  ::  Petal : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Use get_analogies() funnction. The words have to be in the order Positive, negative,  Positve\n",
    "\n",
    "# Man : Woman  ::  King : _____\n",
    "# Positive=(woman, king), Negative=(man)\n",
    "print(ft_en_vectors.get_analogies(\"woman\", \"man\", \"king\",1))\n",
    "\n",
    "# Germany: Berlin :: France : ______\n",
    "# Positive=(berlin, frannce), Negative=(germany)\n",
    "print(ft_en_vectors.get_analogies(\"berlin\", \"germany\", \"france\",1))\n",
    "\n",
    "# Leaf : Tree  ::  Petal : _____\n",
    "# Positive=(tree, petal), Negative=(leaf)\n",
    "print(ft_en_vectors.get_analogies(\"tree\", \"leaf\", \"petal\",1))\n",
    "\n",
    "# Hammer : Nail  ::  Comb : _____\n",
    "# Positive=(nail, comb), Negative=(hammer)\n",
    "print(ft_en_vectors.get_analogies(\"nail\", \"hammer\", \"comb\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "But, does it always work?\n",
    "\n",
    "\n",
    "1.   Poverty : Wealth  :: Sickness : _____\n",
    "2.   train : board :: horse : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Poverty : Wealth  :: Sickness : _____\n",
    "print(ft_en_vectors.get_analogies(\"wealth\", \"poverty\", \"sickness\",1))\n",
    "\n",
    "# train : board :: horse : _____\n",
    "print(ft_en_vectors.get_analogies(\"board\", \"train\", \"horse\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Neural Net with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's use the pretrained FastText embeddings to train a neural network on the IMDB dataset. \n",
    "\n",
    "To recap, the data consists of reviews and sentiments attached to it. It is a binary classification task. As a simple preview of the upcoming neural networks, we are going to introduce neural net with word embeddings. We'll see detailed networks in the next tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 3.1: Simple Feed Forward Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This will load 300 dim FastText embeddings. It will take around 2-3 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Define a vanilla neural network with linear layers. Then average the word embeddings to get an embedding for the entire review.\n",
    "The neural net will have one hidden layer of size 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download embeddings and clear old variables to clean memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Execute this cell!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Download embeddings and clear old variables to clean memory.\n",
    "# @markdown #### Execute this cell!\n",
    "if 'ft_en_vectors' in locals():\n",
    "  del ft_en_vectors\n",
    "if 'w2vmodel' in locals():\n",
    "  del w2vmodel\n",
    "\n",
    "embedding_fasttext = FastText('simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Load the Dataset\n",
    "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(embedding_fasttext, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self, output_size, hidden_size, vocab_size, embedding_length,\n",
    "               word_embeddings):\n",
    "    super(NeuralNet, self).__init__()\n",
    "\n",
    "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "    self.word_embeddings.weight = nn.Parameter(word_embeddings,\n",
    "                                               requires_grad=False)\n",
    "    self.fc1 = nn.Linear(embedding_length, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    input = self.word_embeddings(inputs)  # convert text to embeddings\n",
    "    ####################################################################\n",
    "    # Fill in missing code below (...)\n",
    "    raise NotImplementedError(\"Fill in the Neural Net\")\n",
    "    ####################################################################\n",
    "    # Average the word embeddings in a sentence\n",
    "    # Use torch.nn.functional.avg_pool2d to compute the averages\n",
    "    pooled = ...\n",
    "\n",
    "    # Pass the embeddings through the neural net\n",
    "    # A fully-connected layer\n",
    "    x = ...\n",
    "    # ReLU activation\n",
    "    x = ...\n",
    "    # Another fully-connected layer\n",
    "    x = ...\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Uncomment to check your code\n",
    "# nn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)\n",
    "# print(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial1_Solution_e80c284c.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "NeuralNet(\n",
    "  (word_embeddings): Embedding(100, 300)\n",
    "  (fc1): Linear(in_features=300, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training and Testing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Training and Testing Functions\n",
    "\n",
    "# @markdown #### `train(model, device, train_iter, valid_iter, epochs, learning_rate)`\n",
    "# @markdown #### `test(model, device, test_iter)`\n",
    "\n",
    "def train(model, device, train_iter, valid_iter, epochs, learning_rate):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "    steps = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "      text = batch.text[0]\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      # add micro for coding training loop\n",
    "      optimizer.zero_grad()\n",
    "      output = model(text)\n",
    "      loss = criterion(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      steps += 1\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # get accuracy\n",
    "      _, predicted = torch.max(output, 1)\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "    train_loss.append(running_loss/len(train_iter))\n",
    "    train_acc.append(correct/total)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, '\n",
    "          f'Training Loss: {running_loss/len(train_iter):.4f}, '\n",
    "          f'Training Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "    # evaluate on validation data\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for idx, batch in enumerate(valid_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        text, target = text.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    validation_loss.append(running_loss/len(valid_iter))\n",
    "    validation_acc.append(correct/total)\n",
    "\n",
    "    print (f'Validation Loss: {running_loss/len(valid_iter):.4f}, '\n",
    "           f'Validation Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc\n",
    "\n",
    "\n",
    "def test(model, device, test_iter):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "      text = batch.text[0]\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      outputs = model(text)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "learning_rate = 0.0003\n",
    "output_size = 2\n",
    "hidden_size = 128\n",
    "embedding_length = 300\n",
    "epochs = 15\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "# Model set-up\n",
    "nn_model = NeuralNet(output_size,\n",
    "                     hidden_size,\n",
    "                     vocab_size,\n",
    "                     embedding_length,\n",
    "                     word_embeddings)\n",
    "nn_model.to(DEVICE)\n",
    "nn_start_time = time.time()\n",
    "set_seed(522)\n",
    "nn_train_loss, nn_train_acc, nn_validation_loss, nn_validation_acc = train(nn_model,\n",
    "                                                                           DEVICE,\n",
    "                                                                           train_iter,\n",
    "                                                                           valid_iter,\n",
    "                                                                           epochs,\n",
    "                                                                           learning_rate)\n",
    "print(f\"--- Time taken to train = {(time.time() - nn_start_time)} seconds ---\")\n",
    "test_accuracy = test(nn_model, DEVICE, test_iter)\n",
    "print(f'\\n\\nTest Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plot_train_val(np.arange(0, epochs), nn_train_acc, nn_validation_acc,\n",
    "               'train accuracy', 'val accuracy',\n",
    "               'Neural Net on IMDB text classification', 'accuracy',\n",
    "               color='C0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(212)\n",
    "plot_train_val(np.arange(0, epochs), nn_train_loss,\n",
    "               nn_validation_loss,\n",
    "               'train loss', 'val loss',\n",
    "               '',\n",
    "               'loss [a.u.]',\n",
    "               color='C0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we explored two different concepts linked to sequences, and text in particular, that will be the conceptual foundation for Recurrent Neural Networks.\n",
    "\n",
    "The first concept was that of sequences and probabilities. We saw how we can model language as sequences of text, and use this analogy to generate text. Such a setup is also used to classify text or identify parts of speech. We can either build chains manually using simple python and numerical computation, or use a package such as ```hmmlearn``` that allows us to train models a lot easier. These notions of sequences and probabilities (i.e, creating language models!) are key to the internals of a recurrent neural network as well. \n",
    "\n",
    "The second concept is that of word embeddings, now a mainstay of natural language processing. By using a neural network to predict context of words, these neural networks learn internal representions of words that are a decent approximation of semantic meaning (i.e embeddings!). We saw how these embeddings can be visualised, as well as how they capture meaning. We finally saw how they can be integrated into neural networks to better classify text documents."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "W2D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}