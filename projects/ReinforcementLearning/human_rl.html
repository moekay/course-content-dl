
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Using RL to Model Cognitive Tasks &#8212; Neuromatch Academy: Deep Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../_static/nma-dl-logo-square-4xp.jpeg"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Natural Language Processing" href="../NaturalLanguageProcessing/README.html" />
    <link rel="prev" title="Performance Analysis of DQN Algorithm on the Lunar Lander task" href="lunar_lander.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/nma-dl-logo-square-4xp.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/Schedule/schedule_intro.html">
   Schedule
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/daily_schedules.html">
     General schedule
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/shared_calendars.html">
     Shared calendars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/timezone_widget.html">
     Timezone widget
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/TechnicalHelp/Discord.html">
     Using Discord
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Training loop of CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     Tutorial 2: Facial recognition using modern convnets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Advanced topics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Introduction to projects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/project_guidance.html">
   Daily guide for projects
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../docs/projects_overview.html">
   Project Templates
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../ComputerVision/README.html">
     Computer Vision
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
    <label for="toctree-checkbox-20">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/screws.html">
       Setup matplotlib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/image_alignment.html">
       Image Alignment
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="README.html">
     Reinforcement Learning
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
    <label for="toctree-checkbox-21">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Using RL to Model Cognitive Tasks
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
    <label for="toctree-checkbox-22">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Neuroscience/README.html">
     Neuroscience
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
    <label for="toctree-checkbox-23">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/datasets_and_models.html">
   Models and Data sets
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/projects/ReinforcementLearning/human_rl.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/NeuromatchAcademy/course-content-dl"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Fprojects/ReinforcementLearning/human_rl.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#todo">
   TODO
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#datasets">
   Datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#n-back">
     N-back
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-rl-environments-for-cognitive-tests">
   Create RL environments for cognitive tests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#human-dataset">
   Human dataset
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/fix_links/projects/ReinforcementLearning/human_rl.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="using-rl-to-model-cognitive-tasks">
<h1>Using RL to Model Cognitive Tasks<a class="headerlink" href="#using-rl-to-model-cognitive-tasks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="todo">
<h2>TODO<a class="headerlink" href="#todo" title="Permalink to this headline">¶</a></h2>
<p><strong>⚠️ This is a work in progress.</strong></p>
<ul class="simple">
<li><p>Implement more intelligent agents (using Acme).</p></li>
<li><p>find appropriate behavioral datasets.</p></li>
<li><p>add more documentations.</p></li>
<li><p>utility functions to plot the agent performance and the env.</p></li>
</ul>
</div>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).</p></li>
<li><p>Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).</p></li>
<li><p>Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.</p></li>
<li><p>This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.</p></li>
<li><p>In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.</p></li>
<li><p>One example would be the N-back task.</p>
<ul>
<li><p>In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.</p></li>
<li><p>The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a <code class="docutils literal notranslate"><span class="pre">match</span></code>.</p></li>
</ul>
</li>
<li><p>Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities.</p></li>
</ul>
</div>
<div class="section" id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>HCP WM task (<a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI">NMA-CN HCP notebooks</a>)</p></li>
</ul>
<p>Any dataset that used cognitive tests would work.
Question: limit to behavioral data vs fMRI?
Question: Which stimuli and actions to use?
classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)
The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back.</p>
<div class="section" id="n-back">
<h3>N-back<a class="headerlink" href="#n-back" title="Permalink to this headline">¶</a></h3>
<p>In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.</p>
<p>In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses.</p>
</div>
</div>
<div class="section" id="create-rl-environments-for-cognitive-tests">
<h2>Create RL environments for cognitive tests<a class="headerlink" href="#create-rl-environments-for-cognitive-tests" title="Permalink to this headline">¶</a></h2>
<p>First install and import all the required packages:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>%%capture

# Install required packages

!pip install dm-env
!pip install dm-sonnet
!pip install dm-acme==0.2.0 dm-acme[tf]==0.2.0 dm-acme[reverb]==0.2.0

# Imports

from acme.tf import networks

import time

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

import dm_env
from dm_env import specs
import acme
import sonnet as snt
from acme import EnvironmentLoop
from acme.testing import fakes
import acme.tf.networks as networks
from acme.tf import utils as tf2_utils
from acme.agents.tf.d4pg import D4PG
from acme.agents.tf.ddpg import DDPG
from acme.agents.tf.dqn import DQN
from acme.agents.tf.dmpo import DistributionalMPO
from acme.utils.loggers import TerminalLogger
from acme.tf import networks
from acme import specs
from acme import wrappers
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="human-dataset">
<h2>Human dataset<a class="headerlink" href="#human-dataset" title="Permalink to this headline">¶</a></h2>
<p>We need a dataset of human perfoming a N-back test, with the following features:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">participant_id</span></code>: following the BIDS format, it contains a unique identifier for each participant.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trial_index</span></code>: same as <code class="docutils literal notranslate"><span class="pre">time_step</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stimulus</span></code>: same as <code class="docutils literal notranslate"><span class="pre">observation</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response</span></code>: same as <code class="docutils literal notranslate"><span class="pre">action</span></code>, recorded response by the human subject.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">expected_response</span></code>: correct response.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_correct</span></code>: same as <code class="docutils literal notranslate"><span class="pre">reward</span></code>, whether the human subject responded correctly.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_time</span></code>: won’t be used here.</p></li>
</ul>
<p>Here we generate a mock dataset with those features, but remember to replace it with real human data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_mock_nback_dataset</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                <span class="n">n_participants</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                                <span class="n">stimulus_choices</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="s1">&#39;ABCDEF&#39;</span><span class="p">),</span>
                                <span class="n">response_choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;match&#39;</span><span class="p">,</span> <span class="s1">&#39;non-match&#39;</span><span class="p">]):</span>
  <span class="sd">&quot;&quot;&quot;Generate a mock dataset for the N-back task.&quot;&quot;&quot;</span>

  <span class="n">n_rows</span> <span class="o">=</span> <span class="n">n_participants</span> <span class="o">*</span> <span class="n">n_trials</span>

  <span class="n">participant_ids</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;sub-</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_participants</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="n">n_trials</span><span class="p">)</span>
  <span class="n">trial_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_trials</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">n_participants</span>
  <span class="n">stimulus_sequence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">stimulus_choices</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">)</span>

  <span class="n">responses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">response_choices</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">)</span>
  <span class="n">is_corrects</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="n">n_rows</span><span class="p">)</span>
  <span class="n">response_times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_rows</span><span class="p">)</span>

  <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
      <span class="s1">&#39;participant_id&#39;</span><span class="p">:</span> <span class="n">participant_ids</span><span class="p">,</span>
      <span class="s1">&#39;trial_index&#39;</span><span class="p">:</span> <span class="n">trial_indices</span><span class="p">,</span>
      <span class="s1">&#39;stimulus&#39;</span><span class="p">:</span> <span class="n">stimulus_sequence</span><span class="p">,</span>
      <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="n">responses</span><span class="p">,</span>
      <span class="s1">&#39;is_correct&#39;</span><span class="p">:</span> <span class="n">is_corrects</span><span class="p">,</span>
      <span class="c1">#TODO: is_match</span>
      <span class="s1">&#39;response_time&#39;</span><span class="p">:</span> <span class="n">response_times</span>
  <span class="p">})</span>

  <span class="c1"># mark matchig stimuli</span>
  <span class="n">_nback_stim</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;stimulus&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="n">df</span><span class="p">[</span><span class="s1">&#39;expected_response&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;stimulus&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">_nback_stim</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="kc">True</span><span class="p">:</span> <span class="s1">&#39;match&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">:</span> <span class="s1">&#39;non-match&#39;</span><span class="p">})</span>

  <span class="c1"># we don&#39;t care about burn-in trials (trial &lt; N)</span>
  <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;trial_index&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">N</span><span class="p">,</span> <span class="s1">&#39;is_correct&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;trial_index&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">N</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">,</span><span class="s1">&#39;response_time&#39;</span><span class="p">,</span><span class="s1">&#39;expected_response&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">return</span> <span class="n">df</span>

<span class="c1"># ========</span>
<span class="c1"># now generate the actual data with the provided function and plot some of its features</span>
<span class="n">mock_nback_data</span> <span class="o">=</span> <span class="n">generate_mock_nback_dataset</span><span class="p">()</span>

<span class="n">sns</span><span class="o">.</span><span class="n">displot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">mock_nback_data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;response_time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;response time distribution of the mock N-back dataset&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">sns</span><span class="o">.</span><span class="n">displot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">mock_nback_data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;is_correct&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Accuracy distribution of the mock N-back dataset&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.06</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">mock_nback_data</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It receives human N-back data (or mock data if you prefer), and returns what participants performed as the observation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NBack</span><span class="p">(</span><span class="n">dm_env</span><span class="o">.</span><span class="n">Environment</span><span class="p">):</span>

  <span class="n">ACTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;match&#39;</span><span class="p">,</span> <span class="s1">&#39;non-match&#39;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
               <span class="n">episode_steps</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
               <span class="n">stimuli_choices</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="s1">&#39;ABCDEF&#39;</span><span class="p">),</span>
               <span class="n">human_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TODO</span>

<span class="sd">    Args:</span>
<span class="sd">      TODO</span>
<span class="sd">    Returns:</span>
<span class="sd">      TODO</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">episode_steps</span> <span class="o">=</span> <span class="n">episode_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stimuli_choices</span> <span class="o">=</span> <span class="n">stimuli_choices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">episode_steps</span><span class="p">)</span>  <span class="c1"># will be filled in the `reset()`</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_next_step</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># whether mimic humans or reward the agent once it responds optimally.</span>
    <span class="k">if</span> <span class="n">human_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_imitate_human</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">human_data</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">human_subject_data</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_imitate_human</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">human_data</span> <span class="o">=</span> <span class="n">human_data</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">human_subject_data</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_action_history</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_next_step</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action_history</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="c1"># generate a random sequence instead of relying on human data</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">human_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stimuli_choices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode_steps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">human_subject_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">human_data</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;participant_id == participant_id.sample().iloc[0]&#39;</span><span class="p">,</span>
                                                <span class="n">engine</span><span class="o">=</span><span class="s1">&#39;python&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;trial_index&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">human_subject_data</span><span class="p">[</span><span class="s1">&#39;stimulus&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
      <span class="c1"># FIXME should we always use one specific human subject or randomly select one in each episode?</span>
      <span class="c1"># randomly choose a subject from the human data and follow her trials and responses:</span>

    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">restart</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_observation</span><span class="p">())</span>


  <span class="k">def</span> <span class="nf">_episode_return</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_imitate_human</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">human_subject_data</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action_history</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="mi">0</span>

  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reset_next_step</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_imitate_human</span><span class="p">:</span>
      <span class="c1"># if it was the same action as the human subject, then reward the agent</span>
      <span class="n">human_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">human_subject_data</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span><span class="p">]</span>
      <span class="n">agent_action</span> <span class="o">=</span> <span class="n">NBack</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
      <span class="n">step_reward</span> <span class="o">=</span> <span class="p">(</span><span class="n">agent_action</span> <span class="o">==</span> <span class="n">human_action</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># assume the agent is rationale and doesn&#39;t want to reproduce human, reward once the response it correct</span>
      <span class="n">expected_action</span> <span class="o">=</span> <span class="s1">&#39;match&#39;</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">])</span> <span class="k">else</span> <span class="s1">&#39;non-match&#39;</span>
      <span class="n">agent_action</span> <span class="o">=</span> <span class="n">NBack</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
      <span class="n">step_reward</span> <span class="o">=</span> <span class="mf">1.</span> <span class="k">if</span> <span class="p">(</span><span class="n">agent_action</span> <span class="o">==</span> <span class="n">expected_action</span><span class="p">)</span> <span class="k">else</span> <span class="o">-</span><span class="mf">1.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_action_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">agent_action</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="c1"># Check for termination.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_reset_next_step</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="c1"># we are using the mean of total time step rewards as the episode return</span>
      <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">termination</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_episode_return</span><span class="p">(),</span>
                                <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_observation</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="n">step_reward</span><span class="p">,</span>
                               <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_observation</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">observation_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">specs</span><span class="o">.</span><span class="n">BoundedArray</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;nback_stimuli&#39;</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">action_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">specs</span><span class="o">.</span><span class="n">DiscreteArray</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">num_values</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">NBack</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;action&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_observation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

    <span class="c1"># agents observe only the current stimulus</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># TODO uncomment to observe all the previrous stimuli instead of only the current stimulus</span>
    <span class="c1"># obs = self.stimuli[:self.current_step]</span>
    <span class="c1"># obs = &#39;&#39;.join(obs)</span>

    <span class="k">return</span> <span class="n">obs</span> 

  <span class="k">def</span> <span class="nf">plot_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
    <span class="n">stimuli</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stimuli</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;M&#39;</span> <span class="k">if</span> <span class="n">a</span><span class="o">==</span><span class="s1">&#39;match&#39;</span> <span class="k">else</span> <span class="s1">&#39;.&#39;</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action_history</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">HTML</span><span class="p">(</span>
        <span class="s1">&#39;&lt;b&gt;Environment:&lt;/b&gt;&lt;br /&gt;&#39;</span>
        <span class="sa">f</span><span class="s1">&#39;&lt;pre&gt;&lt;b&gt;Stimuli:&lt;/b&gt; </span><span class="si">{</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stimuli</span><span class="p">)</span><span class="si">}</span><span class="s1">&lt;/pre&gt;&#39;</span>
        <span class="sa">f</span><span class="s1">&#39;&lt;pre&gt;&lt;b&gt;Actions:&lt;/b&gt; </span><span class="si">{</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="si">}</span><span class="s1">&lt;/pre&gt;&#39;</span>
    <span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">create_environment</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Utility to create a N-back environment and its spec.&quot;&quot;&quot;</span>

    <span class="c1"># Make sure the environment outputs single-precision floats.</span>
    <span class="n">environment</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">SinglePrecisionWrapper</span><span class="p">(</span><span class="n">NBack</span><span class="p">())</span>

    <span class="c1"># Grab the spec of the environment.</span>
    <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">specs</span><span class="o">.</span><span class="n">make_environment_spec</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span>
</pre></div>
</div>
</div>
</div>
<p>Define a random agent (see W3D2):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">environment_spec</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the number of available actions from the environment spec.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Selects an action uniformly at random.&quot;&quot;&quot;</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">action</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Does not record as the RandomAgent has no use for data.&quot;&quot;&quot;</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Does not record as the RandomAgent has no use for data.&quot;&quot;&quot;</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Does not update as the RandomAgent does not learn from data.&quot;&quot;&quot;</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<p>Init the environment and the agent:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">NBack</span><span class="o">.</span><span class="n">create_environment</span><span class="p">()</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">environment_spec</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;actions:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;observations:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">observations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;rewards:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>

<span class="c1"># DEBUG</span>
<span class="c1"># timestep = environment.step(NBack.ACTIONS[0])  # pytype: dm_env.TimeStep</span>
<span class="c1"># timestep</span>
</pre></div>
</div>
</div>
</div>
<p>Run the loop (see W3D2):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># fittinng parameters</span>
<span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_total_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">log_loss</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_episodes</span> <span class="o">*</span> <span class="mi">32</span>
<span class="n">all_returns</span> <span class="o">=</span> <span class="p">[]</span>


<span class="c1"># main loop</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
  <span class="n">episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">episode_loss</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

  <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

  <span class="c1"># Make the first observation.</span>
  <span class="n">agent</span><span class="o">.</span><span class="n">observe_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

  <span class="c1"># Run an episode</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">timestep</span><span class="o">.</span><span class="n">last</span><span class="p">():</span>
    
    <span class="c1"># DEBUG</span>
    <span class="c1"># print(timestep)</span>

    <span class="c1"># Generate an action from the agent&#39;s policy and step the environment.</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="c1"># Have the agent observe the timestep and let the agent update itself.</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

    <span class="c1"># Book-keeping.</span>
    <span class="n">episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">n_total_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span>

    <span class="k">if</span> <span class="n">log_loss</span><span class="p">:</span>
      <span class="n">episode_loss</span> <span class="o">+=</span> <span class="n">agent</span><span class="o">.</span><span class="n">last_loss</span>

    <span class="k">if</span> <span class="n">n_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">n_total_steps</span> <span class="o">&gt;=</span> <span class="n">n_steps</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="c1"># Collect the results and combine with counts.</span>
  <span class="n">steps_per_second</span> <span class="o">=</span> <span class="n">episode_steps</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s1">&#39;episode&#39;</span><span class="p">:</span> <span class="n">episode</span><span class="p">,</span>
      <span class="s1">&#39;episode_length&#39;</span><span class="p">:</span> <span class="n">episode_steps</span><span class="p">,</span>
      <span class="s1">&#39;episode_return&#39;</span><span class="p">:</span> <span class="n">episode_return</span><span class="p">,</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="n">log_loss</span><span class="p">:</span>
    <span class="n">result</span><span class="p">[</span><span class="s1">&#39;loss_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">episode_loss</span><span class="o">/</span><span class="n">episode_steps</span>

  <span class="n">all_returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>

  <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
  <span class="n">display</span><span class="p">(</span><span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">())</span>
  <span class="c1"># Log the given results.</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">n_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">n_total_steps</span> <span class="o">&gt;=</span> <span class="n">n_steps</span><span class="p">:</span>
    <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;All episode returns:&#39;</span><span class="p">,</span> <span class="n">all_returns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can simplify the environment loop using <a class="reference external" href="https://github.com/deepmind/acme">DeepMind Acme</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">InMemoryLogger</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">loggers</span><span class="o">.</span><span class="n">Logger</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A simple logger that keeps all data in memory.</span>
<span class="sd">  </span>
<span class="sd">  Reference:</span>
<span class="sd">    https://github.com/deepmind/acme/blob/master/acme/utils/loggers/dataframe.py</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">acme</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">loggers</span><span class="o">.</span><span class="n">LoggingData</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_networks_d4pg</span><span class="p">(</span><span class="n">action_spec</span><span class="p">,</span>
                       <span class="n">policy_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
                       <span class="n">critic_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
                       <span class="n">vmin</span><span class="o">=-</span><span class="mf">150.</span><span class="p">,</span>
                       <span class="n">vmax</span><span class="o">=</span><span class="mf">150.</span><span class="p">,</span>
                       <span class="n">num_atoms</span><span class="o">=</span><span class="mi">51</span><span class="p">,</span>
                      <span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Networks for D4PG agent.&quot;&quot;&quot;</span>
  <span class="n">action_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">action_spec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

  <span class="n">policy_network</span> <span class="o">=</span> <span class="n">snt</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf2_utils</span><span class="o">.</span><span class="n">batch_concat</span><span class="p">,</span>
      <span class="n">networks</span><span class="o">.</span><span class="n">LayerNormMLP</span><span class="p">(</span><span class="n">layer_sizes</span><span class="o">=</span><span class="n">policy_layer_sizes</span> <span class="o">+</span> <span class="p">(</span><span class="n">action_size</span><span class="p">,)),</span>
      <span class="n">networks</span><span class="o">.</span><span class="n">TanhToSpec</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">action_spec</span><span class="p">)</span>
      <span class="p">])</span>
  <span class="n">critic_network</span> <span class="o">=</span> <span class="n">snt</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">networks</span><span class="o">.</span><span class="n">CriticMultiplexer</span><span class="p">(</span>
          <span class="n">action_network</span><span class="o">=</span><span class="n">networks</span><span class="o">.</span><span class="n">ClipToSpec</span><span class="p">(</span><span class="n">action_spec</span><span class="p">),</span>
          <span class="n">critic_network</span><span class="o">=</span><span class="n">networks</span><span class="o">.</span><span class="n">LayerNormMLP</span><span class="p">(</span>
              <span class="n">layer_sizes</span><span class="o">=</span><span class="n">critic_layer_sizes</span><span class="p">,</span>
              <span class="n">activate_final</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="p">),</span>
      <span class="n">networks</span><span class="o">.</span><span class="n">DiscreteValuedHead</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> 
                                  <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> 
                                  <span class="n">num_atoms</span><span class="o">=</span><span class="n">num_atoms</span><span class="p">)</span>
      <span class="p">])</span>

  <span class="k">return</span> <span class="n">policy_network</span><span class="p">,</span> <span class="n">critic_network</span>


<span class="k">def</span> <span class="nf">make_networks_dqn</span><span class="p">(</span><span class="n">action_spec</span><span class="p">):</span>
  <span class="n">network</span> <span class="o">=</span> <span class="n">snt</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">snt</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
      <span class="n">snt</span><span class="o">.</span><span class="n">nets</span><span class="o">.</span><span class="n">MLP</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">action_spec</span><span class="o">.</span><span class="n">num_values</span><span class="p">]),</span>
  <span class="p">])</span>
  <span class="k">return</span> <span class="n">network</span>


<span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">snt</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">critic_optimizer</span> <span class="o">=</span> <span class="n">snt</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the N-back environment</span>
<span class="n">env</span><span class="p">,</span> <span class="n">env_spec</span> <span class="o">=</span> <span class="n">NBack</span><span class="o">.</span><span class="n">create_environment</span><span class="p">()</span>

<span class="c1"># DEBUG fake testing environment</span>
<span class="c1"># env = fakes.DiscreteEnvironment(</span>
<span class="c1">#     num_actions=5,</span>
<span class="c1">#     num_observations=50,</span>
<span class="c1">#     obs_dtype=np.float32,</span>
<span class="c1">#     episode_length=50)</span>
<span class="c1"># env_spec = specs.make_environment_spec(env)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DQN agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="n">environment_spec</span><span class="o">=</span><span class="n">env_spec</span><span class="p">,</span>
    <span class="n">network</span><span class="o">=</span><span class="n">make_networks_dqn</span><span class="p">(</span><span class="n">env_spec</span><span class="o">.</span><span class="n">actions</span><span class="p">))</span>

<span class="c1"># D4PG agent</span>
<span class="n">policy_network</span><span class="p">,</span> <span class="n">critic_network</span> <span class="o">=</span> <span class="n">make_networks_d4pg</span><span class="p">(</span><span class="n">env_spec</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">D4PG</span><span class="p">(</span><span class="n">environment_spec</span><span class="o">=</span><span class="n">env_spec</span><span class="p">,</span>
             <span class="n">policy_network</span><span class="o">=</span><span class="n">policy_network</span><span class="p">,</span>
             <span class="n">critic_network</span><span class="o">=</span><span class="n">critic_network</span><span class="p">,</span>
             <span class="n">observation_network</span><span class="o">=</span><span class="n">tf2_utils</span><span class="o">.</span><span class="n">batch_concat</span><span class="p">,</span> <span class="c1"># Identity Op.</span>
             <span class="n">policy_optimizer</span><span class="o">=</span><span class="n">policy_optimizer</span><span class="p">,</span>
             <span class="n">critic_optimizer</span><span class="o">=</span><span class="n">critic_optimizer</span><span class="p">,</span>
             <span class="n">logger</span><span class="o">=</span><span class="n">InMemoryLogger</span><span class="p">())</span>

<span class="c1"># random agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">env_spec</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># training loop</span>
<span class="n">loop</span> <span class="o">=</span> <span class="n">EnvironmentLoop</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">InMemoryLogger</span><span class="p">())</span>
<span class="n">loop</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">)</span>

<span class="c1"># print logs</span>
<span class="n">logs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">loop</span><span class="o">.</span><span class="n">_logger</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
<span class="n">logs</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./projects/ReinforcementLearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="lunar_lander.html" title="previous page">Performance Analysis of DQN Algorithm on the Lunar Lander task</a>
    <a class='right-next' id="next-link" href="../NaturalLanguageProcessing/README.html" title="next page">Natural Language Processing</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Neuromatch<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>