
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Performance Analysis of DQN Algorithm on the Lunar Lander task &#8212; Neuromatch Academy: Deep Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../_static/nma-dl-logo-square-4xp.jpeg"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using RL to Model Cognitive Tasks" href="human_rl.html" />
    <link rel="prev" title="NMA Robolympics: Controlling robots using reinforcement learning" href="robolympics.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/nma-dl-logo-square-4xp.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/Schedule/schedule_intro.html">
   Schedule
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/daily_schedules.html">
     General schedule
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/shared_calendars.html">
     Shared calendars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/timezone_widget.html">
     Timezone widget
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/TechnicalHelp/Discord.html">
     Using Discord
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Training loop of CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     Tutorial 2: Facial recognition using modern convnets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Advanced topics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Introduction to projects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/project_guidance.html">
   Daily guide for projects
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../docs/projects_overview.html">
   Project Templates
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../ComputerVision/README.html">
     Computer Vision
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
    <label for="toctree-checkbox-20">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/screws.html">
       Setup matplotlib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/image_alignment.html">
       Image Alignment
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="README.html">
     Reinforcement Learning
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
    <label for="toctree-checkbox-21">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="human_rl.html">
       Using RL to Model Cognitive Tasks
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
    <label for="toctree-checkbox-22">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Neuroscience/README.html">
     Neuroscience
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
    <label for="toctree-checkbox-23">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/datasets_and_models.html">
   Models and Data sets
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/projects/ReinforcementLearning/lunar_lander.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/NeuromatchAcademy/course-content-dl"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Fprojects/ReinforcementLearning/lunar_lander.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Performance Analysis of DQN Algorithm on the Lunar Lander task
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective:
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-dependencies">
     Install dependencies
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plotting-video-functions">
     Plotting/Video functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-project-ideas">
   Additional Project Ideas
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#play-with-exploration-exploitation-trade-off">
     1. Play with exploration-exploitation trade-off.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reward-shaping">
     2. Reward Shaping
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#identify-the-state-information-crucial-to-its-performance">
     3. Identify the state information crucial to its performance.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extension-to-atari-games">
     4. Extension to Atari Games
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#obstacle-avoidance-and-transfer-learning">
     5. Obstacle Avoidance and Transfer Learning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-transfer-learning-in-minigrid-environment">
     5(b) Transfer Learning in minigrid environment
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preference-based-rl-pbrl">
     6. Preference-Based RL (PBRL)
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ReinforcementLearning/lunar_lander.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="performance-analysis-of-dqn-algorithm-on-the-lunar-lander-task">
<h1>Performance Analysis of DQN Algorithm on the Lunar Lander task<a class="headerlink" href="#performance-analysis-of-dqn-algorithm-on-the-lunar-lander-task" title="Permalink to this headline">¶</a></h1>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content Creators:</strong> Raghuram Bharadwaj Diddigi, Geraud Nangue Tasse, Yamil Vidal, Sanjukta Krishnagopal, Sara Rajaee</p>
<p><strong>Content editors:</strong> Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p></div>
<hr class="docutils" />
<div class="section" id="objective">
<h1>Objective:<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h1>
<p>In this project, the objective is to analyze the performance of the Deep Q-Learning algorithm on an exciting task- Lunar Lander. Before we describe the task, let us focus on two keywords here - analysis and performance. What exactly do we mean by these keywords in the context of Reinforcement Learning (RL)?</p>
<p>In a standard RL setting, an agent learns optimal behavior from an environment through a feedback mechanism to maximize a given objective. Many algorithms have been proposed in the RL literature that an agent can apply to learn the optimal behavior. One such popular algorithm is the Deep Q-Network (DQN). This algorithm makes use of deep neural networks to compute optimal actions. In this project, your goal is to understand the effect of the number of neural network layers on the algorithm’s performance. The performance of the algorithm can be evaluated through two metrics - Speed and Stability.</p>
<p><strong>Speed:</strong> How fast the algorithm reaches the maximum possible reward.</p>
<p><strong>Stability</strong> In some applications (especially when online learning is involved), along with speed, stability of the algorithm, i.e., minimal fluctuations in performance, is equally important.</p>
<p>In this project, you should investigate the following question:</p>
<p><strong>What is the impact of number of neural network layers on speed and stability of the algorithm?</strong></p>
<p>You do not have to write the DQN code from scratch. We have provided a basic implementation of the DQN algorithm. You only have to tune the hyperparameters (neural network size, learning rate, etc), observe the performance, and analyze. More details on this are provided below.</p>
<p>Now, let us discuss the RL task we have chosen, i.e., Lunar Lander. This task consists of the lander and a landing pad marked by two flags. The episode starts with the lander moving downwards due to gravity. The objective is to land safely using different engines available on the lander with zero speed on the landing pad as quickly and fuel efficient as possible. Reward for moving from the top of the screen and landing on landing pad with zero speed is between 100 to 140 points. Each leg ground contact yields a reward of 10 points. Firing main engine leads to a reward of -0.3 points in each frame. Firing the side engine leads to a reward of -0.03 points in each frame. An additional reward of -100 or +100 points is received if the lander crashes or comes to rest respectively which also leads to end of the episode.</p>
<p>The input state of the Lunar Lander consists of following components:</p>
<ol class="simple">
<li><p>Horizontal Position</p></li>
<li><p>Vertical Position</p></li>
<li><p>Horizontal Velocity</p></li>
<li><p>Vertical Velocity</p></li>
<li><p>Angle</p></li>
<li><p>Angular Velocity</p></li>
<li><p>Left Leg Contact</p></li>
<li><p>Right Leg Contact</p></li>
</ol>
<p>The actions of the agents are:</p>
<ol class="simple">
<li><p>Do Nothing</p></li>
<li><p>Fire Main Engine</p></li>
<li><p>Fire Left Engine</p></li>
<li><p>Fire Right Engine</p></li>
</ol>
<p>![lunarlander_image.png](data:image/png;base64,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)</p>
</div>
<hr class="docutils" />
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="section" id="install-dependencies">
<h2>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># @title Install dependencies
!pip install stable-baselines3 &gt; /dev/null
!pip install box2d-py &gt; /dev/null
!pip install gym pyvirtualdisplay &gt; /dev/null 2&gt;&amp;1
!sudo apt-get install -y xvfb python-opengl ffmpeg &gt; /dev/null 2&gt;&amp;1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">base64</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">stable_baselines3</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">DQN</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.evaluation</span> <span class="kn">import</span> <span class="n">evaluate_policy</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.results_plotter</span> <span class="kn">import</span> <span class="n">ts2xy</span><span class="p">,</span> <span class="n">load_results</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.callbacks</span> <span class="kn">import</span> <span class="n">EvalCallback</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.results_plotter</span> <span class="kn">import</span> <span class="n">ts2xy</span><span class="p">,</span> <span class="n">load_results</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.vec_env</span> <span class="kn">import</span> <span class="n">VecFrameStack</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.env_util</span> <span class="kn">import</span> <span class="n">make_atari_env</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">spaces</span>
<span class="kn">from</span> <span class="nn">gym.wrappers</span> <span class="kn">import</span> <span class="n">Monitor</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">pyvirtualdisplay</span> <span class="kn">import</span> <span class="n">Display</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">ipythondisplay</span>
</pre></div>
</div>
</div>
</div>
<p>#<strong>Basic DQN Implementation</strong></p>
<p>We will now implement the DQN algorithm using the existing code base. We encourage you to understand this example and re-use it in an application/project of your choice!</p>
<p>Now, let us set some hyperparameters for our algorithm. This is the only part you would play around with, to solve the first part of the project.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">]</span> <span class="c1">#This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons. </span>
                    <span class="c1">#If you want three layers with 64 neurons each, set the value to [64,64,64] and so on. </span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#This is the step-size with which the gradient descent is carried out. </span>
                      <span class="c1">#Tip: Use smaller step-sizes for larger networks. </span>
                    
</pre></div>
</div>
</div>
</div>
<p>Now, let us setup our model and the DQN algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_dir</span> <span class="o">=</span> <span class="s2">&quot;/tmp/gym/&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>
<span class="c1">#You can also load other environments like cartpole, MountainCar, Acrobot. Refer to https://gym.openai.com/docs/ for descriptions.</span>
<span class="c1">#For example, if you would like to load Cartpole, just replace the above statement with &quot;env = gym.make(&#39;CartPole-v1&#39;)&quot;. </span>

<span class="n">env</span> <span class="o">=</span> <span class="n">stable_baselines3</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">log_dir</span> <span class="p">)</span>

<span class="n">callback</span> <span class="o">=</span> <span class="n">EvalCallback</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="n">log_path</span> <span class="o">=</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1">#For evaluating the performance of the agent periodically and logging the results.</span>
<span class="n">policy_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">activation_fn</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                     <span class="n">net_arch</span><span class="o">=</span><span class="n">nn_layers</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="s2">&quot;MlpPolicy&quot;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span><span class="n">policy_kwargs</span> <span class="o">=</span> <span class="n">policy_kwargs</span><span class="p">,</span> 
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1">#for simplicity, we are not doing batch update.</span>
            <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1">#size of experience of replay buffer. Set to 1 as batch update is not done</span>
            <span class="n">learning_starts</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1">#learning starts immediately!</span>
            <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="c1">#discount facto. range is between 0 and 1. </span>
            <span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1">#the soft update coefficient for updating the target network</span>
            <span class="n">target_update_interval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1">#update the target network immediately.</span>
            <span class="n">train_freq</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="s2">&quot;step&quot;</span><span class="p">),</span> <span class="c1">#train the network at every step.</span>
            <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="c1">#the maximum value for the gradient clipping</span>
            <span class="n">exploration_initial_eps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="c1">#initial value of random action probability</span>
            <span class="n">exploration_fraction</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="c1">#fraction of entire training period over which the exploration rate is reduced</span>
            <span class="n">gradient_steps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="c1">#number of gradient steps </span>
            <span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="c1">#seed for the pseudo random generators</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1. </span>

<span class="c1"># You can also experiment with other RL algorithms like A2C, PPO, DDPG etc. Refer to  https://stable-baselines3.readthedocs.io/en/master/guide/examples.html</span>
<span class="c1">#for documentation. For example, if you would like to run DDPG, just replace &quot;DQN&quot; above with &quot;DDPG&quot;.</span>
</pre></div>
</div>
</div>
</div>
<p>Before we train the model, let us look at an instance of Lunar Lander <strong>before training</strong>.</p>
<p><strong>Note:</strong> The following code for rendering the video is taken from https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb#scrollTo=T9RpF49oOsZj</p>
</div>
<div class="section" id="plotting-video-functions">
<h2>Plotting/Video functions<a class="headerlink" href="#plotting-video-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting/Video functions</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">pyvirtualdisplay</span> <span class="kn">import</span> <span class="n">Display</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">ipythondisplay</span>

<span class="n">display</span> <span class="o">=</span> <span class="n">Display</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1400</span><span class="p">,</span> <span class="mi">900</span><span class="p">))</span>
<span class="n">display</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Utility functions to enable video recording of gym environment </span>
<span class="sd">and displaying it.</span>
<span class="sd">To enable video, just do &quot;env = wrap_env(env)&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">show_video</span><span class="p">():</span>
  <span class="n">mp4list</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;video/*.mp4&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mp4list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">mp4</span> <span class="o">=</span> <span class="n">mp4list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">video</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">mp4</span><span class="p">,</span> <span class="s1">&#39;r+b&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
    <span class="n">ipythondisplay</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;&lt;video alt=&quot;test&quot; autoplay </span>
<span class="s1">                loop controls style=&quot;height: 400px;&quot;&gt;</span>
<span class="s1">                &lt;source src=&quot;data:video/mp4;base64,</span><span class="si">{0}</span><span class="s1">&quot; type=&quot;video/mp4&quot; /&gt;</span>
<span class="s1">             &lt;/video&gt;&#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoded</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">))))</span>
  <span class="k">else</span><span class="p">:</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Could not find video&quot;</span><span class="p">)</span>
    

<span class="k">def</span> <span class="nf">wrap_env</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s1">&#39;./video&#39;</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">env</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_env</span> <span class="o">=</span> <span class="n">wrap_env</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">))</span>
<span class="n">observation</span> <span class="o">=</span> <span class="n">test_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
  <span class="n">test_env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
  <span class="n">action</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">test_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> 
  <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
  <span class="k">if</span> <span class="n">done</span><span class="p">:</span> 
    <span class="k">break</span><span class="p">;</span>

<span class="c1"># print(total_reward)</span>
<span class="n">test_env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">show_video</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>From the video above, we see that the lander has crashed!
It is now the time for training!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>
<span class="c1"># The performance of the training will be printed every 10 episodes. Change it to 1, if you wish to</span>
<span class="c1"># view the performance at every training episode. </span>
</pre></div>
</div>
</div>
</div>
<p>The training takes time. We encourage you to analyze the output logs (set verbose to 1 to print the output logs). The main component of the logs that you should track is “ep_rew_mean” (mean of episode rewards). As the training proceeds, the value of “ep_rew_mean” should increase. The improvement need not be monotonic, but the trend should be upwards!</p>
<p>Along with training, we are also periodically evaluating the performance of the current model during the training. This was reported in logs as follows:</p>
<p><em>Eval num_timesteps=100000, episode_reward=63.41 +/- 130.02</em></p>
<p><em>Episode length: 259.80 +/- 47.47</em></p>
<p>Now, let us look at the visual performance of the lander.</p>
<p><strong>Note:</strong> The performance varies across different seeds and runs. This code is not optimized to be stable across all runs and seeds. We hope you will be able to find an optimal configuration!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">wrap_env</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">))</span>
<span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
  <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
  <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> 
  <span class="k">if</span> <span class="n">done</span><span class="p">:</span> 
    <span class="k">break</span><span class="p">;</span>
            
<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">show_video</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The lander has landed safely!!</p>
<p>Let us analyze its performance (speed and stability). For this purpose, we plot the number of time steps on the X-axis and the episodic reward given by the trained model on the Y-axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ts2xy</span><span class="p">(</span><span class="n">load_results</span><span class="p">(</span><span class="n">log_dir</span><span class="p">),</span> <span class="s1">&#39;timesteps&#39;</span><span class="p">)</span>  <span class="c1"># Organising the logged results in to a clean format for plotting. </span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Timesteps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Episode Rewards&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>From the above plot, we observe that, although the maximum reward is achieved quickly. Achieving an episodic reward of &gt; 200 is good. We see that the agent has achieved it in less than 50000 timesteps (speed is good!). However, there are a lot of fluctuations in the performance (stability is not good!).</p>
<p>Your objective now is to modify the model parameters (nn_layers, learning_rate in the code cell #2 above), run all the cells following it and investigate the stability and speed of the chosen configuration.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="additional-project-ideas">
<h1>Additional Project Ideas<a class="headerlink" href="#additional-project-ideas" title="Permalink to this headline">¶</a></h1>
<div class="section" id="play-with-exploration-exploitation-trade-off">
<h2>1. Play with exploration-exploitation trade-off.<a class="headerlink" href="#play-with-exploration-exploitation-trade-off" title="Permalink to this headline">¶</a></h2>
<p>Exploration (selecting random actions) and exploitation (selecting greedy action) is a crucial component of the DQN algorithm. Explore random actions for a long time will slow down the training process. At the same time, if all actions are not explored enough, it might lead to a sub-optimal performance. In the DQN code above, we have used the following parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exploration_initial_eps</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># initial value of random action probability. Range is between 0 and 1.</span>
<span class="n">exploration_fraction</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># fraction of entire training period over which the exploration rate is reduced. Range is between 0 and 1.</span>
<span class="n">exploration_final_eps</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># (set by defualt) final value of random action probability. Range is between 0 and 1. </span>
</pre></div>
</div>
</div>
</div>
<p>Your objective is to play around with these parameters and analyze their performance (speed and stability). You can modify these parameters and set them as arguments in DQN(…,exploration_initial_eps = 1, exploration_fraction = 0.5, exploration_final_eps = 0.05,…).</p>
</div>
<div class="section" id="reward-shaping">
<h2>2. Reward Shaping<a class="headerlink" href="#reward-shaping" title="Permalink to this headline">¶</a></h2>
<p>Your objective here is to construct a modified reward function that improves the performance of the Lunar Lander. To this end, you would have to create your own custom environment. An example of a custom environment is given below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Taken from https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html</span>
<span class="k">class</span> <span class="nc">CustomEnv</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Custom Environment that follows gym interface&quot;&quot;&quot;</span>
  <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;render.modes&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;human&#39;</span><span class="p">]}</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CustomEnv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># Define action and observation space</span>
    <span class="c1"># They must be gym.spaces objects</span>
    <span class="c1"># Example when using discrete actions:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="n">N_DISCRETE_ACTIONS</span><span class="p">)</span>
    <span class="c1"># Example for using image as input (channel-first; channel-last also works):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N_CHANNELS</span><span class="p">,</span> <span class="n">HEIGHT</span><span class="p">,</span> <span class="n">WIDTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span>
  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">observation</span>  <span class="c1"># reward, done, info can&#39;t be included</span>
  <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;human&#39;</span><span class="p">):</span>
    <span class="o">...</span>
  <span class="k">def</span> <span class="nf">close</span> <span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
</div>
</div>
<p>As you are only changing the reward structure, you can inherit the original Lunar Lander environment from https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py and modify just the “step” function. Focus on modifying the following part of the code in the “step” function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">shaping</span> <span class="o">=</span> <span class="p">(</span>
      <span class="o">-</span><span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="o">-</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
      <span class="o">-</span> <span class="mi">100</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
      <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
      <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
  <span class="p">)</span>  <span class="c1"># And ten points for legs contact, the idea is if you</span>
  <span class="c1"># lose contact again after landing, you get negative reward</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_shaping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="n">shaping</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_shaping</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">prev_shaping</span> <span class="o">=</span> <span class="n">shaping</span>

  <span class="n">reward</span> <span class="o">-=</span> <span class="p">(</span>
      <span class="n">m_power</span> <span class="o">*</span> <span class="mf">0.30</span>
  <span class="p">)</span>  <span class="c1"># less fuel spent is better, about -30 for heuristic landing. You should modify these values. </span>
  <span class="n">reward</span> <span class="o">-=</span> <span class="n">s_power</span> <span class="o">*</span> <span class="mf">0.03</span>

  <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">game_over</span> <span class="ow">or</span> <span class="nb">abs</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">:</span>
      <span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span> 
  <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lander</span><span class="o">.</span><span class="n">awake</span><span class="p">:</span>
      <span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="o">+</span><span class="mi">100</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<p>Once you have cutomized your own environment, you can execute that environment by just calling:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Enter the name of the custome environment you created and uncomment the line below. </span>
<span class="c1">#env = Custom_LunarLander() </span>
<span class="c1"># Refer to https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html, if you would like to create more complex environments.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="identify-the-state-information-crucial-to-its-performance">
<h2>3. Identify the state information crucial to its performance.<a class="headerlink" href="#identify-the-state-information-crucial-to-its-performance" title="Permalink to this headline">¶</a></h2>
<p>Your objective here is to alter the input state information and analyze the performance. The input state of the Lunar Lander consists of following components:</p>
<ol class="simple">
<li><p>Horizontal Position</p></li>
<li><p>Vertical Position</p></li>
<li><p>Horizontal Velocity</p></li>
<li><p>Vertical Velocity</p></li>
<li><p>Angle</p></li>
<li><p>Angular Velocity</p></li>
<li><p>Left Leg Contact</p></li>
<li><p>Right Leg Contact</p></li>
</ol>
<p>You can train the algorithm by masking one of the eight components at a time and understand how that affects the performance of the algorithm. Similar to the reward shaping task, you would have to create a custom environment and modify the state space. Again, you can inherit all the necessary functions and modify the following portion of the “Step” function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="n">state</span> <span class="o">=</span> <span class="p">[</span> <span class="c1"># Remove one component at a time to investigate the effect on performance!</span>
            <span class="p">(</span><span class="n">pos</span><span class="o">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">VIEWPORT_W</span> <span class="o">/</span> <span class="n">SCALE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">VIEWPORT_W</span> <span class="o">/</span> <span class="n">SCALE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span>
            <span class="p">(</span><span class="n">pos</span><span class="o">.</span><span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">helipad_y</span> <span class="o">+</span> <span class="n">LEG_DOWN</span> <span class="o">/</span> <span class="n">SCALE</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">VIEWPORT_H</span> <span class="o">/</span> <span class="n">SCALE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">vel</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">VIEWPORT_W</span> <span class="o">/</span> <span class="n">SCALE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">FPS</span><span class="p">,</span>
            <span class="n">vel</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="n">VIEWPORT_H</span> <span class="o">/</span> <span class="n">SCALE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">FPS</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lander</span><span class="o">.</span><span class="n">angle</span><span class="p">,</span>
            <span class="mf">20.0</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lander</span><span class="o">.</span><span class="n">angularVelocity</span> <span class="o">/</span> <span class="n">FPS</span><span class="p">,</span>
            <span class="mf">1.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ground_contact</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="mf">1.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">legs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ground_contact</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="extension-to-atari-games">
<h2>4. Extension to Atari Games<a class="headerlink" href="#extension-to-atari-games" title="Permalink to this headline">¶</a></h2>
<p>In the Lunar Lander task, the input to the algorithm is a vector of state information. Deep RL algorithms can also be applied when the input to the training is image frames, which is the case in the Atari games. For example, consider an Atari game - Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). To train the Pong game, you can start with the following sample code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>#Necessary components required for running Atari games.
#Taken from https://github.com/openai/atari-py/issues/83#issuecomment-864356148
! wget http://www.atarimania.com/roms/Roms.rar &gt; /dev/null
! mkdir /content/ROM/ &gt; /dev/null
! unrar e /content/Roms.rar /content/ROM/ &gt; /dev/null
! python -m atari_py.import_roms /content/ROM/ &gt; /dev/null
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Taken from: https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb#scrollTo=f3K4rMXwimBO</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_atari_env</span><span class="p">(</span><span class="s1">&#39;PongNoFrameskip-v4&#39;</span><span class="p">,</span> <span class="n">n_envs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">#Atari Games take a lot of memory. Following commands crash on Coalb. Run the following code on Colab Pro or your local Jupyter notebook!</span>
<span class="c1"># env = VecFrameStack(env, n_stack=4)</span>
<span class="c1"># model = DQN(&#39;CnnPolicy&#39;, env, verbose=1)  # Note the difference here! We use &#39;CnnPolicy&quot; here instead of &#39;MlpPolicy&#39; as the input is frames. </span>
<span class="c1"># model.learn(total_timesteps=1) #change the number of timesteps as desired and run this command!</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="obstacle-avoidance-and-transfer-learning">
<h2>5. Obstacle Avoidance and Transfer Learning<a class="headerlink" href="#obstacle-avoidance-and-transfer-learning" title="Permalink to this headline">¶</a></h2>
<p>Your obstacle here is to add an obstacle in the path of the lunar lander (by creating a custom environment as described in point 2 above) and train the model such that the lander lands safely, avoiding collisions.</p>
<p>You would first want to devise a mechansim for adding obstacles. For example, you could have an imaginary obstacle at some horizantal and vertical position cooridnates and modify the reward function such that a penalty is levied if the lander comes close to it.</p>
<p>An interesting approach to solve this problem is to apply the techniques of transfer learning. For example, you could initialise the neural network model with the weights of the trained model on the original problem to improve the sample effeciency. This can be done using the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Specify the load path and uncomment below: </span>
<span class="c1"># model = load(load_path,</span>
<span class="c1">#              env=gym.make(&#39;LunarLander-v2&#39;),</span>
<span class="c1">#              custom_objects=None, **kwargs)</span>
</pre></div>
</div>
</div>
</div>
<p>Following are some of the resources on transfer learning that you would want to start with.</p>
<p><strong>Research Papers</strong></p>
<p>Surveys:</p>
<ol class="simple">
<li><p>(Long, Old, Highly cited) Taylor, M. E.,  et al. (2009). Transfer learning for reinforcement learning domains. https://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf</p></li>
<li><p>(Medium, Old, Good for a quick read) Lazaric, A. (2012). Transfer in reinforcement learning: a framework and a survey. https://hal.inria.fr/docs/00/77/26/26/PDF/transfer.pdf</p></li>
<li><p>(Medium, Recent, Good for a quick read) Zhu, Z., Lin, K., &amp; Zhou, J. (2020). Transfer learning in deep reinforcement learning. https://arxiv.org/pdf/2009.07888.pdf</p></li>
<li><p>Specific example:
Barreto, A., et al. (2016).  Successor features for transfer in reinforcement learning. https://arxiv.org/pdf/1606.05312</p></li>
</ol>
</div>
<div class="section" id="b-transfer-learning-in-minigrid-environment">
<h2>5(b) Transfer Learning in minigrid environment<a class="headerlink" href="#b-transfer-learning-in-minigrid-environment" title="Permalink to this headline">¶</a></h2>
<p>These are some simple gridworld gym environments designed to be particularly simple, lightweight and fast. Refer to https://github.com/maximecb/gym-minigrid for description of the environments. An example to load a minigrid environment is given below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!pip install gym-minigrid &gt; /dev/null
import gym_minigrid
env = gym.make(&#39;MiniGrid-Empty-5x5-v0&#39;)
</pre></div>
</div>
</div>
</div>
<p>You can train a standard DQN agent in this env by wrapping the env with full image observation wrappers:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym_minigrid</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">ImgObsWrapper</span><span class="p">(</span><span class="n">gym_minigrid</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">RGBImgObsWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Note that with full image observations, the shape of the image observations may differ between envs. For e.g., MiniGrid-Empty-5x5-v0 is (40,40,3) while MiniGrid-Empty-8x8-v0 is (64,64,3). So you may need to resize the observations for transfer learning to work with the same DQN architecture.</p>
<p>Now try training a DQN (or another method) in one (or multiple) minigrid env(s),and see if that knowledge transfers to another (or multiple other) minigrid env(s).</p>
</div>
<div class="section" id="preference-based-rl-pbrl">
<h2>6. Preference-Based RL (PBRL)<a class="headerlink" href="#preference-based-rl-pbrl" title="Permalink to this headline">¶</a></h2>
<p>PBRL is an exciting sub-area in RL where the traditional reward structure is replaced with human preferences. This setting is very useful in applications where it is difficult to construct a reward function.</p>
<p>In the earlier section, we have successfully trained the lunar lander to land safely. Here, the path that the lander follows to land safely can be arbitrary. In this project, using the techniques of PBRL, you will solve the lunar lander problem with an additional requirement that the lander should follow a specially curated path (for example, a straight line path). Following are some of the resources that will help you to get started with this project.</p>
<p><strong>Research papers:</strong></p>
<ol class="simple">
<li><p>Deep Reinforcement Learning
from Human Preferences https://papers.nips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf</p></li>
<li><p>Deep Q-learning from Demonstrations https://arxiv.org/pdf/1704.03732.pdf</p></li>
<li><p>Reward learning from human preferences https://arxiv.org/pdf/1811.06521.pdf</p></li>
<li><p>T-REX https://arxiv.org/pdf/1904.06387.pdf</p></li>
</ol>
<p><strong>Code Bases:</strong></p>
<ol class="simple">
<li><p>https://github.com/nottombrown/rl-teacher</p></li>
<li><p>https://github.com/hiwonjoon/ICML2019-TREX</p></li>
</ol>
<p>#<strong>References</strong></p>
<ol class="simple">
<li><p>Stable Baselines Framework: https://stable-baselines3.readthedocs.io/en/master/guide/examples.html</p></li>
<li><p>Lunar Lander Environment: https://gym.openai.com/envs/LunarLander-v2/</p></li>
<li><p>OpenAI gym environments: https://gym.openai.com/docs/</p></li>
<li><p>A good reference for introduction to RL: http://incompleteideas.net/book/the-book-2nd.html</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./projects/ReinforcementLearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="robolympics.html" title="previous page">NMA Robolympics: Controlling robots using reinforcement learning</a>
    <a class='right-next' id="next-link" href="human_rl.html" title="next page">Using RL to Model Cognitive Tasks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Neuromatch<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>